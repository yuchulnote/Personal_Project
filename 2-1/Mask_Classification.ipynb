{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61231b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 가능 여부: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3023f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리 위치: /Users/yuchul/Anaconda/Baram/MaskClassificaion\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder = \"Anaconda/Baram\"\n",
    "project_dir = \"MaskClassificaion\"\n",
    "\n",
    "base_path = Path(\"/Users/yuchul/\")\n",
    "project_path = base_path / folder / project_dir\n",
    "os.chdir(project_path)\n",
    "for x in list(project_path.glob(\"*\")):\n",
    "    if x.is_dir():\n",
    "        dir_name = str(x.relative_to(project_path))\n",
    "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
    "print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f38860",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = Path().absolute()\n",
    "data_path = current_path / \"data_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ccea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리 위치: /Users/yuchul/Anaconda/Baram/MaskClassificaion\n"
     ]
    }
   ],
   "source": [
    "print(\"현재 디렉토리 위치: {}\".format(current_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67271d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 'data/mask_cnn' 폴더가 있습니다! 이어서 진행하세요~\n"
     ]
    }
   ],
   "source": [
    "if (data_path / \"mask_cnn\").exists():\n",
    "    print(\"이미 'data/mask_cnn' 폴더가 있습니다! 이어서 진행하세요~\")\n",
    "else: print(\"없습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a482c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data_old/mask_cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e273881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuchul/Anaconda/Baram/MaskClassificaion/data_old\n"
     ]
    }
   ],
   "source": [
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf08ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6c5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskNonMaskDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode=None, transform=None, train_size=None):\n",
    "        self.all_data = sorted(glob.glob(os.path.join(data_dir, mode, '*', '*')))\n",
    "        \n",
    "        #RGB 가 아닌 파일필터링\n",
    "#         self.all_data = [i for i in self.old_all_data if transform(Image.open(i)).shape[0] == 3]\n",
    "    \n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.train_size = train_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_path = self.all_data[index]\n",
    "        img = Image.open(data_path)\n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        if os.path.basename(data_path).startswith(\"Mask\"):\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "            \n",
    "        return img, label # [1, 0, 0] , [0, 1, 0], [0, 0, 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.all_data)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2152e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_old/mask_cnn\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435844a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mMask\u001b[m\u001b[m     \u001b[34mNon_Mask\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data_old/mask_cnn/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7ffd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c983f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val' : transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485,  0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# train_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Train', transform=data_transforms['train'])\n",
    "# val_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Validation', transform=data_transforms['val'])\n",
    "# test_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Test', transform=data_transforms['val'])\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86a55118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Train', transform=data_transforms['train'])\n",
    "val_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Validation', transform=data_transforms['val'])\n",
    "test_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Test', transform=data_transforms['val'])\n",
    "\n",
    "add_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/add_data/Train_', transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4be8b9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <PIL.Image.Image image mode=RGB size=53x53 at 0x7FE69AB400A0> 0\n"
     ]
    }
   ],
   "source": [
    "for num, value in enumerate(add_data):\n",
    "    data, label = value\n",
    "    \n",
    "#     print(num, label)\n",
    "    \n",
    "    if num ==3:\n",
    "        print(num, data, label) \n",
    "    \n",
    "#     if(label == 0):\n",
    "#         data.save('./data_old/mask_cnn/add_data/Train/Mask/%s_%d_%d.jpg'%(\"Mask\" ,num, label))\n",
    "#     else:\n",
    "#         data.save('./data_old/mask_cnn/add_data/Train/NonMask/%s_%d_%d.jpg'%(\"NonMask\", num, label))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d67b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/add_data/Train', transform=data_transforms['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50f825be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18238 395 100\n"
     ]
    }
   ],
   "source": [
    "temp1 = glob.glob(os.path.join(data_dir, \"Train\", '*', '*'))\n",
    "temp2 = glob.glob(os.path.join(data_dir, \"validation\", '*', '*'))\n",
    "temp3 = glob.glob(os.path.join(data_dir, \"test\", '*', '*'))\n",
    "print(len(temp1), len(temp2), len(temp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5895cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18238\n",
      "395\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "x_train = []\n",
    "t_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19c2ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "680cafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "878c8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(f, x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flag=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) #f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) #f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f41c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "        \n",
    "        for key in params.keys():\n",
    "#             print(\"계산 전 m[key]: \", self.m[key].shape)\n",
    "#             print(\"계산 전 grads[key]: \", grads[key].shape)\n",
    "            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "#             print(\"key: \", key)\n",
    "#             print(\"m키값\", self.m[key].shape)\n",
    "#             print(\"grads키값\", grads[key].shape)\n",
    "#             print(\"v키값\", self.v[key].shape)\n",
    "                  \n",
    "#             self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "#             self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "#             print(\"params키값\", params[key].shape)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3eb3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Layer:\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        #가중치와 편향 매개변수 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #텐서 대응\n",
    "#         print(\"FC입력\", x.shape)\n",
    "#         print(\"가중치\", self.W.shape)\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "#         print(\"FC reshape 입력\", x.shape)\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "#         print(\"FC출력\", out.shape)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "#         print(\"FC back입력\", dout.shape)\n",
    "        dx = np.dot(dout, self.W.T) #.T는 Transpose\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape) # 입력 데이터 모양 변경(텐서 대응)\n",
    "#         print(\"FC back출력\", dx.shape)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "370bd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio \n",
    "            # x모양에 맞춰서 랜덤한 행렬생성, 원소별 0~1 사이의 랜던한 값, \n",
    "            #그 값이 dropout_ratio보다 큰 값이 true, 작으면 false인 행렬을 mask에 저장\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78a1fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "568c0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    # print(pad)\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "891da185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"데이터셋을 뒤섞는다.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 훈련 데이터\n",
    "    t : 정답 레이블\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5b02a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "#         print(\"BN 입력\", x.shape)\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "#         print(\"BN 출력\", out.reshape(*self.input_shape).shape)       \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "#         print('BN back입력', dout.shape)\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "#         print('BN back출력 reshape전', dx.shape)\n",
    "        \n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "#         print('BN back출력 reshape후', dx.shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35d54f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=None, pad=None):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Con입력값=\", x.shape)\n",
    "#         print(\"가중치\", self.W.shape)\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "#         print(\"out_h=\", out_h)\n",
    "#         print(\"out_w=\", out_w)\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "#         print(\"Con출력값\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "#         print(\"Con back입력\", dout.shape)\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "#         print(\"Con back출력\", dx.shape)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfe1816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Pool 입력\", x.shape)\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "#         print(\"Pool 출력\", out.shape)  \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "#         print(\"Pool back입력\", dout.shape)\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "#         print(\"Pool back출력\", dx.shape)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c3ada2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e29ac349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    \n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c27ed5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #손실함수\n",
    "        self.y = None #softmax의 출력\n",
    "        self.t = None #정답 레이블(원-핫 인코딩 형태)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "#         print(\"SoftmaxWithLoss의 t\", t.shape)\n",
    "        \n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        \n",
    "#         print(\"Softmax(x) = y: \", self.y.shape)\n",
    "\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "#         print(\"크로스엔트로피에러함수\", self.loss.shape)\n",
    "#         print(\"크로스엔트로피에러함수\", self.loss)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "#         print(\"SoftmaxWithLoss back의 batchsize\", batch_size)\n",
    "\n",
    "#         print(\"SoftmaxWithLoss back의 y모양\", self.y.shape)\n",
    "#         print(\"SoftmaxWithLoss back의 t모양\", self.t.shape)\n",
    "#         print(\"SoftmaxWithLoss back의 y\", self.y)\n",
    "#         print(\"SoftmaxWithLoss back의 t\", self.t)\n",
    "        \n",
    "        if self.t.size == self.y.size: #정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "#         print(\"SoftmaxWithLoss back의 dx\", dx.shape)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aafaf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numba\n",
    "class VGG6:\n",
    "    \n",
    "    def __init__(self, input_dim={3, 224, 224},\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':2},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':2},\n",
    "                 conv_param_4 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':2},\n",
    "                 hidden_size=50, output_size=2):\n",
    "        \n",
    "        self.first_flg = True\n",
    "        \n",
    "        # ======= 가중치 초기화 =======\n",
    "        \n",
    "        pre_node_nums = np.array([3*3*3, 16*3*3, 32*3*3, 32*3*3, 64*7*7, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums) # ReLU 사용할 때 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = 3\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "            self.params['gamma' + str(idx+1)] = 1.0\n",
    "            self.params['beta' + str(idx+1)] = 0.0\n",
    "            \n",
    "        self.params['W5'] = weight_init_scales[4] * np.random.randn(64*7*7, hidden_size)\n",
    "        self.params['b5'] = np.zeros(hidden_size)\n",
    "        self.params['W6'] = weight_init_scales[5] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b6'] = np.zeros(output_size)\n",
    "        \n",
    "        # ======= 계층 생성 =======\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])) # 0\n",
    "        self.layers.append(BatchNormalization(self.params['gamma1'], self.params['beta1'])) #1\n",
    "        self.layers.append(ReLU())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])) # 3\n",
    "        self.layers.append(BatchNormalization(self.params['gamma2'], self.params['beta2'])) #4\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad'])) # 7\n",
    "        self.layers.append(BatchNormalization(self.params['gamma3'], self.params['beta3'])) #8\n",
    "        self.layers.append(ReLU())\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad'])) # 10\n",
    "        self.layers.append(BatchNormalization(self.params['gamma4'], self.params['beta4']))#11\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(FC_Layer(self.params['W5'], self.params['b5'])) # 14\n",
    "#         self.layers.append(ReLU())\n",
    "#         self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(FC_Layer(self.params['W6'], self.params['b6'])) # 17\n",
    "#         self.layers.append(ReLU())\n",
    "#         self.layers.append(Dropout(0.5))\n",
    "        \n",
    "#         self.layers.append(SoftmaxWithLoss())\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x, train_flg=False, first_flg=None):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                if self.first_flg and isinstance(layer, BatchNormalization):\n",
    "                    x = layer.__forward(x, train_flg)\n",
    "                else:\n",
    "                    x = layer.forward(x, train_flg)\n",
    "            \n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "                     \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True, first_flg=self.first_flg)\n",
    "        self.first_flg = False\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=32):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "#     def accuracy(self, inputs, labels):\n",
    "        \n",
    "#         # 0, 1 -> [1, 0], [0, 1]\n",
    "#          for i in range()\n",
    "        \n",
    "#         if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "#         acc = 0.0\n",
    "        \n",
    "#         for i in range(int(x.shape[0] / batch_size)):\n",
    "#             tx = x[i*batch_size:(i+1)*batch_size]\n",
    "#             tt = t[i*batch_size:(i+1)*batch_size]\n",
    "#             y = self.predict(tx, train_flg=False)\n",
    "#             y = np.argmax(y, axis=1)\n",
    "#             acc += np.sum(y == tt)\n",
    "        \n",
    "#         return acc / x.shape[0]\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \n",
    "        #forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "                \n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        #결과 저장\n",
    "        grads = {}\n",
    "        idx = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            \n",
    "            if isinstance(layer, Convolution) or isinstance(layer, FC_Layer):\n",
    "\n",
    "                grads['W' + str(idx+1)] = self.layers[i].dW\n",
    "                grads['b' + str(idx+1)] = self.layers[i].db\n",
    "                \n",
    "                if(i==14):\n",
    "                    idx += 1\n",
    "                            \n",
    "            elif isinstance(layer, BatchNormalization):\n",
    "                grads['gamma'+str(idx+1)] = self.layers[i].dgamma\n",
    "                grads['beta'+str(idx+1)] = self.layers[i].dbeta\n",
    "                idx += 1\n",
    "                \n",
    "        return grads\n",
    "                    \n",
    "        \n",
    "#         for i, layer_idx in enumerate((0, 1, 3, 4, 7, 8, 10, 11, 14, 17)):\n",
    "#             # print(\"len(self.layers):\",len(self.layers))\n",
    "#             grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "#             grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "#             grads['gamma'+str(i+1)] = self.layers[layer_idx].dgamma\n",
    "#             grads['beta'+str(i+1)] = self.layers[layer_idx].dbeta\n",
    "                \n",
    "#         return grads\n",
    "    \n",
    "    def save_params(self, file_name='params.pkl'):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "        print(\"training network params:\", self.params.keys())\n",
    "        \n",
    "        \n",
    "    def load_params(self, file_name='epoch.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        \n",
    "#         for i, layer_idx in enumerate((0, 1, 3, 4, 7, 8, 10, 11, 14, 17)):\n",
    "#             self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "#             self.layers[layer_idx].b = self.params['b' + str(i+1)]\n",
    "        \n",
    "        idx = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Convolution) or isinstance(layer, FC_Layer):\n",
    "\n",
    "                self.layers[i].W = self.params['W' + str(idx+1)]\n",
    "                self.layers[i].b = self.params['b' + str(idx+1)]\n",
    "                \n",
    "                if(i==14):\n",
    "                    idx += 1\n",
    "                            \n",
    "            elif isinstance(layer, BatchNormalization):\n",
    "                self.layers[i].gamma = self.params['gamma'+str(idx+1)]\n",
    "                self.layers[i].beta = self.params['beta'+str(idx+1)]\n",
    "                idx += 1\n",
    "\n",
    "        print(\"Params is successfully loaded!:\", self.params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54682451",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGG6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d8240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(data_list, title, color, save_path):\n",
    "    batch_num_list = [i for i in range(0, len(data_list))]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.rc('font', size=25)\n",
    "    plt.plot(batch_num_list, data_list, color=color, marker='o', linestyle='solid')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    title = plt.ylabel(title)\n",
    "\n",
    "    plt.savefig(save_path, dpi=600)\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86ccf78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, network, x_train_loader, x_test_loader,\n",
    "                 epochs=30, mini_batch_size=32,\n",
    "                 optimizer='adagrad', optimizer_param={'lr':0.0001}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        # print(f\"Network Spec: {self.network.keys()}\")\n",
    "        self.verbose = verbose\n",
    "        self.train_loader = x_train_loader\n",
    "        self.test_loader = x_test_loader\n",
    "        # print(x_train[0][0][0][0])\n",
    "#         self.x_train = x_train\n",
    "#         self.t_train = t_train\n",
    "#         self.x_test = x_test\n",
    "#         self.t_test = t_test\n",
    "#         self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = len(x_train) # x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.epochs = epochs\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "        self.train_mode = True\n",
    "\n",
    "    def train_step(self):\n",
    "#         batch_mask = np.random.choice(self.train_size, self.batch_size).tolist()\n",
    "        \n",
    "#         x_batch = []\n",
    "#         t_batch = []\n",
    "\n",
    "#         for idx in batch_mask:\n",
    "#             x_batch.append(self.x_train[idx])\n",
    "#             t_batch.append(self.t_train[idx])\n",
    "            \n",
    "        dataloader = self.train_loader if self.train_mode else self.test_loader\n",
    "        name = \"train\" if self.train_mode else \"evaluate\"\n",
    "\n",
    "        for x_batch, t_batch in dataloader:\n",
    "#             print(x_batch.shape)\n",
    "            grads = self.network.gradient(x_batch, t_batch)\n",
    "            \n",
    "#             for key in grads.keys():\n",
    "#                 print(key, \":\", grads[key].shape)\n",
    "            self.optimizer.update(self.network.params, grads)\n",
    "            loss = self.network.loss(x_batch, t_batch)\n",
    "            \n",
    "            if self.train_mode:\n",
    "                self.train_loss_list.append(loss)\n",
    "                \n",
    "            if self.verbose: print(f\"\\t{name} loss: {loss}\")\n",
    "\n",
    "            if self.current_iter % self.iter_per_epoch == 0:\n",
    "                self.current_epoch += 1\n",
    "    \n",
    "                x_train_sample, t_train_sample = x_batch, t_batch\n",
    "                train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            \n",
    "            if self.train_mode:\n",
    "                self.train_acc_list.append(train_acc)\n",
    "            else:\n",
    "                self.test_acc_list.append(test_acc)\n",
    "\n",
    "                if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "            self.current_iter += 1\n",
    "            \n",
    "\n",
    "    def train(self, current_epochs):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.train_step()\n",
    "            self.network.save_params(file_name=f\"epoch_{epoch+current_epochs+1}.pkl\")\n",
    "            print(f\"model({epoch+1}/{self.epochs}) is saved!\")\n",
    "            graph(self.train_loss_list, 'loss', 'red', f\"epoch_{epoch+current_epochs+1}\")\n",
    "\n",
    "    def test(self, test_data_length = len(test_data)):\n",
    "        hits = 0.0\n",
    "        acc = 0.0\n",
    "        for imgs, labels in self.test_loader:\n",
    "            hits += self.network.accuracy(imgs, labels)\n",
    "        acc = hist/test_data_length\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4782b3ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain loss: 0.932920466646347\n",
      "\ttrain loss: 1.2462551918526121\n",
      "\ttrain loss: 1.2579625542757378\n",
      "\ttrain loss: 1.0372404111626599\n",
      "\ttrain loss: 1.1039877634988347\n",
      "\ttrain loss: 0.7768785958466948\n",
      "\ttrain loss: 1.0357872327505186\n",
      "\ttrain loss: 1.075600137638843\n",
      "\ttrain loss: 0.721568545174064\n",
      "\ttrain loss: 0.4968873597112372\n",
      "\ttrain loss: 0.8013717740289874\n",
      "\ttrain loss: 1.0052422613627683\n",
      "\ttrain loss: 1.1099211454327862\n",
      "\ttrain loss: 1.1080662388296219\n",
      "\ttrain loss: 0.9237806216784271\n",
      "\ttrain loss: 0.7426394244088946\n",
      "\ttrain loss: 0.6787362589845172\n",
      "\ttrain loss: 0.6565640511394739\n",
      "\ttrain loss: 0.4592017493579996\n",
      "\ttrain loss: 0.42357977034660144\n",
      "\ttrain loss: 0.5528648507275867\n",
      "\ttrain loss: 0.5881014713904753\n",
      "\ttrain loss: 0.7086240402659556\n",
      "\ttrain loss: 0.977522153834252\n",
      "\ttrain loss: 0.7283940744024928\n",
      "\ttrain loss: 0.7786790269942743\n",
      "\ttrain loss: 0.8127712568961745\n",
      "\ttrain loss: 0.5715624670614001\n",
      "\ttrain loss: 0.7064608486173649\n",
      "\ttrain loss: 0.6118853580871106\n",
      "\ttrain loss: 1.0566976274538749\n",
      "\ttrain loss: 0.787306866083148\n",
      "\ttrain loss: 0.7831368322871997\n",
      "\ttrain loss: 0.7350451170109134\n",
      "\ttrain loss: 0.594865408239957\n",
      "\ttrain loss: 0.8519549426435344\n",
      "\ttrain loss: 0.6175546869122301\n",
      "\ttrain loss: 0.8244756936555588\n",
      "\ttrain loss: 0.579737895455578\n",
      "\ttrain loss: 1.1101631975000743\n",
      "\ttrain loss: 0.6373770191707333\n",
      "\ttrain loss: 0.8978094114520325\n",
      "\ttrain loss: 0.6104555422451095\n",
      "\ttrain loss: 0.6794854313176939\n",
      "\ttrain loss: 0.5931524721990022\n",
      "\ttrain loss: 0.5498823015446698\n",
      "\ttrain loss: 0.6268650661969865\n",
      "\ttrain loss: 0.370673857977743\n",
      "\ttrain loss: 0.700908072389605\n",
      "\ttrain loss: 0.7908216034333683\n",
      "\ttrain loss: 0.5556175141855961\n",
      "\ttrain loss: 0.6681916156980032\n",
      "\ttrain loss: 1.0714273171088848\n",
      "\ttrain loss: 0.7078833715837187\n",
      "\ttrain loss: 0.5467169439355971\n",
      "\ttrain loss: 0.5436225624864852\n",
      "\ttrain loss: 0.5297970852535561\n",
      "\ttrain loss: 0.7350346433238313\n",
      "\ttrain loss: 0.8950920683493557\n",
      "\ttrain loss: 1.0723738881944256\n",
      "\ttrain loss: 0.6264399496106094\n",
      "\ttrain loss: 0.5762033929178361\n",
      "\ttrain loss: 0.7115712986067346\n",
      "\ttrain loss: 0.6082128720882353\n",
      "\ttrain loss: 0.7691226359055614\n",
      "\ttrain loss: 0.6914967684508597\n",
      "\ttrain loss: 0.48980511644352775\n",
      "\ttrain loss: 0.47232204965247404\n",
      "\ttrain loss: 0.6294298906647682\n",
      "\ttrain loss: 0.631953420970581\n",
      "\ttrain loss: 0.7235717917644384\n",
      "\ttrain loss: 0.7836285616213479\n",
      "\ttrain loss: 0.8845274373866492\n",
      "\ttrain loss: 0.6065027086008143\n",
      "\ttrain loss: 0.6530335068647259\n",
      "\ttrain loss: 0.5433072530713179\n",
      "\ttrain loss: 0.29096308297495443\n",
      "\ttrain loss: 0.8096603221447375\n",
      "\ttrain loss: 0.8255031117674301\n",
      "\ttrain loss: 0.6108163093079296\n",
      "\ttrain loss: 0.6381350253788165\n",
      "\ttrain loss: 0.5101223617348707\n",
      "\ttrain loss: 0.5584374439696429\n",
      "\ttrain loss: 0.648057113471097\n",
      "\ttrain loss: 0.705346311393473\n",
      "\ttrain loss: 0.7886491212852329\n",
      "\ttrain loss: 0.7545888803735454\n",
      "\ttrain loss: 0.5428815729777816\n",
      "\ttrain loss: 0.46512893727214605\n",
      "\ttrain loss: 0.712576153562285\n",
      "\ttrain loss: 0.5920653402621703\n",
      "\ttrain loss: 0.8326258923294354\n",
      "\ttrain loss: 0.5413634361292412\n",
      "\ttrain loss: 0.47326997839658796\n",
      "\ttrain loss: 0.8999115687101393\n",
      "\ttrain loss: 0.6059987048579041\n",
      "\ttrain loss: 0.5963469182481557\n",
      "\ttrain loss: 0.6438992286798377\n",
      "\ttrain loss: 0.32688773011354777\n",
      "\ttrain loss: 0.7050773440264781\n",
      "\ttrain loss: 0.5352102246276693\n",
      "\ttrain loss: 0.5061704379076649\n",
      "\ttrain loss: 0.6837150357422148\n",
      "\ttrain loss: 0.9554495501835591\n",
      "\ttrain loss: 0.7926010897338818\n",
      "\ttrain loss: 0.8350402478250025\n",
      "\ttrain loss: 0.6620637589308455\n",
      "\ttrain loss: 0.8682014557350877\n",
      "\ttrain loss: 0.7627662218924829\n",
      "\ttrain loss: 0.42346788620324344\n",
      "\ttrain loss: 0.5079274751731402\n",
      "\ttrain loss: 0.6124418181705821\n",
      "\ttrain loss: 0.41926587980362856\n",
      "\ttrain loss: 0.5445091874514042\n",
      "\ttrain loss: 0.4872482476514328\n",
      "\ttrain loss: 0.7474520128718073\n",
      "\ttrain loss: 0.5589365518897748\n",
      "\ttrain loss: 0.6273935184431414\n",
      "\ttrain loss: 0.6619516106352881\n",
      "\ttrain loss: 0.3687200103197861\n",
      "\ttrain loss: 0.7212106924384658\n",
      "\ttrain loss: 0.3855413540855499\n",
      "\ttrain loss: 0.8102733489629411\n",
      "\ttrain loss: 0.7740894534351656\n",
      "\ttrain loss: 0.5144403902140235\n",
      "\ttrain loss: 0.5047240138850924\n",
      "\ttrain loss: 0.6156042159242102\n",
      "\ttrain loss: 1.0187377765622345\n",
      "\ttrain loss: 0.5427819645746321\n",
      "\ttrain loss: 0.3500415846075275\n",
      "\ttrain loss: 0.4087913289799997\n",
      "\ttrain loss: 0.4536141544449028\n",
      "\ttrain loss: 0.43545294594566675\n",
      "\ttrain loss: 0.8740457908083231\n",
      "\ttrain loss: 0.620353886749731\n",
      "\ttrain loss: 0.6439257449590833\n",
      "\ttrain loss: 0.5652919873423422\n",
      "\ttrain loss: 0.6851942629090164\n",
      "\ttrain loss: 0.5742413363935313\n",
      "\ttrain loss: 0.640995410350551\n",
      "\ttrain loss: 0.8533249390064508\n",
      "\ttrain loss: 1.101339029617762\n",
      "\ttrain loss: 0.44340960228043624\n",
      "\ttrain loss: 0.37841068446145304\n",
      "\ttrain loss: 0.5573420676250695\n",
      "\ttrain loss: 0.5484787373973619\n",
      "\ttrain loss: 0.6834033144022915\n",
      "\ttrain loss: 0.5105102131170044\n",
      "\ttrain loss: 0.5027706598696804\n",
      "\ttrain loss: 0.6760965071592577\n",
      "\ttrain loss: 0.5141287578482046\n",
      "\ttrain loss: 0.9540284365579867\n",
      "\ttrain loss: 0.5666885937516873\n",
      "\ttrain loss: 0.7329036292207556\n",
      "\ttrain loss: 0.6397324496858673\n",
      "\ttrain loss: 0.6025097211390378\n",
      "\ttrain loss: 0.6069373253824155\n",
      "\ttrain loss: 0.5514087546155393\n",
      "\ttrain loss: 0.6798509258858962\n",
      "\ttrain loss: 0.5469897175244729\n",
      "\ttrain loss: 0.676786391624004\n",
      "\ttrain loss: 0.7946797582504472\n",
      "\ttrain loss: 0.5314730324214454\n",
      "\ttrain loss: 0.5806188402855401\n",
      "\ttrain loss: 0.8974154959532445\n",
      "\ttrain loss: 0.4025066579767015\n",
      "\ttrain loss: 0.7489512432165677\n",
      "\ttrain loss: 0.6616985811749456\n",
      "\ttrain loss: 0.46124533551213687\n",
      "\ttrain loss: 0.6296761898230916\n",
      "\ttrain loss: 0.7005988054176956\n",
      "\ttrain loss: 0.5946874430268498\n",
      "\ttrain loss: 0.5513070462980993\n",
      "\ttrain loss: 0.506875231738842\n",
      "\ttrain loss: 0.6176206084193218\n",
      "\ttrain loss: 0.8990844293830326\n",
      "\ttrain loss: 0.5525518322822123\n",
      "\ttrain loss: 0.5797561698579894\n",
      "\ttrain loss: 0.4588359092630736\n",
      "\ttrain loss: 0.47575747161910786\n",
      "\ttrain loss: 0.49512160290647395\n",
      "\ttrain loss: 0.623452297543704\n",
      "\ttrain loss: 0.5746538429322732\n",
      "\ttrain loss: 0.5910284390837249\n",
      "\ttrain loss: 0.6451917753945711\n",
      "\ttrain loss: 0.7546813917231504\n",
      "\ttrain loss: 0.6250646372341209\n",
      "\ttrain loss: 0.525802592884157\n",
      "\ttrain loss: 0.5621626543031334\n",
      "\ttrain loss: 0.7963667357908477\n",
      "\ttrain loss: 0.6585585388817317\n",
      "\ttrain loss: 0.652303754076299\n",
      "\ttrain loss: 0.72011805879112\n",
      "\ttrain loss: 0.39123442005992926\n",
      "\ttrain loss: 0.700110597663647\n",
      "\ttrain loss: 0.6517130165877225\n",
      "\ttrain loss: 0.40312369550554805\n",
      "\ttrain loss: 0.6154197743480481\n",
      "\ttrain loss: 0.618664985968546\n",
      "\ttrain loss: 0.7993947151083336\n",
      "\ttrain loss: 0.4854696390930501\n",
      "\ttrain loss: 0.631372547822975\n",
      "\ttrain loss: 0.5306726652751914\n",
      "\ttrain loss: 0.6101832648444128\n",
      "\ttrain loss: 0.5240812006594275\n",
      "\ttrain loss: 0.45705071937683844\n",
      "\ttrain loss: 0.4746888339163991\n",
      "\ttrain loss: 0.5629295582569437\n",
      "\ttrain loss: 0.5630645924664988\n",
      "\ttrain loss: 0.5186861542755904\n",
      "\ttrain loss: 0.60950524092734\n",
      "\ttrain loss: 0.5926755471808816\n",
      "\ttrain loss: 0.41821394418031554\n",
      "\ttrain loss: 0.4416531597026872\n",
      "\ttrain loss: 0.807132239346416\n",
      "\ttrain loss: 0.45070252531322064\n",
      "\ttrain loss: 0.49710376286388197\n",
      "\ttrain loss: 0.3991534554045927\n",
      "\ttrain loss: 0.4211745749670911\n",
      "\ttrain loss: 0.5721753074053364\n",
      "\ttrain loss: 0.9425904559327977\n",
      "\ttrain loss: 0.28766196557093837\n",
      "\ttrain loss: 0.4871568202947827\n",
      "\ttrain loss: 0.5699325349958415\n",
      "\ttrain loss: 0.4753877182690351\n",
      "\ttrain loss: 0.6433324459440205\n",
      "\ttrain loss: 0.4486335328808662\n",
      "\ttrain loss: 0.5173071896898416\n",
      "\ttrain loss: 0.6282963269315968\n",
      "\ttrain loss: 0.7358146374542938\n",
      "\ttrain loss: 0.5931222154489698\n",
      "\ttrain loss: 0.4593574814957443\n",
      "\ttrain loss: 0.9048955048595293\n",
      "\ttrain loss: 0.48369926539817265\n",
      "\ttrain loss: 0.38300345095199306\n",
      "\ttrain loss: 0.5622487016174498\n",
      "\ttrain loss: 0.4983826966483176\n",
      "\ttrain loss: 0.4365475000189576\n",
      "\ttrain loss: 0.5822820359685171\n",
      "\ttrain loss: 0.3556077046012117\n",
      "\ttrain loss: 0.41090647222364807\n",
      "\ttrain loss: 0.497824374471604\n",
      "\ttrain loss: 0.7378407193395478\n",
      "\ttrain loss: 0.3961833742499472\n",
      "\ttrain loss: 0.2630012467922548\n",
      "\ttrain loss: 0.7481416968749401\n",
      "\ttrain loss: 0.5949843905686213\n",
      "\ttrain loss: 0.5277758053211479\n",
      "\ttrain loss: 0.3480287646855767\n",
      "\ttrain loss: 0.4495385074119712\n",
      "\ttrain loss: 0.3970438182494995\n",
      "\ttrain loss: 0.6430399239440421\n",
      "\ttrain loss: 0.681457265236115\n",
      "\ttrain loss: 0.3974919440143617\n",
      "\ttrain loss: 0.6948755589605746\n",
      "\ttrain loss: 0.5628621166427291\n",
      "\ttrain loss: 0.35810449531163435\n",
      "\ttrain loss: 0.5021613651873531\n",
      "\ttrain loss: 0.6308731565729593\n",
      "\ttrain loss: 0.46060915911447065\n",
      "\ttrain loss: 0.6876160385214146\n",
      "\ttrain loss: 0.46657434526908126\n",
      "\ttrain loss: 0.4566388170559659\n",
      "\ttrain loss: 0.4688674855104458\n",
      "\ttrain loss: 0.6485626290948736\n",
      "\ttrain loss: 0.5310049954408184\n",
      "\ttrain loss: 0.6474737032091089\n",
      "\ttrain loss: 0.6341046273475933\n",
      "\ttrain loss: 0.5184243744937245\n",
      "\ttrain loss: 0.44918890707086606\n",
      "\ttrain loss: 0.6888241215691324\n",
      "\ttrain loss: 0.3637134087655521\n",
      "\ttrain loss: 0.2285818163131164\n",
      "\ttrain loss: 0.5431386665240363\n",
      "\ttrain loss: 0.48964361178500504\n",
      "\ttrain loss: 0.7261329235916266\n",
      "\ttrain loss: 0.5493981841837414\n",
      "\ttrain loss: 0.38863002960728577\n",
      "\ttrain loss: 0.6017397091866235\n",
      "\ttrain loss: 0.381130608599338\n",
      "\ttrain loss: 0.5472094569441632\n",
      "\ttrain loss: 0.47163867343133636\n",
      "\ttrain loss: 0.6509995284069726\n",
      "\ttrain loss: 0.5362141934156945\n",
      "\ttrain loss: 0.6817559165989934\n",
      "\ttrain loss: 0.45404137154177093\n",
      "\ttrain loss: 0.5447510118315247\n",
      "\ttrain loss: 0.5258185465875505\n",
      "\ttrain loss: 0.6816828911070023\n",
      "\ttrain loss: 0.6501500623437823\n",
      "\ttrain loss: 0.4793478028168741\n",
      "\ttrain loss: 0.6944086420432101\n",
      "\ttrain loss: 0.41622301452154326\n",
      "\ttrain loss: 0.7496069372256636\n",
      "\ttrain loss: 0.6904087034871287\n",
      "\ttrain loss: 0.5327745647205024\n",
      "\ttrain loss: 1.2293063792818097\n",
      "\ttrain loss: 0.9547056568853878\n",
      "\ttrain loss: 0.9125320168136171\n",
      "\ttrain loss: 0.4946228425820142\n",
      "\ttrain loss: 0.5933029611233549\n",
      "\ttrain loss: 0.4227749185675183\n",
      "\ttrain loss: 0.4913877417847504\n",
      "\ttrain loss: 0.6158747765700816\n",
      "\ttrain loss: 0.6835514078538973\n",
      "\ttrain loss: 0.537495634472031\n",
      "\ttrain loss: 0.42881479847036674\n",
      "\ttrain loss: 0.6572426646237067\n",
      "\ttrain loss: 0.5169848457440097\n",
      "\ttrain loss: 0.7045981028313237\n",
      "\ttrain loss: 0.5044217028279971\n",
      "\ttrain loss: 0.4888985507040916\n",
      "\ttrain loss: 0.46213122430841214\n",
      "\ttrain loss: 0.5117470219049782\n",
      "\ttrain loss: 0.7819134934511108\n",
      "\ttrain loss: 0.5868761564674051\n",
      "\ttrain loss: 0.6820722771701674\n",
      "\ttrain loss: 0.5901198941487624\n",
      "\ttrain loss: 0.4609661356316549\n",
      "\ttrain loss: 0.5981822968583954\n",
      "\ttrain loss: 0.3061506941997043\n",
      "\ttrain loss: 0.4768958751887058\n",
      "\ttrain loss: 0.42122959524839443\n",
      "\ttrain loss: 0.5688977540344553\n",
      "\ttrain loss: 0.3991311634872307\n",
      "\ttrain loss: 0.6206208088462335\n",
      "\ttrain loss: 0.4851516368366622\n",
      "\ttrain loss: 0.7717009476537624\n",
      "\ttrain loss: 0.5562352948138835\n",
      "\ttrain loss: 0.4281432859042851\n",
      "\ttrain loss: 0.5566747653353572\n",
      "\ttrain loss: 0.49393549333283354\n",
      "\ttrain loss: 0.5491902616658574\n",
      "\ttrain loss: 0.5096014264870379\n",
      "\ttrain loss: 0.6816596036313982\n",
      "\ttrain loss: 0.6998264379378698\n",
      "\ttrain loss: 0.7775254439532577\n",
      "\ttrain loss: 0.60051588691929\n",
      "\ttrain loss: 0.44704133518563616\n",
      "\ttrain loss: 0.4268588355814342\n",
      "\ttrain loss: 0.3552403465546694\n",
      "\ttrain loss: 0.5879692098579514\n",
      "\ttrain loss: 0.595559103970764\n",
      "\ttrain loss: 0.49459804417329406\n",
      "\ttrain loss: 0.5606644195718226\n",
      "\ttrain loss: 0.6330807507508148\n",
      "\ttrain loss: 0.381047401374199\n",
      "\ttrain loss: 0.7190997971305546\n",
      "\ttrain loss: 0.3056939068171396\n",
      "\ttrain loss: 0.7228878526586234\n",
      "\ttrain loss: 0.5105944155711879\n",
      "\ttrain loss: 0.3218552212402165\n",
      "\ttrain loss: 0.3975891026592496\n",
      "\ttrain loss: 0.7386805987073805\n",
      "\ttrain loss: 0.605433883767482\n",
      "\ttrain loss: 0.5131771609590792\n",
      "\ttrain loss: 0.454934331731742\n",
      "\ttrain loss: 0.6180578106538435\n",
      "\ttrain loss: 0.35543537266325465\n",
      "\ttrain loss: 0.5772777223359249\n",
      "\ttrain loss: 0.543645500087993\n",
      "\ttrain loss: 0.6253988702930197\n",
      "\ttrain loss: 0.8655341764674271\n",
      "\ttrain loss: 0.409281774912536\n",
      "\ttrain loss: 0.5561320269843055\n",
      "\ttrain loss: 0.7707471601619013\n",
      "\ttrain loss: 0.5027860968618418\n",
      "\ttrain loss: 0.5164825097936716\n",
      "\ttrain loss: 0.3494011219705955\n",
      "\ttrain loss: 0.46639706786061175\n",
      "\ttrain loss: 0.5418654656225967\n",
      "\ttrain loss: 0.5739168689384293\n",
      "\ttrain loss: 0.44196409059777875\n",
      "\ttrain loss: 0.42808210832678073\n",
      "\ttrain loss: 0.45422779384354395\n",
      "\ttrain loss: 0.46271753870439025\n",
      "\ttrain loss: 0.5545134477364906\n",
      "\ttrain loss: 0.4965388967526036\n",
      "\ttrain loss: 0.31165930392949176\n",
      "\ttrain loss: 0.3419270647347906\n",
      "\ttrain loss: 0.6361061957491351\n",
      "\ttrain loss: 0.5734789879425009\n",
      "\ttrain loss: 0.55190781328269\n",
      "\ttrain loss: 0.5251076401103479\n",
      "\ttrain loss: 0.397467473309042\n",
      "\ttrain loss: 0.6344174093386472\n",
      "\ttrain loss: 0.539359663347322\n",
      "\ttrain loss: 0.8828086964505057\n",
      "\ttrain loss: 0.6345097249476406\n",
      "\ttrain loss: 0.7168100186270501\n",
      "\ttrain loss: 0.43902282022210215\n",
      "\ttrain loss: 0.5407574410021182\n",
      "\ttrain loss: 0.6604213619132167\n",
      "\ttrain loss: 0.3903168301398268\n",
      "\ttrain loss: 0.5717473999061506\n",
      "\ttrain loss: 0.5771800300668\n",
      "\ttrain loss: 0.37147949811655456\n",
      "\ttrain loss: 0.6633386165788486\n",
      "\ttrain loss: 0.48067463486646367\n",
      "\ttrain loss: 0.3917655745214313\n",
      "\ttrain loss: 0.39809011994170374\n",
      "\ttrain loss: 0.3432120283047152\n",
      "\ttrain loss: 0.47945561050001606\n",
      "\ttrain loss: 0.7740843599764894\n",
      "\ttrain loss: 0.48491633812059054\n",
      "\ttrain loss: 0.5349119889732613\n",
      "\ttrain loss: 0.3591797045571322\n",
      "\ttrain loss: 0.43265590445717267\n",
      "\ttrain loss: 0.490971219825597\n",
      "\ttrain loss: 0.4560308555615653\n",
      "\ttrain loss: 0.45984742009249624\n",
      "\ttrain loss: 0.42893401832703354\n",
      "\ttrain loss: 0.6906194202168792\n",
      "\ttrain loss: 0.6861019619984104\n",
      "\ttrain loss: 0.5140078765410502\n",
      "\ttrain loss: 0.5486891326301201\n",
      "\ttrain loss: 0.444115165171032\n",
      "\ttrain loss: 0.5203011337140636\n",
      "\ttrain loss: 0.3445298890497038\n",
      "\ttrain loss: 0.5413667363298054\n",
      "\ttrain loss: 0.8697931593781301\n",
      "\ttrain loss: 0.5216596803535642\n",
      "\ttrain loss: 0.3797095147252223\n",
      "\ttrain loss: 0.822558146720785\n",
      "\ttrain loss: 0.8116073433832209\n",
      "\ttrain loss: 0.620113836315799\n",
      "\ttrain loss: 0.7203526063142118\n",
      "\ttrain loss: 0.4752714509603947\n",
      "\ttrain loss: 0.4427580686501439\n",
      "\ttrain loss: 0.40201637159483833\n",
      "\ttrain loss: 0.5774511983422245\n",
      "\ttrain loss: 0.42572348638703683\n",
      "\ttrain loss: 0.6005205759831713\n",
      "\ttrain loss: 0.6228443716304132\n",
      "\ttrain loss: 0.6912149730847943\n",
      "\ttrain loss: 0.578912406021378\n",
      "\ttrain loss: 0.5357214109022049\n",
      "\ttrain loss: 0.46718511228868087\n",
      "\ttrain loss: 0.5767738233875359\n",
      "\ttrain loss: 0.607220960103993\n",
      "\ttrain loss: 0.717148828553437\n",
      "\ttrain loss: 0.23622026869043042\n",
      "\ttrain loss: 0.41554764551160567\n",
      "\ttrain loss: 0.42043362756856917\n",
      "\ttrain loss: 0.3975107018308698\n",
      "\ttrain loss: 0.5289995505408149\n",
      "\ttrain loss: 0.47791567048243633\n",
      "\ttrain loss: 0.7085124705453337\n",
      "\ttrain loss: 1.06397495450904\n",
      "\ttrain loss: 0.6196194168930789\n",
      "\ttrain loss: 0.5732068980082471\n",
      "\ttrain loss: 0.383948137435131\n",
      "\ttrain loss: 0.544354780596491\n",
      "\ttrain loss: 0.6895369552774997\n",
      "\ttrain loss: 0.5809240386870043\n",
      "\ttrain loss: 0.6769591525814648\n",
      "\ttrain loss: 0.43994881450834944\n",
      "\ttrain loss: 0.7121761263190098\n",
      "\ttrain loss: 0.5346465851980766\n",
      "\ttrain loss: 0.37557513440877055\n",
      "\ttrain loss: 0.6654736751232874\n",
      "\ttrain loss: 0.6494272463561452\n",
      "\ttrain loss: 0.5776611071021251\n",
      "\ttrain loss: 0.35460679720757793\n",
      "\ttrain loss: 0.6277878768743439\n",
      "\ttrain loss: 0.6012473751711531\n",
      "\ttrain loss: 0.6448198647565966\n",
      "\ttrain loss: 0.629384641412875\n",
      "\ttrain loss: 0.4944412964618812\n",
      "\ttrain loss: 0.41844470137786216\n",
      "\ttrain loss: 0.4750224059626661\n",
      "\ttrain loss: 0.39959769782915067\n",
      "\ttrain loss: 0.27847148706419583\n",
      "\ttrain loss: 0.43015888247940165\n",
      "\ttrain loss: 0.5182637657941686\n",
      "\ttrain loss: 0.6145658550145805\n",
      "\ttrain loss: 0.6770406401444076\n",
      "\ttrain loss: 0.690289030395606\n",
      "\ttrain loss: 0.448684631260423\n",
      "\ttrain loss: 0.66166290852791\n",
      "\ttrain loss: 0.33325136831914026\n",
      "\ttrain loss: 0.5577959371652463\n",
      "\ttrain loss: 0.45957280331196765\n",
      "\ttrain loss: 0.49011404364449834\n",
      "\ttrain loss: 0.48799135026629625\n",
      "\ttrain loss: 0.43594012447835134\n",
      "\ttrain loss: 0.648485149001143\n",
      "\ttrain loss: 0.5020855505016162\n",
      "\ttrain loss: 0.31193080890571856\n",
      "\ttrain loss: 0.47641545857362716\n",
      "\ttrain loss: 0.6071967818632923\n",
      "\ttrain loss: 0.4089811848150201\n",
      "\ttrain loss: 0.5011009132172877\n",
      "\ttrain loss: 0.7760461347378096\n",
      "\ttrain loss: 0.4054668804875578\n",
      "\ttrain loss: 0.6179205691415441\n",
      "\ttrain loss: 0.284240366520895\n",
      "\ttrain loss: 0.5495495175234961\n",
      "\ttrain loss: 0.7384503666668418\n",
      "\ttrain loss: 0.3701708660709432\n",
      "\ttrain loss: 0.7172306102782301\n",
      "\ttrain loss: 0.8001376526078261\n",
      "\ttrain loss: 0.5239180233832218\n",
      "\ttrain loss: 0.38472802208638784\n",
      "\ttrain loss: 0.5898230937007145\n",
      "\ttrain loss: 0.532059394268173\n",
      "\ttrain loss: 0.5788019535801681\n",
      "\ttrain loss: 0.5334311119139941\n",
      "\ttrain loss: 0.525002586481571\n",
      "\ttrain loss: 0.5037133596609459\n",
      "\ttrain loss: 0.6191716896224488\n",
      "\ttrain loss: 0.6138740500641522\n",
      "\ttrain loss: 0.4816902490471147\n",
      "\ttrain loss: 0.38479716371093126\n",
      "\ttrain loss: 0.4980486803410854\n",
      "\ttrain loss: 0.5480712784516899\n",
      "\ttrain loss: 0.5217334721894711\n",
      "\ttrain loss: 0.46760532901861385\n",
      "\ttrain loss: 0.459967779428162\n",
      "\ttrain loss: 0.47528294455448594\n",
      "\ttrain loss: 0.3691780624191094\n",
      "\ttrain loss: 0.5848979759205648\n",
      "\ttrain loss: 0.6689787221954699\n",
      "\ttrain loss: 0.5917780046529604\n",
      "\ttrain loss: 0.6199993411455011\n",
      "\ttrain loss: 0.26288960002624157\n",
      "\ttrain loss: 0.659871305950811\n",
      "\ttrain loss: 0.7164230975174112\n",
      "\ttrain loss: 0.5002537800233388\n",
      "\ttrain loss: 0.47065212911961507\n",
      "\ttrain loss: 0.43094490734036345\n",
      "\ttrain loss: 0.46839520776338506\n",
      "\ttrain loss: 0.7363164586613933\n",
      "\ttrain loss: 0.4349370795888552\n",
      "\ttrain loss: 0.7143942015233277\n",
      "\ttrain loss: 0.6359225874730174\n",
      "\ttrain loss: 0.41010883450145164\n",
      "\ttrain loss: 0.536302605452994\n",
      "\ttrain loss: 0.3777790705383164\n",
      "\ttrain loss: 0.5227827237463198\n",
      "\ttrain loss: 0.6297802881664403\n",
      "\ttrain loss: 0.5962775693289303\n",
      "\ttrain loss: 0.5281877640016537\n",
      "\ttrain loss: 0.6312033703155138\n",
      "\ttrain loss: 0.5538298505186225\n",
      "\ttrain loss: 0.5105034380440947\n",
      "\ttrain loss: 0.583934821222707\n",
      "\ttrain loss: 0.37722826557960387\n",
      "\ttrain loss: 0.3966221150327558\n",
      "\ttrain loss: 0.3299056822479028\n",
      "\ttrain loss: 0.5621870875951082\n",
      "\ttrain loss: 0.30398899245865274\n",
      "\ttrain loss: 0.5588329573373622\n",
      "\ttrain loss: 0.45770516745357626\n",
      "\ttrain loss: 0.6855106615239591\n",
      "\ttrain loss: 0.4830941690564875\n",
      "\ttrain loss: 0.5142594406584146\n",
      "\ttrain loss: 0.44714660590144006\n",
      "\ttrain loss: 0.47477977779747516\n",
      "\ttrain loss: 0.31324097907778314\n",
      "\ttrain loss: 0.45986891390011325\n",
      "\ttrain loss: 0.4703262749128384\n",
      "\ttrain loss: 0.47776525934680586\n",
      "\ttrain loss: 0.31523308686232543\n",
      "\ttrain loss: 0.4268467109229721\n",
      "\ttrain loss: 0.335178823124675\n",
      "\ttrain loss: 0.5518592826043178\n",
      "\ttrain loss: 0.5552433558248203\n",
      "\ttrain loss: 0.3561934696582381\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(1/15) is saved!\n",
      "\ttrain loss: 0.9599191804943927\n",
      "\ttrain loss: 0.32461459204985654\n",
      "\ttrain loss: 0.6010406952457299\n",
      "\ttrain loss: 0.48088168263355147\n",
      "\ttrain loss: 0.39677424567109376\n",
      "\ttrain loss: 0.4166077871068262\n",
      "\ttrain loss: 0.6653982080390892\n",
      "\ttrain loss: 0.3252577605186728\n",
      "\ttrain loss: 0.4424540980438001\n",
      "\ttrain loss: 0.2755262050225117\n",
      "\ttrain loss: 0.44487323733151607\n",
      "\ttrain loss: 0.5327418193990867\n",
      "\ttrain loss: 0.5001355494241935\n",
      "\ttrain loss: 0.4444796108926664\n",
      "\ttrain loss: 0.422380505192164\n",
      "\ttrain loss: 0.5228052129859089\n",
      "\ttrain loss: 0.30628193112163027\n",
      "\ttrain loss: 0.5242303035251764\n",
      "\ttrain loss: 0.25835281943929617\n",
      "\ttrain loss: 0.6399068392911345\n",
      "\ttrain loss: 0.4189757286948994\n",
      "\ttrain loss: 0.4335816468906366\n",
      "\ttrain loss: 0.5772662916950214\n",
      "\ttrain loss: 0.2738471566659491\n",
      "\ttrain loss: 0.30642120360266445\n",
      "\ttrain loss: 0.4760765475991293\n",
      "\ttrain loss: 0.6342029901483013\n",
      "\ttrain loss: 0.43277286542009685\n",
      "\ttrain loss: 0.5861058413112293\n",
      "\ttrain loss: 0.4563691350795742\n",
      "\ttrain loss: 0.5636495895184633\n",
      "\ttrain loss: 0.6015644729474645\n",
      "\ttrain loss: 0.4699796474274299\n",
      "\ttrain loss: 0.27100973116394406\n",
      "\ttrain loss: 0.6349799798347129\n",
      "\ttrain loss: 0.5900856389351428\n",
      "\ttrain loss: 0.2811533940518929\n",
      "\ttrain loss: 0.4872215976358736\n",
      "\ttrain loss: 0.5110666182021147\n",
      "\ttrain loss: 0.5002312240302268\n",
      "\ttrain loss: 0.5427000468883405\n",
      "\ttrain loss: 0.5233894966860413\n",
      "\ttrain loss: 0.49327211356011\n",
      "\ttrain loss: 0.6412977722028195\n",
      "\ttrain loss: 0.7031443030943019\n",
      "\ttrain loss: 0.35207618946178093\n",
      "\ttrain loss: 0.5783297453123478\n",
      "\ttrain loss: 0.6520086416588278\n",
      "\ttrain loss: 0.46413722066614027\n",
      "\ttrain loss: 0.44115433078407346\n",
      "\ttrain loss: 0.3003328169236237\n",
      "\ttrain loss: 0.5921821917938096\n",
      "\ttrain loss: 0.4192237989233566\n",
      "\ttrain loss: 0.7026989775049663\n",
      "\ttrain loss: 0.6175185655908354\n",
      "\ttrain loss: 0.6453598664936973\n",
      "\ttrain loss: 0.41770379605678953\n",
      "\ttrain loss: 0.40469059847673394\n",
      "\ttrain loss: 0.716681826296334\n",
      "\ttrain loss: 0.6055334111532291\n",
      "\ttrain loss: 0.6745721430578873\n",
      "\ttrain loss: 0.5449810248517497\n",
      "\ttrain loss: 0.5606877866694382\n",
      "\ttrain loss: 0.5858735305935177\n",
      "\ttrain loss: 0.42466017377751475\n",
      "\ttrain loss: 0.3356742336615948\n",
      "\ttrain loss: 0.3727329677589214\n",
      "\ttrain loss: 0.5646816492126542\n",
      "\ttrain loss: 0.5340998149997163\n",
      "\ttrain loss: 0.33708078549706955\n",
      "\ttrain loss: 0.5651601135249804\n",
      "\ttrain loss: 0.8167189324444941\n",
      "\ttrain loss: 0.45463356840169544\n",
      "\ttrain loss: 0.3569475710331912\n",
      "\ttrain loss: 0.47048327112330546\n",
      "\ttrain loss: 0.5314749564632254\n",
      "\ttrain loss: 0.36990870125697534\n",
      "\ttrain loss: 0.5356501805999492\n",
      "\ttrain loss: 0.33670844133592726\n",
      "\ttrain loss: 0.40539403078151476\n",
      "\ttrain loss: 0.5543552114993194\n",
      "\ttrain loss: 0.5475125952349762\n",
      "\ttrain loss: 0.3683092215532129\n",
      "\ttrain loss: 0.4496832943450155\n",
      "\ttrain loss: 0.7795816168484114\n",
      "\ttrain loss: 0.7865319700042042\n",
      "\ttrain loss: 0.5689262559278159\n",
      "\ttrain loss: 0.3128544550705983\n",
      "\ttrain loss: 0.4915064691584452\n",
      "\ttrain loss: 0.5890423641724032\n",
      "\ttrain loss: 0.6818265885496388\n",
      "\ttrain loss: 0.6943652873699204\n",
      "\ttrain loss: 0.5172143297429661\n",
      "\ttrain loss: 0.34432032834500853\n",
      "\ttrain loss: 0.26128014643371245\n",
      "\ttrain loss: 0.37127179886969275\n",
      "\ttrain loss: 0.5425790601029377\n",
      "\ttrain loss: 0.38747120215653197\n",
      "\ttrain loss: 0.4776472258796638\n",
      "\ttrain loss: 0.7426902484880689\n",
      "\ttrain loss: 0.37505994369468765\n",
      "\ttrain loss: 0.2505362697766852\n",
      "\ttrain loss: 0.7097879146110617\n",
      "\ttrain loss: 0.47809463214934556\n",
      "\ttrain loss: 0.37142051355631794\n",
      "\ttrain loss: 0.41377833932368424\n",
      "\ttrain loss: 0.5698407927005595\n",
      "\ttrain loss: 0.38478834787222527\n",
      "\ttrain loss: 0.4514567295440978\n",
      "\ttrain loss: 0.459292483782275\n",
      "\ttrain loss: 0.46179670036880177\n",
      "\ttrain loss: 0.516192322526063\n",
      "\ttrain loss: 0.3958763644220587\n",
      "\ttrain loss: 0.8103197371372899\n",
      "\ttrain loss: 0.6466295848169039\n",
      "\ttrain loss: 0.5118534320880703\n",
      "\ttrain loss: 0.5393892609548544\n",
      "\ttrain loss: 0.32502823143803294\n",
      "\ttrain loss: 0.4761932409262158\n",
      "\ttrain loss: 0.6613745029081362\n",
      "\ttrain loss: 0.4072762579626723\n",
      "\ttrain loss: 0.7381479164096034\n",
      "\ttrain loss: 0.35185524533326595\n",
      "\ttrain loss: 0.7006611844466022\n",
      "\ttrain loss: 0.6349411544552115\n",
      "\ttrain loss: 0.548037262900408\n",
      "\ttrain loss: 0.35437157679407044\n",
      "\ttrain loss: 0.41758295018023533\n",
      "\ttrain loss: 0.4687018125027482\n",
      "\ttrain loss: 0.5144536203550445\n",
      "\ttrain loss: 0.6597122691726576\n",
      "\ttrain loss: 0.3277057206862422\n",
      "\ttrain loss: 0.5822370062693742\n",
      "\ttrain loss: 0.4856223423313831\n",
      "\ttrain loss: 0.6486947263948262\n",
      "\ttrain loss: 0.6681374246264757\n",
      "\ttrain loss: 0.6189936016499508\n",
      "\ttrain loss: 0.47606520040991845\n",
      "\ttrain loss: 0.3942363987308848\n",
      "\ttrain loss: 0.5511546705934512\n",
      "\ttrain loss: 0.4184651900578287\n",
      "\ttrain loss: 0.6093695387446796\n",
      "\ttrain loss: 0.7284438160934754\n",
      "\ttrain loss: 0.39910410472759084\n",
      "\ttrain loss: 0.6414625455048912\n",
      "\ttrain loss: 0.5521530413216125\n",
      "\ttrain loss: 0.37205734506676386\n",
      "\ttrain loss: 0.6937865142605637\n",
      "\ttrain loss: 0.5587804409441416\n",
      "\ttrain loss: 0.815555074516669\n",
      "\ttrain loss: 0.8325117806946092\n",
      "\ttrain loss: 0.46339707487230875\n",
      "\ttrain loss: 0.7678765049679199\n",
      "\ttrain loss: 0.6672916135834801\n",
      "\ttrain loss: 0.2834257839076081\n",
      "\ttrain loss: 0.5569077531786911\n",
      "\ttrain loss: 0.374493455485007\n",
      "\ttrain loss: 0.34107660826501207\n",
      "\ttrain loss: 0.371589358249496\n",
      "\ttrain loss: 0.49373787992773255\n",
      "\ttrain loss: 0.45113386445862985\n",
      "\ttrain loss: 0.25811672753272696\n",
      "\ttrain loss: 0.34075675694200247\n",
      "\ttrain loss: 0.6126808536632343\n",
      "\ttrain loss: 0.4488283404025736\n",
      "\ttrain loss: 0.4360683057079044\n",
      "\ttrain loss: 0.4483014216917165\n",
      "\ttrain loss: 0.7668719156116308\n",
      "\ttrain loss: 0.48230669058427994\n",
      "\ttrain loss: 0.4459708832196745\n",
      "\ttrain loss: 0.35102319592899667\n",
      "\ttrain loss: 0.5181187629214274\n",
      "\ttrain loss: 0.5263524719381569\n",
      "\ttrain loss: 0.5148645195716478\n",
      "\ttrain loss: 0.6526671567242806\n",
      "\ttrain loss: 0.9789097570236185\n",
      "\ttrain loss: 0.5860014307682673\n",
      "\ttrain loss: 0.40627963897098196\n",
      "\ttrain loss: 0.28269890754335003\n",
      "\ttrain loss: 0.471713921059712\n",
      "\ttrain loss: 0.3155580010956636\n",
      "\ttrain loss: 0.42196424291352774\n",
      "\ttrain loss: 0.4960800937375119\n",
      "\ttrain loss: 0.5191890751720499\n",
      "\ttrain loss: 0.47464419858795437\n",
      "\ttrain loss: 0.3794240927996772\n",
      "\ttrain loss: 0.8091264635487132\n",
      "\ttrain loss: 0.2626010321311288\n",
      "\ttrain loss: 0.37760375533223434\n",
      "\ttrain loss: 0.3165702603365529\n",
      "\ttrain loss: 0.6025348779018229\n",
      "\ttrain loss: 0.6443374763546621\n",
      "\ttrain loss: 0.40480791754414647\n",
      "\ttrain loss: 0.5764249659795345\n",
      "\ttrain loss: 0.6398890216797046\n",
      "\ttrain loss: 0.6259869220796126\n",
      "\ttrain loss: 0.5961982490199309\n",
      "\ttrain loss: 0.5220726372524997\n",
      "\ttrain loss: 0.43577997365890164\n",
      "\ttrain loss: 0.3968466256226292\n",
      "\ttrain loss: 0.5219056321688862\n",
      "\ttrain loss: 0.37022351268441067\n",
      "\ttrain loss: 0.34656032532995007\n",
      "\ttrain loss: 0.762746198050274\n",
      "\ttrain loss: 1.0549771360913431\n",
      "\ttrain loss: 0.7458109663691306\n",
      "\ttrain loss: 0.46936375660964275\n",
      "\ttrain loss: 0.570432050449776\n",
      "\ttrain loss: 0.5722768925379881\n",
      "\ttrain loss: 0.729389894137245\n",
      "\ttrain loss: 0.7047799446962624\n",
      "\ttrain loss: 0.6071985376514657\n",
      "\ttrain loss: 0.6138885940414162\n",
      "\ttrain loss: 0.9258450659900583\n",
      "\ttrain loss: 0.3784643000823674\n",
      "\ttrain loss: 0.7294437600664796\n",
      "\ttrain loss: 0.6196503977193459\n",
      "\ttrain loss: 0.516887995367525\n",
      "\ttrain loss: 0.36029491343058734\n",
      "\ttrain loss: 0.5243008694654384\n",
      "\ttrain loss: 0.48922521987381296\n",
      "\ttrain loss: 0.5944512383043963\n",
      "\ttrain loss: 0.5053718421693939\n",
      "\ttrain loss: 0.5113404302544341\n",
      "\ttrain loss: 0.39599972702401554\n",
      "\ttrain loss: 0.2794501636048372\n",
      "\ttrain loss: 0.4012261889960985\n",
      "\ttrain loss: 0.48353660333799936\n",
      "\ttrain loss: 0.42752598227059313\n",
      "\ttrain loss: 0.6065360951542835\n",
      "\ttrain loss: 0.4321950773917823\n",
      "\ttrain loss: 0.40917572189397705\n",
      "\ttrain loss: 0.3866390673676871\n",
      "\ttrain loss: 0.42293790505353746\n",
      "\ttrain loss: 0.32639695676983727\n",
      "\ttrain loss: 0.4449041990697289\n",
      "\ttrain loss: 0.5198287107924925\n",
      "\ttrain loss: 0.5709268446490792\n",
      "\ttrain loss: 0.5411948581921109\n",
      "\ttrain loss: 0.5164970918915402\n",
      "\ttrain loss: 0.3793424163283884\n",
      "\ttrain loss: 0.6311466058495999\n",
      "\ttrain loss: 0.4371654173784166\n",
      "\ttrain loss: 0.52479977717649\n",
      "\ttrain loss: 0.6395963228152473\n",
      "\ttrain loss: 0.48144548959829336\n",
      "\ttrain loss: 0.534619808851722\n",
      "\ttrain loss: 0.38627829291376936\n",
      "\ttrain loss: 0.6974119962459242\n",
      "\ttrain loss: 0.4624070964716982\n",
      "\ttrain loss: 0.6292324983959463\n",
      "\ttrain loss: 0.458317011245432\n",
      "\ttrain loss: 0.43351157940690577\n",
      "\ttrain loss: 0.44548325172929537\n",
      "\ttrain loss: 0.7004933908146252\n",
      "\ttrain loss: 0.36660847970840654\n",
      "\ttrain loss: 0.5704092888511466\n",
      "\ttrain loss: 0.7101100664390086\n",
      "\ttrain loss: 0.39898926102932875\n",
      "\ttrain loss: 0.43253328941353675\n",
      "\ttrain loss: 0.4049576010182673\n",
      "\ttrain loss: 0.2659515753741016\n",
      "\ttrain loss: 0.3757191208671609\n",
      "\ttrain loss: 0.7407511446013508\n",
      "\ttrain loss: 0.3701048434711565\n",
      "\ttrain loss: 0.22088328957500877\n",
      "\ttrain loss: 0.39360349375687903\n",
      "\ttrain loss: 0.5760415027514073\n",
      "\ttrain loss: 0.4257046547736738\n",
      "\ttrain loss: 0.6835013907752954\n",
      "\ttrain loss: 0.4731615218220819\n",
      "\ttrain loss: 0.4277281284776855\n",
      "\ttrain loss: 0.5532183247625889\n",
      "\ttrain loss: 0.5862484447233646\n",
      "\ttrain loss: 0.32376625070547177\n",
      "\ttrain loss: 0.26667761054146605\n",
      "\ttrain loss: 0.33461682914589413\n",
      "\ttrain loss: 0.4259460635212805\n",
      "\ttrain loss: 0.4844437606792879\n",
      "\ttrain loss: 0.29747266545835016\n",
      "\ttrain loss: 0.5730350351854531\n",
      "\ttrain loss: 0.5521078511838242\n",
      "\ttrain loss: 1.0248139962602396\n",
      "\ttrain loss: 0.5115617944874484\n",
      "\ttrain loss: 0.49925302392166204\n",
      "\ttrain loss: 0.3709769822018365\n",
      "\ttrain loss: 0.5414239079424877\n",
      "\ttrain loss: 0.6217962006974244\n",
      "\ttrain loss: 0.46109967001927643\n",
      "\ttrain loss: 0.637943727200601\n",
      "\ttrain loss: 0.7870574710109536\n",
      "\ttrain loss: 0.6177202336522308\n",
      "\ttrain loss: 0.7579662650487938\n",
      "\ttrain loss: 0.42421670396092226\n",
      "\ttrain loss: 0.5143420912405406\n",
      "\ttrain loss: 0.5945829040599866\n",
      "\ttrain loss: 0.608086917647917\n",
      "\ttrain loss: 0.6022758843123622\n",
      "\ttrain loss: 0.6469657129544244\n",
      "\ttrain loss: 0.61918393832814\n",
      "\ttrain loss: 0.4765285821938662\n",
      "\ttrain loss: 0.5132977851664101\n",
      "\ttrain loss: 0.7420580732872959\n",
      "\ttrain loss: 0.3627238306170071\n",
      "\ttrain loss: 0.8845955332303673\n",
      "\ttrain loss: 0.7428903665724269\n",
      "\ttrain loss: 0.6720933943914034\n",
      "\ttrain loss: 0.3312861819446052\n",
      "\ttrain loss: 0.5721965546100172\n",
      "\ttrain loss: 0.7185536205918234\n",
      "\ttrain loss: 0.68043905784517\n",
      "\ttrain loss: 0.39177532601683035\n",
      "\ttrain loss: 0.4229701057959687\n",
      "\ttrain loss: 0.6856498804089297\n",
      "\ttrain loss: 0.40526987945192317\n",
      "\ttrain loss: 0.510920925342393\n",
      "\ttrain loss: 0.8890672165448116\n",
      "\ttrain loss: 0.36723365202122904\n",
      "\ttrain loss: 0.244150375475023\n",
      "\ttrain loss: 0.5001988309666858\n",
      "\ttrain loss: 0.4884669029091208\n",
      "\ttrain loss: 0.4162667150800956\n",
      "\ttrain loss: 0.38849556026660065\n",
      "\ttrain loss: 0.27208728586215297\n",
      "\ttrain loss: 0.2876968430536865\n",
      "\ttrain loss: 0.504596870798266\n",
      "\ttrain loss: 0.46540412348778215\n",
      "\ttrain loss: 0.46642784175654645\n",
      "\ttrain loss: 0.41769146995606443\n",
      "\ttrain loss: 0.2677389576492537\n",
      "\ttrain loss: 0.48371975704800435\n",
      "\ttrain loss: 0.36511662935718603\n",
      "\ttrain loss: 0.8775040469641076\n",
      "\ttrain loss: 0.46497628646352174\n",
      "\ttrain loss: 0.42693485366182937\n",
      "\ttrain loss: 0.4011908858271088\n",
      "\ttrain loss: 0.3376629389676626\n",
      "\ttrain loss: 0.3864356827491717\n",
      "\ttrain loss: 0.285910678858486\n",
      "\ttrain loss: 0.8847519553506853\n",
      "\ttrain loss: 0.8043553773798628\n",
      "\ttrain loss: 0.44878377140831366\n",
      "\ttrain loss: 0.4596314414312499\n",
      "\ttrain loss: 0.32754844391062465\n",
      "\ttrain loss: 0.43349823029900914\n",
      "\ttrain loss: 0.785176428674994\n",
      "\ttrain loss: 0.4686511751250359\n",
      "\ttrain loss: 0.4567919312237744\n",
      "\ttrain loss: 0.3094272901263247\n",
      "\ttrain loss: 0.4503979986638256\n",
      "\ttrain loss: 0.7626206849174524\n",
      "\ttrain loss: 0.6037015825945342\n",
      "\ttrain loss: 0.520356415792004\n",
      "\ttrain loss: 0.43123307434292074\n",
      "\ttrain loss: 0.7093181304940284\n",
      "\ttrain loss: 0.5711551347899382\n",
      "\ttrain loss: 0.5036138465294758\n",
      "\ttrain loss: 0.3658777845203809\n",
      "\ttrain loss: 0.39822914977441826\n",
      "\ttrain loss: 0.5074037014082131\n",
      "\ttrain loss: 0.32330314873877286\n",
      "\ttrain loss: 0.40691048917428796\n",
      "\ttrain loss: 0.30608637723088106\n",
      "\ttrain loss: 0.35538919959646387\n",
      "\ttrain loss: 0.3417677976118968\n",
      "\ttrain loss: 0.5330094849749218\n",
      "\ttrain loss: 0.6392239369938397\n",
      "\ttrain loss: 0.4854934928053171\n",
      "\ttrain loss: 0.4805340455131244\n",
      "\ttrain loss: 0.6747875654524329\n",
      "\ttrain loss: 0.41022511458847083\n",
      "\ttrain loss: 0.28138163979425557\n",
      "\ttrain loss: 0.47096915785351734\n",
      "\ttrain loss: 0.47673295772706425\n",
      "\ttrain loss: 0.4427111678223414\n",
      "\ttrain loss: 0.29815339039403554\n",
      "\ttrain loss: 0.4358090340763899\n",
      "\ttrain loss: 0.4036766657823777\n",
      "\ttrain loss: 0.49605321812538483\n",
      "\ttrain loss: 0.38999289893438144\n",
      "\ttrain loss: 0.44672153971384576\n",
      "\ttrain loss: 0.5908088677338659\n",
      "\ttrain loss: 0.45976570748981926\n",
      "\ttrain loss: 0.5298751192890716\n",
      "\ttrain loss: 0.5781193635110776\n",
      "\ttrain loss: 0.6761483267282857\n",
      "\ttrain loss: 0.3594948853954826\n",
      "\ttrain loss: 0.5692681116959762\n",
      "\ttrain loss: 0.5810862453090555\n",
      "\ttrain loss: 0.376066432276405\n",
      "\ttrain loss: 0.44132087089911\n",
      "\ttrain loss: 0.39148700692438715\n",
      "\ttrain loss: 0.6273311612091945\n",
      "\ttrain loss: 0.5469790440208832\n",
      "\ttrain loss: 0.3120062071258751\n",
      "\ttrain loss: 0.620288886492084\n",
      "\ttrain loss: 0.4264285737586356\n",
      "\ttrain loss: 0.37230070748707533\n",
      "\ttrain loss: 0.5997514287544503\n",
      "\ttrain loss: 0.3633413776017708\n",
      "\ttrain loss: 0.3947065548826294\n",
      "\ttrain loss: 0.2722057658928325\n",
      "\ttrain loss: 0.3966353209358186\n",
      "\ttrain loss: 0.501105320524001\n",
      "\ttrain loss: 0.40618049654587873\n",
      "\ttrain loss: 0.2387673023912257\n",
      "\ttrain loss: 0.4719852905188506\n",
      "\ttrain loss: 0.72406990276997\n",
      "\ttrain loss: 0.578714088924132\n",
      "\ttrain loss: 0.47336602478307976\n",
      "\ttrain loss: 0.6251224705548462\n",
      "\ttrain loss: 0.472685747877851\n",
      "\ttrain loss: 0.46474710977477685\n",
      "\ttrain loss: 0.7029495531453716\n",
      "\ttrain loss: 0.5548937167324282\n",
      "\ttrain loss: 0.6452496753409824\n",
      "\ttrain loss: 0.4550130534786879\n",
      "\ttrain loss: 0.3619834869997921\n",
      "\ttrain loss: 0.5279522306591077\n",
      "\ttrain loss: 0.5060642598761169\n",
      "\ttrain loss: 0.4695773967389105\n",
      "\ttrain loss: 0.5550571754494393\n",
      "\ttrain loss: 0.46893453564619847\n",
      "\ttrain loss: 0.5321610406335218\n",
      "\ttrain loss: 0.5411066794388244\n",
      "\ttrain loss: 0.7973558825401827\n",
      "\ttrain loss: 0.5925596925463058\n",
      "\ttrain loss: 0.2876031105228836\n",
      "\ttrain loss: 0.2797016063623114\n",
      "\ttrain loss: 0.30953880610564244\n",
      "\ttrain loss: 0.5981757240552015\n",
      "\ttrain loss: 0.3811870702635894\n",
      "\ttrain loss: 0.3973429899979376\n",
      "\ttrain loss: 0.45522372774471165\n",
      "\ttrain loss: 0.39606444587339196\n",
      "\ttrain loss: 0.6451523194462148\n",
      "\ttrain loss: 0.3518130151041936\n",
      "\ttrain loss: 0.3998455348445892\n",
      "\ttrain loss: 0.3931666181279841\n",
      "\ttrain loss: 0.6758457639052158\n",
      "\ttrain loss: 0.39059058645967004\n",
      "\ttrain loss: 0.5731268947403718\n",
      "\ttrain loss: 0.410478340481067\n",
      "\ttrain loss: 0.22558356545150487\n",
      "\ttrain loss: 0.43896285550896647\n",
      "\ttrain loss: 0.6027971621070558\n",
      "\ttrain loss: 0.43117766723050954\n",
      "\ttrain loss: 0.3719031587747679\n",
      "\ttrain loss: 0.4448601557473572\n",
      "\ttrain loss: 0.4118400820407735\n",
      "\ttrain loss: 0.4732332888346968\n",
      "\ttrain loss: 0.34745214260658686\n",
      "\ttrain loss: 0.4972793555346816\n",
      "\ttrain loss: 0.5176908279830233\n",
      "\ttrain loss: 0.36994386491468084\n",
      "\ttrain loss: 0.5401819797934946\n",
      "\ttrain loss: 0.6177886317897506\n",
      "\ttrain loss: 0.7232133200238225\n",
      "\ttrain loss: 0.5478759105666617\n",
      "\ttrain loss: 0.6529459476731504\n",
      "\ttrain loss: 0.44207780980742867\n",
      "\ttrain loss: 0.41471037123003457\n",
      "\ttrain loss: 0.6225303637159312\n",
      "\ttrain loss: 0.5157067185095463\n",
      "\ttrain loss: 0.43174308242010717\n",
      "\ttrain loss: 0.5332177054872249\n",
      "\ttrain loss: 0.5285537369134667\n",
      "\ttrain loss: 0.3156705281530582\n",
      "\ttrain loss: 0.45407892036584685\n",
      "\ttrain loss: 0.2962863154527335\n",
      "\ttrain loss: 0.33972599556631344\n",
      "\ttrain loss: 0.5286417514042714\n",
      "\ttrain loss: 0.7803271278265931\n",
      "\ttrain loss: 0.5631838866048473\n",
      "\ttrain loss: 0.8927990292635314\n",
      "\ttrain loss: 0.45877296412943347\n",
      "\ttrain loss: 0.3194897729874425\n",
      "\ttrain loss: 0.3239234749634443\n",
      "\ttrain loss: 0.4824386766443952\n",
      "\ttrain loss: 0.5051544585452583\n",
      "\ttrain loss: 0.7091685318370913\n",
      "\ttrain loss: 0.3111231985274785\n",
      "\ttrain loss: 0.27852042048683645\n",
      "\ttrain loss: 0.3982205467480149\n",
      "\ttrain loss: 0.4992442265968938\n",
      "\ttrain loss: 0.46228981021959237\n",
      "\ttrain loss: 0.6241269672046202\n",
      "\ttrain loss: 0.6470085329242439\n",
      "\ttrain loss: 0.5375723720561401\n",
      "\ttrain loss: 0.5837303330818591\n",
      "\ttrain loss: 0.4241058678293379\n",
      "\ttrain loss: 0.6035354254140499\n",
      "\ttrain loss: 0.4315597064851915\n",
      "\ttrain loss: 0.44724492691162665\n",
      "\ttrain loss: 0.40908489675665993\n",
      "\ttrain loss: 0.32377728891888025\n",
      "\ttrain loss: 0.6053531555151288\n",
      "\ttrain loss: 0.35971854023671435\n",
      "\ttrain loss: 0.38243540372738044\n",
      "\ttrain loss: 0.6851851946302454\n",
      "\ttrain loss: 0.5153804676134985\n",
      "\ttrain loss: 0.49541562864115035\n",
      "\ttrain loss: 0.3141427611709401\n",
      "\ttrain loss: 0.5490719943230509\n",
      "\ttrain loss: 0.4645045934671419\n",
      "\ttrain loss: 0.42164849036195484\n",
      "\ttrain loss: 0.531523464173959\n",
      "\ttrain loss: 0.3544754367519273\n",
      "\ttrain loss: 0.319894439960531\n",
      "\ttrain loss: 0.42187672461756676\n",
      "\ttrain loss: 0.5823706106698086\n",
      "\ttrain loss: 0.3241671356057006\n",
      "\ttrain loss: 0.434102736640491\n",
      "\ttrain loss: 0.6643396827230627\n",
      "\ttrain loss: 0.4131357345187855\n",
      "\ttrain loss: 0.6449536301912209\n",
      "\ttrain loss: 0.3402802334202822\n",
      "\ttrain loss: 0.5360475751527011\n",
      "\ttrain loss: 0.4577995759318563\n",
      "\ttrain loss: 0.4125838964823093\n",
      "\ttrain loss: 0.42668248906378525\n",
      "\ttrain loss: 0.38569702534893013\n",
      "\ttrain loss: 0.4691614904049431\n",
      "\ttrain loss: 0.5823756288848435\n",
      "\ttrain loss: 0.4232385774442995\n",
      "\ttrain loss: 0.3811198736055998\n",
      "\ttrain loss: 0.4945839987023143\n",
      "\ttrain loss: 0.3291990771468809\n",
      "\ttrain loss: 0.32087874121488547\n",
      "\ttrain loss: 0.6153343298450377\n",
      "\ttrain loss: 0.45634584819727064\n",
      "\ttrain loss: 0.5641731167964327\n",
      "\ttrain loss: 0.5096078314771111\n",
      "\ttrain loss: 0.6201294388855145\n",
      "\ttrain loss: 0.3795565970735283\n",
      "\ttrain loss: 0.5648079843512206\n",
      "\ttrain loss: 0.49770602870307196\n",
      "\ttrain loss: 0.6184974076871812\n",
      "\ttrain loss: 0.637933589505279\n",
      "\ttrain loss: 0.25875456959671767\n",
      "\ttrain loss: 0.371214665975909\n",
      "\ttrain loss: 0.5308937223254715\n",
      "\ttrain loss: 0.5186683055110656\n",
      "\ttrain loss: 0.319332595807169\n",
      "\ttrain loss: 0.2956305766765019\n",
      "\ttrain loss: 0.3916442646574261\n",
      "\ttrain loss: 0.39971482567529787\n",
      "\ttrain loss: 0.5717027028023656\n",
      "\ttrain loss: 0.43739735027775284\n",
      "\ttrain loss: 0.5489530618157028\n",
      "\ttrain loss: 0.5164089186535984\n",
      "\ttrain loss: 0.40971360716736754\n",
      "\ttrain loss: 0.3116399500455131\n",
      "\ttrain loss: 0.5348330225687528\n",
      "\ttrain loss: 0.44372752629090606\n",
      "\ttrain loss: 0.5257666639878964\n",
      "\ttrain loss: 0.384887839294473\n",
      "\ttrain loss: 0.34997407759991633\n",
      "\ttrain loss: 0.3784737212267011\n",
      "\ttrain loss: 0.5137146340688559\n",
      "\ttrain loss: 0.4853428912252307\n",
      "\ttrain loss: 0.49823160351260903\n",
      "\ttrain loss: 0.43694705520681765\n",
      "\ttrain loss: 0.8749783256718839\n",
      "\ttrain loss: 0.552559956111334\n",
      "\ttrain loss: 0.5597820219721346\n",
      "\ttrain loss: 0.35722030471490196\n",
      "\ttrain loss: 0.39391628172584253\n",
      "\ttrain loss: 0.4366211687374826\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(2/15) is saved!\n",
      "\ttrain loss: 0.45528769622757076\n",
      "\ttrain loss: 0.5507064834549165\n",
      "\ttrain loss: 0.5929712754469767\n",
      "\ttrain loss: 0.4762954769571103\n",
      "\ttrain loss: 0.33786818306700195\n",
      "\ttrain loss: 0.40998197811839965\n",
      "\ttrain loss: 0.4008530152169668\n",
      "\ttrain loss: 0.6951872503209928\n",
      "\ttrain loss: 0.3793168178065117\n",
      "\ttrain loss: 0.4319646214162738\n",
      "\ttrain loss: 0.3296831719637635\n",
      "\ttrain loss: 0.422891848028216\n",
      "\ttrain loss: 0.45507387488267165\n",
      "\ttrain loss: 0.5085281804597634\n",
      "\ttrain loss: 0.38726653189261195\n",
      "\ttrain loss: 0.5014778129226647\n",
      "\ttrain loss: 0.21997481933753404\n",
      "\ttrain loss: 0.5147362666236878\n",
      "\ttrain loss: 0.440655752122456\n",
      "\ttrain loss: 0.5099556784595815\n",
      "\ttrain loss: 0.34462344179554505\n",
      "\ttrain loss: 0.6667656837676359\n",
      "\ttrain loss: 0.7082673808494829\n",
      "\ttrain loss: 0.5016332783676009\n",
      "\ttrain loss: 0.34566991159647104\n",
      "\ttrain loss: 0.5951581990395914\n",
      "\ttrain loss: 0.4508866091297524\n",
      "\ttrain loss: 0.36146564466720477\n",
      "\ttrain loss: 0.5533191764681722\n",
      "\ttrain loss: 0.3626480988426179\n",
      "\ttrain loss: 0.623721325313987\n",
      "\ttrain loss: 0.5547484465917838\n",
      "\ttrain loss: 0.43615077079286396\n",
      "\ttrain loss: 0.4985915488387779\n",
      "\ttrain loss: 0.4801693200046023\n",
      "\ttrain loss: 0.3321658097530366\n",
      "\ttrain loss: 0.3236580197682623\n",
      "\ttrain loss: 0.35796742018441485\n",
      "\ttrain loss: 0.5085957091765224\n",
      "\ttrain loss: 0.4358816091471729\n",
      "\ttrain loss: 0.7383696354845846\n",
      "\ttrain loss: 0.5041996287216267\n",
      "\ttrain loss: 0.5082925724755292\n",
      "\ttrain loss: 0.6265708706923739\n",
      "\ttrain loss: 0.45081007056622446\n",
      "\ttrain loss: 0.4746687726029496\n",
      "\ttrain loss: 0.3589091669364812\n",
      "\ttrain loss: 0.33520920798736087\n",
      "\ttrain loss: 0.4011669482158318\n",
      "\ttrain loss: 0.4193606957779603\n",
      "\ttrain loss: 0.5883645992801925\n",
      "\ttrain loss: 0.46520327279183327\n",
      "\ttrain loss: 0.5488746569059899\n",
      "\ttrain loss: 0.6203660727350029\n",
      "\ttrain loss: 0.5745356727133806\n",
      "\ttrain loss: 0.33639228197643545\n",
      "\ttrain loss: 0.7346853305972981\n",
      "\ttrain loss: 0.5080921331285619\n",
      "\ttrain loss: 0.5850837485741011\n",
      "\ttrain loss: 0.37188539952978816\n",
      "\ttrain loss: 0.5281104220951939\n",
      "\ttrain loss: 0.31274071273971765\n",
      "\ttrain loss: 0.43137320932557127\n",
      "\ttrain loss: 0.48682821223454725\n",
      "\ttrain loss: 0.5776490357630363\n",
      "\ttrain loss: 0.35500421450977626\n",
      "\ttrain loss: 0.3497399872495499\n",
      "\ttrain loss: 0.43783375515374545\n",
      "\ttrain loss: 0.4901373946669121\n",
      "\ttrain loss: 0.38767443331858026\n",
      "\ttrain loss: 0.38602805023121023\n",
      "\ttrain loss: 0.5266452640035275\n",
      "\ttrain loss: 0.3268905885092024\n",
      "\ttrain loss: 0.3138796342536919\n",
      "\ttrain loss: 0.4210559412531869\n",
      "\ttrain loss: 0.3774013781275795\n",
      "\ttrain loss: 0.7832223791116512\n",
      "\ttrain loss: 0.44831599331738214\n",
      "\ttrain loss: 0.4418985738791622\n",
      "\ttrain loss: 0.4759790357023206\n",
      "\ttrain loss: 0.6623992155480934\n",
      "\ttrain loss: 0.4086698110817497\n",
      "\ttrain loss: 0.4687066125719259\n",
      "\ttrain loss: 0.26831300105211\n",
      "\ttrain loss: 0.3498643082024107\n",
      "\ttrain loss: 0.3412855098319255\n",
      "\ttrain loss: 0.4086972160023714\n",
      "\ttrain loss: 0.3422853968792803\n",
      "\ttrain loss: 0.633513914802974\n",
      "\ttrain loss: 0.37165144542120354\n",
      "\ttrain loss: 0.7899007176925219\n",
      "\ttrain loss: 0.44000185844340567\n",
      "\ttrain loss: 0.40037716566076637\n",
      "\ttrain loss: 0.5261301853494083\n",
      "\ttrain loss: 0.7041422934666717\n",
      "\ttrain loss: 0.46003857803154946\n",
      "\ttrain loss: 0.32643308750904354\n",
      "\ttrain loss: 0.32743085082129225\n",
      "\ttrain loss: 0.3264787252833292\n",
      "\ttrain loss: 0.44833182703054664\n",
      "\ttrain loss: 0.4322735781489879\n",
      "\ttrain loss: 0.45679203464749496\n",
      "\ttrain loss: 0.36566689984840123\n",
      "\ttrain loss: 0.41931929615924673\n",
      "\ttrain loss: 0.4156800692226016\n",
      "\ttrain loss: 0.5273739293467871\n",
      "\ttrain loss: 0.464783158804218\n",
      "\ttrain loss: 0.37347125968846273\n",
      "\ttrain loss: 0.3937534604384385\n",
      "\ttrain loss: 0.5639069778344705\n",
      "\ttrain loss: 0.5089729650328783\n",
      "\ttrain loss: 0.5835025216694107\n",
      "\ttrain loss: 0.6882332730591598\n",
      "\ttrain loss: 0.5636745388773519\n",
      "\ttrain loss: 0.2972789209852241\n",
      "\ttrain loss: 0.7684658039550152\n",
      "\ttrain loss: 0.6276447889940362\n",
      "\ttrain loss: 0.47222592805109953\n",
      "\ttrain loss: 0.30418184830625944\n",
      "\ttrain loss: 0.4275617862528044\n",
      "\ttrain loss: 0.43643372980473055\n",
      "\ttrain loss: 0.6409272182722068\n",
      "\ttrain loss: 0.4724232588566848\n",
      "\ttrain loss: 0.3290492223569739\n",
      "\ttrain loss: 0.6676179692059252\n",
      "\ttrain loss: 0.44570973699093125\n",
      "\ttrain loss: 0.44270356568792224\n",
      "\ttrain loss: 0.5539685531561986\n",
      "\ttrain loss: 0.3090289905955377\n",
      "\ttrain loss: 0.369430492957872\n",
      "\ttrain loss: 0.31928483705350724\n",
      "\ttrain loss: 0.48544494448053055\n",
      "\ttrain loss: 0.5295859208057623\n",
      "\ttrain loss: 0.610534861549325\n",
      "\ttrain loss: 0.3630849150565762\n",
      "\ttrain loss: 0.3818856813213568\n",
      "\ttrain loss: 0.5738576328305502\n",
      "\ttrain loss: 0.39742843669346894\n",
      "\ttrain loss: 0.5427789948839473\n",
      "\ttrain loss: 0.43753341069162466\n",
      "\ttrain loss: 0.4735124802674963\n",
      "\ttrain loss: 0.3609644170566594\n",
      "\ttrain loss: 0.45430745844210607\n",
      "\ttrain loss: 0.5158481559869672\n",
      "\ttrain loss: 0.48713964685163585\n",
      "\ttrain loss: 0.545150199864104\n",
      "\ttrain loss: 0.465758396391743\n",
      "\ttrain loss: 0.3890150599308409\n",
      "\ttrain loss: 0.48063080088224963\n",
      "\ttrain loss: 0.6385024867626126\n",
      "\ttrain loss: 0.3641431751326204\n",
      "\ttrain loss: 0.5424328287433555\n",
      "\ttrain loss: 0.5773725800197127\n",
      "\ttrain loss: 0.42117893409441387\n",
      "\ttrain loss: 0.4800402728591851\n",
      "\ttrain loss: 0.4230952853562623\n",
      "\ttrain loss: 0.7740731883410481\n",
      "\ttrain loss: 0.5730739835875878\n",
      "\ttrain loss: 0.4765979143421703\n",
      "\ttrain loss: 0.4973798012477155\n",
      "\ttrain loss: 0.5484001528943581\n",
      "\ttrain loss: 0.6521499325235377\n",
      "\ttrain loss: 0.6177244742457293\n",
      "\ttrain loss: 0.7958401424931012\n",
      "\ttrain loss: 0.3580583582069429\n",
      "\ttrain loss: 0.4704637960667319\n",
      "\ttrain loss: 0.28033954662310684\n",
      "\ttrain loss: 0.5985897766225315\n",
      "\ttrain loss: 0.3602490554192661\n",
      "\ttrain loss: 0.30886321144271484\n",
      "\ttrain loss: 0.35075214380755393\n",
      "\ttrain loss: 0.55918211838368\n",
      "\ttrain loss: 0.2415075380492617\n",
      "\ttrain loss: 0.6401637446584905\n",
      "\ttrain loss: 0.361406894849869\n",
      "\ttrain loss: 0.32290307388530903\n",
      "\ttrain loss: 0.43534003289404644\n",
      "\ttrain loss: 0.45084570999291856\n",
      "\ttrain loss: 0.3698479526778357\n",
      "\ttrain loss: 0.3977279330901312\n",
      "\ttrain loss: 0.4357008171614745\n",
      "\ttrain loss: 0.570836523749411\n",
      "\ttrain loss: 0.4246391340592406\n",
      "\ttrain loss: 0.5745529383592523\n",
      "\ttrain loss: 0.5361923888672038\n",
      "\ttrain loss: 0.3074370451929237\n",
      "\ttrain loss: 0.49164188334679676\n",
      "\ttrain loss: 0.49299338864473763\n",
      "\ttrain loss: 0.5617849263134498\n",
      "\ttrain loss: 0.514082027541125\n",
      "\ttrain loss: 0.6187887727610405\n",
      "\ttrain loss: 0.42110979211898286\n",
      "\ttrain loss: 0.5434000095140665\n",
      "\ttrain loss: 0.4099200877944865\n",
      "\ttrain loss: 0.6542689066555708\n",
      "\ttrain loss: 0.5488473407103109\n",
      "\ttrain loss: 0.49784234047209036\n",
      "\ttrain loss: 0.2936765838006994\n",
      "\ttrain loss: 0.6163308531146208\n",
      "\ttrain loss: 0.4404837547140516\n",
      "\ttrain loss: 0.37561276543796585\n",
      "\ttrain loss: 0.5223732618350198\n",
      "\ttrain loss: 0.7606669810852738\n",
      "\ttrain loss: 0.3568491454101192\n",
      "\ttrain loss: 0.8421960609739881\n",
      "\ttrain loss: 0.4365459511047645\n",
      "\ttrain loss: 0.45560815111913694\n",
      "\ttrain loss: 0.33145483610182\n",
      "\ttrain loss: 0.48481617317006037\n",
      "\ttrain loss: 0.6582960310348329\n",
      "\ttrain loss: 0.5889299307881828\n",
      "\ttrain loss: 0.5374218674500475\n",
      "\ttrain loss: 0.31197354922193776\n",
      "\ttrain loss: 0.494127765721689\n",
      "\ttrain loss: 0.37324515636942\n",
      "\ttrain loss: 0.4944698571260372\n",
      "\ttrain loss: 0.4558635952076158\n",
      "\ttrain loss: 0.4692321223716717\n",
      "\ttrain loss: 0.20027452783238703\n",
      "\ttrain loss: 0.38055214465039044\n",
      "\ttrain loss: 0.2961275266084571\n",
      "\ttrain loss: 0.5530425400959017\n",
      "\ttrain loss: 0.478972909981812\n",
      "\ttrain loss: 0.5069810749877388\n",
      "\ttrain loss: 0.40423489984204813\n",
      "\ttrain loss: 0.27698919658042903\n",
      "\ttrain loss: 0.3944281338951442\n",
      "\ttrain loss: 0.45865490653631\n",
      "\ttrain loss: 0.42830685224772214\n",
      "\ttrain loss: 0.44712984524961785\n",
      "\ttrain loss: 0.48012543559707066\n",
      "\ttrain loss: 0.8116659483828119\n",
      "\ttrain loss: 0.2767093516036558\n",
      "\ttrain loss: 0.5105759125390305\n",
      "\ttrain loss: 0.4063073815367451\n",
      "\ttrain loss: 0.5461223852844321\n",
      "\ttrain loss: 0.567378006043449\n",
      "\ttrain loss: 0.4725134254737184\n",
      "\ttrain loss: 0.4148371555240918\n",
      "\ttrain loss: 0.3803831127377176\n",
      "\ttrain loss: 0.4003616172515289\n",
      "\ttrain loss: 0.45811521525194776\n",
      "\ttrain loss: 0.7747049778733197\n",
      "\ttrain loss: 0.1972674283728559\n",
      "\ttrain loss: 0.3917877580266784\n",
      "\ttrain loss: 0.4190414551899577\n",
      "\ttrain loss: 0.43313938574602096\n",
      "\ttrain loss: 0.42979623568095976\n",
      "\ttrain loss: 0.5610970113045352\n",
      "\ttrain loss: 0.4152709041809105\n",
      "\ttrain loss: 0.4424476988935253\n",
      "\ttrain loss: 0.3407676664411189\n",
      "\ttrain loss: 0.664639709898999\n",
      "\ttrain loss: 0.4543834485290884\n",
      "\ttrain loss: 0.3801865570282069\n",
      "\ttrain loss: 0.7703189337851164\n",
      "\ttrain loss: 0.43890215910948527\n",
      "\ttrain loss: 0.5885754430815464\n",
      "\ttrain loss: 0.6536901108502446\n",
      "\ttrain loss: 0.4001181140140782\n",
      "\ttrain loss: 0.3514831555037547\n",
      "\ttrain loss: 0.3413652268343073\n",
      "\ttrain loss: 0.49381538508529\n",
      "\ttrain loss: 0.38877372657324155\n",
      "\ttrain loss: 0.5512663665919777\n",
      "\ttrain loss: 0.45384925354996125\n",
      "\ttrain loss: 0.3505072207069212\n",
      "\ttrain loss: 0.47096442713133907\n",
      "\ttrain loss: 0.5728581383229278\n",
      "\ttrain loss: 0.453140479999155\n",
      "\ttrain loss: 0.3454631276250013\n",
      "\ttrain loss: 0.470245911489085\n",
      "\ttrain loss: 0.42883593147072757\n",
      "\ttrain loss: 0.3961731172680348\n",
      "\ttrain loss: 0.4114336287013382\n",
      "\ttrain loss: 0.34573606673270013\n",
      "\ttrain loss: 0.5716970806165222\n",
      "\ttrain loss: 0.3351477126151927\n",
      "\ttrain loss: 0.4544366135418227\n",
      "\ttrain loss: 0.4693953995161646\n",
      "\ttrain loss: 0.5333182329080379\n",
      "\ttrain loss: 0.7221442271016782\n",
      "\ttrain loss: 0.4460871225624328\n",
      "\ttrain loss: 0.5270379018323474\n",
      "\ttrain loss: 0.3323263012350932\n",
      "\ttrain loss: 0.49202985594491927\n",
      "\ttrain loss: 0.40842251562540954\n",
      "\ttrain loss: 0.41745705947687284\n",
      "\ttrain loss: 0.45001150431809334\n",
      "\ttrain loss: 0.5295175226195894\n",
      "\ttrain loss: 0.5573131079335987\n",
      "\ttrain loss: 0.44339151863050047\n",
      "\ttrain loss: 0.4896159447932076\n",
      "\ttrain loss: 0.5597352512650627\n",
      "\ttrain loss: 0.4463754371311691\n",
      "\ttrain loss: 0.4960445301784439\n",
      "\ttrain loss: 0.508033409388095\n",
      "\ttrain loss: 0.5185083343417523\n",
      "\ttrain loss: 0.501165672282897\n",
      "\ttrain loss: 0.3713052381507319\n",
      "\ttrain loss: 0.6779576319685853\n",
      "\ttrain loss: 0.4598153959969994\n",
      "\ttrain loss: 0.6803833264997752\n",
      "\ttrain loss: 0.5520672374394886\n",
      "\ttrain loss: 0.5038113123774316\n",
      "\ttrain loss: 0.7890578904356103\n",
      "\ttrain loss: 0.35143153642650876\n",
      "\ttrain loss: 0.6970526669046062\n",
      "\ttrain loss: 0.3146466716857653\n",
      "\ttrain loss: 0.4997304208234369\n",
      "\ttrain loss: 0.40303152428209266\n",
      "\ttrain loss: 0.3557889743010444\n",
      "\ttrain loss: 0.5703724298729822\n",
      "\ttrain loss: 0.32760371385111137\n",
      "\ttrain loss: 0.4499715007642071\n",
      "\ttrain loss: 0.5277031980249192\n",
      "\ttrain loss: 0.4819795986641011\n",
      "\ttrain loss: 0.4069363278509057\n",
      "\ttrain loss: 0.4346974284325971\n",
      "\ttrain loss: 0.46391575808887\n",
      "\ttrain loss: 0.6538494131173482\n",
      "\ttrain loss: 0.5848947283233283\n",
      "\ttrain loss: 0.3757527747661249\n",
      "\ttrain loss: 0.5687090548769371\n",
      "\ttrain loss: 0.38139790759352005\n",
      "\ttrain loss: 0.5038057849453965\n",
      "\ttrain loss: 0.4462976861614204\n",
      "\ttrain loss: 0.4534243591011918\n",
      "\ttrain loss: 0.46856836199369495\n",
      "\ttrain loss: 0.3890584624279559\n",
      "\ttrain loss: 0.7001732837743818\n",
      "\ttrain loss: 0.6269665126348132\n",
      "\ttrain loss: 0.33711015708143116\n",
      "\ttrain loss: 0.5650188383337827\n",
      "\ttrain loss: 0.5087389248011243\n",
      "\ttrain loss: 0.4132542461638873\n",
      "\ttrain loss: 0.47016510974353487\n",
      "\ttrain loss: 0.38852455240142786\n",
      "\ttrain loss: 0.346461767432901\n",
      "\ttrain loss: 0.48601160535805515\n",
      "\ttrain loss: 0.3938443278984065\n",
      "\ttrain loss: 0.5033585585287632\n",
      "\ttrain loss: 0.21540489385502531\n",
      "\ttrain loss: 0.3760351232392297\n",
      "\ttrain loss: 0.4650947884043972\n",
      "\ttrain loss: 0.7059124091115407\n",
      "\ttrain loss: 0.4258744866203531\n",
      "\ttrain loss: 0.44958327814436944\n",
      "\ttrain loss: 0.2721814172404664\n",
      "\ttrain loss: 0.4111384554829753\n",
      "\ttrain loss: 0.35260756768688195\n",
      "\ttrain loss: 0.6657102751464286\n",
      "\ttrain loss: 0.31325632325421116\n",
      "\ttrain loss: 0.42378487139282783\n",
      "\ttrain loss: 0.6018187356161095\n",
      "\ttrain loss: 0.5640602668249829\n",
      "\ttrain loss: 0.3907992467213422\n",
      "\ttrain loss: 0.6839868786135435\n",
      "\ttrain loss: 0.562141882660022\n",
      "\ttrain loss: 0.4180993339325747\n",
      "\ttrain loss: 0.5414013229543075\n",
      "\ttrain loss: 0.3380260204419265\n",
      "\ttrain loss: 0.2952744350026044\n",
      "\ttrain loss: 0.5619536422658344\n",
      "\ttrain loss: 0.2798475059892331\n",
      "\ttrain loss: 0.5269351004642924\n",
      "\ttrain loss: 0.4173106358955811\n",
      "\ttrain loss: 0.4688004722413588\n",
      "\ttrain loss: 0.39050290443399427\n",
      "\ttrain loss: 0.35908267253015413\n",
      "\ttrain loss: 0.3325570223259237\n",
      "\ttrain loss: 0.5561932604050385\n",
      "\ttrain loss: 0.38040386861277764\n",
      "\ttrain loss: 0.30566082414223517\n",
      "\ttrain loss: 0.5788360152102237\n",
      "\ttrain loss: 0.3020795880521515\n",
      "\ttrain loss: 0.3624728211199928\n",
      "\ttrain loss: 0.4031400273490461\n",
      "\ttrain loss: 0.6234928815203329\n",
      "\ttrain loss: 0.5285345425157884\n",
      "\ttrain loss: 0.4577831138115017\n",
      "\ttrain loss: 0.4758107762026116\n",
      "\ttrain loss: 0.38425852142643324\n",
      "\ttrain loss: 0.4115743242512834\n",
      "\ttrain loss: 0.6058315201369842\n",
      "\ttrain loss: 0.5707769736546221\n",
      "\ttrain loss: 0.31786474286579725\n",
      "\ttrain loss: 0.3629590695249557\n",
      "\ttrain loss: 0.5680156512886432\n",
      "\ttrain loss: 0.4683656682515374\n",
      "\ttrain loss: 0.4191317103329951\n",
      "\ttrain loss: 0.46054544281318155\n",
      "\ttrain loss: 0.3323125571582975\n",
      "\ttrain loss: 0.3787179064568301\n",
      "\ttrain loss: 0.5251776226639246\n",
      "\ttrain loss: 0.5018135518703685\n",
      "\ttrain loss: 0.48752884174812683\n",
      "\ttrain loss: 0.446670848527365\n",
      "\ttrain loss: 0.4290902992689211\n",
      "\ttrain loss: 0.4037452315357573\n",
      "\ttrain loss: 0.3831428830567891\n",
      "\ttrain loss: 0.4210078425761075\n",
      "\ttrain loss: 0.5219578225071613\n",
      "\ttrain loss: 0.4926414227105955\n",
      "\ttrain loss: 0.3846181945054885\n",
      "\ttrain loss: 0.4260766477090051\n",
      "\ttrain loss: 0.5943760991990021\n",
      "\ttrain loss: 0.519636458437788\n",
      "\ttrain loss: 0.406132794319341\n",
      "\ttrain loss: 0.4551279828246958\n",
      "\ttrain loss: 0.428435841857823\n",
      "\ttrain loss: 0.6069491797550133\n",
      "\ttrain loss: 0.5260327570373033\n",
      "\ttrain loss: 0.2303910347757323\n",
      "\ttrain loss: 0.6341982147824601\n",
      "\ttrain loss: 0.467669868470464\n",
      "\ttrain loss: 0.5155886987405405\n",
      "\ttrain loss: 0.3113814784801274\n",
      "\ttrain loss: 0.5939048749252422\n",
      "\ttrain loss: 0.4584429976848591\n",
      "\ttrain loss: 0.36516743899392146\n",
      "\ttrain loss: 0.4340463793817273\n",
      "\ttrain loss: 0.5833391020452481\n",
      "\ttrain loss: 0.4229848286374213\n",
      "\ttrain loss: 0.3573462673026479\n",
      "\ttrain loss: 0.4143242255437125\n",
      "\ttrain loss: 0.5622227888532809\n",
      "\ttrain loss: 0.3681331322501791\n",
      "\ttrain loss: 0.47493874233605454\n",
      "\ttrain loss: 0.6247057771968891\n",
      "\ttrain loss: 0.373333466209699\n",
      "\ttrain loss: 0.41177601489181215\n",
      "\ttrain loss: 0.32347776619061946\n",
      "\ttrain loss: 0.4143064636924044\n",
      "\ttrain loss: 0.46797588777414256\n",
      "\ttrain loss: 0.5759153042985516\n",
      "\ttrain loss: 0.6312864061303507\n",
      "\ttrain loss: 0.34526992539236867\n",
      "\ttrain loss: 0.4242144785492875\n",
      "\ttrain loss: 0.3114160633092107\n",
      "\ttrain loss: 0.23907552203218033\n",
      "\ttrain loss: 0.36557737064064433\n",
      "\ttrain loss: 0.7940144431940581\n",
      "\ttrain loss: 0.31908423226768484\n",
      "\ttrain loss: 0.46791866832263235\n",
      "\ttrain loss: 0.4626502304055964\n",
      "\ttrain loss: 0.3910369316847797\n",
      "\ttrain loss: 0.7379024434586016\n",
      "\ttrain loss: 0.5590598891472085\n",
      "\ttrain loss: 0.3585980915544915\n",
      "\ttrain loss: 0.3515029482450016\n",
      "\ttrain loss: 0.4411318527154512\n",
      "\ttrain loss: 0.4071652363522249\n",
      "\ttrain loss: 0.2956757915835268\n",
      "\ttrain loss: 0.47950435155636495\n",
      "\ttrain loss: 0.5323495289426179\n",
      "\ttrain loss: 0.5460613778450919\n",
      "\ttrain loss: 0.40337143400511105\n",
      "\ttrain loss: 0.38997672713750015\n",
      "\ttrain loss: 0.45419745353374147\n",
      "\ttrain loss: 0.5273877173178341\n",
      "\ttrain loss: 0.46213540516311\n",
      "\ttrain loss: 0.4342184001140611\n",
      "\ttrain loss: 0.6501639468779042\n",
      "\ttrain loss: 0.5138749857296916\n",
      "\ttrain loss: 0.5283076768805357\n",
      "\ttrain loss: 0.32578440592964153\n",
      "\ttrain loss: 0.45119587536339595\n",
      "\ttrain loss: 0.3883462174324246\n",
      "\ttrain loss: 0.46958915611304686\n",
      "\ttrain loss: 0.34955076124150875\n",
      "\ttrain loss: 0.36497376811438115\n",
      "\ttrain loss: 0.5412816944157102\n",
      "\ttrain loss: 0.4263916981037682\n",
      "\ttrain loss: 0.2535299231738288\n",
      "\ttrain loss: 0.23085137120514254\n",
      "\ttrain loss: 0.4972776358406593\n",
      "\ttrain loss: 0.4114016084281549\n",
      "\ttrain loss: 0.4345207869151142\n",
      "\ttrain loss: 0.42272163361112747\n",
      "\ttrain loss: 0.32815969187444105\n",
      "\ttrain loss: 0.3766888874705732\n",
      "\ttrain loss: 0.4261911626163579\n",
      "\ttrain loss: 0.37160621565352553\n",
      "\ttrain loss: 0.5161909297226397\n",
      "\ttrain loss: 0.45403505936447425\n",
      "\ttrain loss: 0.5310587180514683\n",
      "\ttrain loss: 0.3350234895794748\n",
      "\ttrain loss: 0.47558713252055046\n",
      "\ttrain loss: 0.6889774272764605\n",
      "\ttrain loss: 0.3855439231477622\n",
      "\ttrain loss: 0.4761487068241633\n",
      "\ttrain loss: 0.3673535335998963\n",
      "\ttrain loss: 0.2983894931817376\n",
      "\ttrain loss: 0.3904722635698832\n",
      "\ttrain loss: 0.5155933719405036\n",
      "\ttrain loss: 0.3533361284831944\n",
      "\ttrain loss: 0.2999603237692329\n",
      "\ttrain loss: 0.4476177749885058\n",
      "\ttrain loss: 0.5641523719092134\n",
      "\ttrain loss: 0.3374072302859322\n",
      "\ttrain loss: 0.3149121318268464\n",
      "\ttrain loss: 0.3726702168024474\n",
      "\ttrain loss: 0.4241159915992072\n",
      "\ttrain loss: 0.3856157551643439\n",
      "\ttrain loss: 0.5628704485248858\n",
      "\ttrain loss: 0.4772078911843158\n",
      "\ttrain loss: 0.45896627059407247\n",
      "\ttrain loss: 0.4674543275636181\n",
      "\ttrain loss: 0.7896253795807913\n",
      "\ttrain loss: 0.7244638621494814\n",
      "\ttrain loss: 0.7396685991005172\n",
      "\ttrain loss: 0.3299166687675773\n",
      "\ttrain loss: 0.3778932726259576\n",
      "\ttrain loss: 0.25271331677287495\n",
      "\ttrain loss: 0.3999955855571353\n",
      "\ttrain loss: 0.31060406433817767\n",
      "\ttrain loss: 0.32089541449269465\n",
      "\ttrain loss: 0.502535699539129\n",
      "\ttrain loss: 0.7274031380860451\n",
      "\ttrain loss: 0.3913503878303581\n",
      "\ttrain loss: 0.32537882030334164\n",
      "\ttrain loss: 0.43728321978759743\n",
      "\ttrain loss: 0.4324656781087878\n",
      "\ttrain loss: 0.5055107425253361\n",
      "\ttrain loss: 0.6508644034439299\n",
      "\ttrain loss: 0.5296051560248767\n",
      "\ttrain loss: 0.42267159781362623\n",
      "\ttrain loss: 0.31558502899713126\n",
      "\ttrain loss: 0.7307172341444028\n",
      "\ttrain loss: 0.36055953579324407\n",
      "\ttrain loss: 0.43355940427429873\n",
      "\ttrain loss: 0.5210494394998817\n",
      "\ttrain loss: 0.5207388988742006\n",
      "\ttrain loss: 0.5233455420571635\n",
      "\ttrain loss: 0.38884893103542445\n",
      "\ttrain loss: 0.47444570445883627\n",
      "\ttrain loss: 0.48333908757823846\n",
      "\ttrain loss: 0.5418433777542071\n",
      "\ttrain loss: 0.39000841209854803\n",
      "\ttrain loss: 0.2836788471582713\n",
      "\ttrain loss: 0.3500640698545806\n",
      "\ttrain loss: 0.4175047651048182\n",
      "\ttrain loss: 0.5737302026088011\n",
      "\ttrain loss: 0.546310790703628\n",
      "\ttrain loss: 0.34914777107670286\n",
      "\ttrain loss: 0.46388163886099554\n",
      "\ttrain loss: 0.4548773995695038\n",
      "\ttrain loss: 0.37022891336362157\n",
      "\ttrain loss: 0.23613558850418453\n",
      "\ttrain loss: 0.5331339139885485\n",
      "\ttrain loss: 0.6224974391034417\n",
      "\ttrain loss: 0.8004335110840066\n",
      "\ttrain loss: 0.7625570635402859\n",
      "\ttrain loss: 0.4330872739799957\n",
      "\ttrain loss: 0.4004870563428351\n",
      "\ttrain loss: 0.38786353120184763\n",
      "\ttrain loss: 0.4557071892194549\n",
      "\ttrain loss: 0.9077233805884791\n",
      "\ttrain loss: 0.5930276554002394\n",
      "\ttrain loss: 0.5526130187654315\n",
      "\ttrain loss: 0.5717380284279936\n",
      "\ttrain loss: 0.5498933517857347\n",
      "\ttrain loss: 0.5431166259644242\n",
      "\ttrain loss: 0.888002595206756\n",
      "\ttrain loss: 0.4793703053792172\n",
      "\ttrain loss: 0.2381394233032196\n",
      "\ttrain loss: 0.5407964868583919\n",
      "\ttrain loss: 0.4690787504711915\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(3/15) is saved!\n",
      "\ttrain loss: 0.48677149911622397\n",
      "\ttrain loss: 0.28652001071386923\n",
      "\ttrain loss: 0.275129791696777\n",
      "\ttrain loss: 0.4030744545265221\n",
      "\ttrain loss: 0.4427934135603668\n",
      "\ttrain loss: 0.42837090534891753\n",
      "\ttrain loss: 0.3668095863722155\n",
      "\ttrain loss: 0.2756199118975713\n",
      "\ttrain loss: 0.36920992942708086\n",
      "\ttrain loss: 0.4916268929600979\n",
      "\ttrain loss: 0.3627519430117012\n",
      "\ttrain loss: 0.36337925248723435\n",
      "\ttrain loss: 0.32721680532174846\n",
      "\ttrain loss: 0.4523829300156076\n",
      "\ttrain loss: 0.5471684730331509\n",
      "\ttrain loss: 0.2845551678531754\n",
      "\ttrain loss: 0.4921164607998804\n",
      "\ttrain loss: 0.427684498061798\n",
      "\ttrain loss: 0.4228547911588816\n",
      "\ttrain loss: 0.6179988287646664\n",
      "\ttrain loss: 0.47965003285112817\n",
      "\ttrain loss: 0.28154847592701615\n",
      "\ttrain loss: 0.3793570626479533\n",
      "\ttrain loss: 0.4816246115258124\n",
      "\ttrain loss: 0.5449103918458773\n",
      "\ttrain loss: 0.5202865898095813\n",
      "\ttrain loss: 0.6055107885259949\n",
      "\ttrain loss: 0.6008753631936445\n",
      "\ttrain loss: 0.5316875797431184\n",
      "\ttrain loss: 0.676340378178975\n",
      "\ttrain loss: 0.41339118189541213\n",
      "\ttrain loss: 0.37791703916220365\n",
      "\ttrain loss: 0.42949947165890956\n",
      "\ttrain loss: 0.835301709718036\n",
      "\ttrain loss: 0.4435975306942232\n",
      "\ttrain loss: 0.48923594568938356\n",
      "\ttrain loss: 0.4549848600655844\n",
      "\ttrain loss: 0.2920261125881748\n",
      "\ttrain loss: 0.4559233528918015\n",
      "\ttrain loss: 0.7118496342818094\n",
      "\ttrain loss: 0.5620916022478195\n",
      "\ttrain loss: 0.29864073755165615\n",
      "\ttrain loss: 0.4224910792633158\n",
      "\ttrain loss: 0.37036573699160685\n",
      "\ttrain loss: 0.33554789448981315\n",
      "\ttrain loss: 0.5299350162448437\n",
      "\ttrain loss: 0.456304631802056\n",
      "\ttrain loss: 0.5943312767027421\n",
      "\ttrain loss: 0.44664275725411556\n",
      "\ttrain loss: 0.5781450430505612\n",
      "\ttrain loss: 0.6421356636372884\n",
      "\ttrain loss: 0.2933117322386344\n",
      "\ttrain loss: 0.5662534778229698\n",
      "\ttrain loss: 0.34779065331892156\n",
      "\ttrain loss: 0.4684111146432275\n",
      "\ttrain loss: 0.6723822123982474\n",
      "\ttrain loss: 0.4391875923491419\n",
      "\ttrain loss: 0.29811402124969144\n",
      "\ttrain loss: 0.2390633802610142\n",
      "\ttrain loss: 0.4691795167513829\n",
      "\ttrain loss: 0.4569491990137179\n",
      "\ttrain loss: 0.5466203356287083\n",
      "\ttrain loss: 0.39982169627283626\n",
      "\ttrain loss: 0.4536098805188232\n",
      "\ttrain loss: 0.37089566384976924\n",
      "\ttrain loss: 0.33510730342311845\n",
      "\ttrain loss: 0.4419981736038704\n",
      "\ttrain loss: 0.5252961389183728\n",
      "\ttrain loss: 0.5066414909085775\n",
      "\ttrain loss: 0.3519757962232256\n",
      "\ttrain loss: 0.46820262163513693\n",
      "\ttrain loss: 0.5984199543521768\n",
      "\ttrain loss: 0.3280111231298667\n",
      "\ttrain loss: 0.4667630649199852\n",
      "\ttrain loss: 0.2922981093900773\n",
      "\ttrain loss: 0.4805370264273118\n",
      "\ttrain loss: 0.655754751534142\n",
      "\ttrain loss: 0.3034854722687119\n",
      "\ttrain loss: 0.45544442404749486\n",
      "\ttrain loss: 0.4289101432597856\n",
      "\ttrain loss: 0.378091606374248\n",
      "\ttrain loss: 0.5445643518209877\n",
      "\ttrain loss: 0.45766521835736435\n",
      "\ttrain loss: 0.4648487976926704\n",
      "\ttrain loss: 0.40451712698973347\n",
      "\ttrain loss: 0.39159933235346467\n",
      "\ttrain loss: 0.3830611787951993\n",
      "\ttrain loss: 0.23904492423910664\n",
      "\ttrain loss: 0.6730088978836218\n",
      "\ttrain loss: 0.36950535007123353\n",
      "\ttrain loss: 0.48912464277840934\n",
      "\ttrain loss: 0.3477637327241231\n",
      "\ttrain loss: 0.5261928250855485\n",
      "\ttrain loss: 0.5376421650810358\n",
      "\ttrain loss: 0.368124846508931\n",
      "\ttrain loss: 0.4092030110254381\n",
      "\ttrain loss: 0.4288495680694533\n",
      "\ttrain loss: 0.7544179269379283\n",
      "\ttrain loss: 0.4049110409324587\n",
      "\ttrain loss: 0.39295855389841816\n",
      "\ttrain loss: 0.3614381768605125\n",
      "\ttrain loss: 0.5167100945556093\n",
      "\ttrain loss: 0.555999724162263\n",
      "\ttrain loss: 0.4962107686753683\n",
      "\ttrain loss: 0.44282350494576506\n",
      "\ttrain loss: 0.4329956426522356\n",
      "\ttrain loss: 0.5209012144890174\n",
      "\ttrain loss: 0.44192226635695286\n",
      "\ttrain loss: 0.5032904576251821\n",
      "\ttrain loss: 0.4818462943156465\n",
      "\ttrain loss: 0.40340156443624375\n",
      "\ttrain loss: 0.3988414758520963\n",
      "\ttrain loss: 0.4038332102154349\n",
      "\ttrain loss: 0.3636839912247336\n",
      "\ttrain loss: 0.46988040080645194\n",
      "\ttrain loss: 0.6590803110342331\n",
      "\ttrain loss: 0.3967477146274441\n",
      "\ttrain loss: 0.18429507598446518\n",
      "\ttrain loss: 0.3984286340686328\n",
      "\ttrain loss: 0.4179921110826845\n",
      "\ttrain loss: 0.4085940895077053\n",
      "\ttrain loss: 0.5473959002688303\n",
      "\ttrain loss: 0.48995531505415457\n",
      "\ttrain loss: 0.2719114640343235\n",
      "\ttrain loss: 0.21027727274077282\n",
      "\ttrain loss: 0.38421463487323987\n",
      "\ttrain loss: 0.47793120548758194\n",
      "\ttrain loss: 0.4281837626671895\n",
      "\ttrain loss: 0.528030293698454\n",
      "\ttrain loss: 0.397996782897564\n",
      "\ttrain loss: 0.47809416171777835\n",
      "\ttrain loss: 0.3656506811521404\n",
      "\ttrain loss: 0.45474020256945896\n",
      "\ttrain loss: 0.6426393801114909\n",
      "\ttrain loss: 0.4518128546398569\n",
      "\ttrain loss: 0.4176150673541378\n",
      "\ttrain loss: 0.5205983014626017\n",
      "\ttrain loss: 0.5660033817038084\n",
      "\ttrain loss: 0.3515908630674227\n",
      "\ttrain loss: 0.32002814126658125\n",
      "\ttrain loss: 0.393136892989629\n",
      "\ttrain loss: 0.3582190629584543\n",
      "\ttrain loss: 0.39689156420839977\n",
      "\ttrain loss: 0.37557422969935716\n",
      "\ttrain loss: 0.5919043915876374\n",
      "\ttrain loss: 0.3394898033795054\n",
      "\ttrain loss: 0.4409475138676797\n",
      "\ttrain loss: 0.35748298878602724\n",
      "\ttrain loss: 0.508644900002309\n",
      "\ttrain loss: 0.4125292350699481\n",
      "\ttrain loss: 0.4478124872718801\n",
      "\ttrain loss: 0.3288147948636041\n",
      "\ttrain loss: 0.4003418189438722\n",
      "\ttrain loss: 0.42645146980911475\n",
      "\ttrain loss: 0.6178070060259356\n",
      "\ttrain loss: 0.2800573699633164\n",
      "\ttrain loss: 0.3202976284430359\n",
      "\ttrain loss: 0.5283567298758917\n",
      "\ttrain loss: 0.39649498802125466\n",
      "\ttrain loss: 0.5149621216054702\n",
      "\ttrain loss: 0.42947862485580723\n",
      "\ttrain loss: 0.5553639005606718\n",
      "\ttrain loss: 0.5111738598724433\n",
      "\ttrain loss: 0.6386858734904022\n",
      "\ttrain loss: 0.5569290596986394\n",
      "\ttrain loss: 0.42157139011168576\n",
      "\ttrain loss: 0.3194540427680904\n",
      "\ttrain loss: 0.4311723038385771\n",
      "\ttrain loss: 0.3504686267318218\n",
      "\ttrain loss: 0.3673884961707248\n",
      "\ttrain loss: 0.4625365678225108\n",
      "\ttrain loss: 0.30604482336839967\n",
      "\ttrain loss: 0.5310896317388542\n",
      "\ttrain loss: 0.2662666781851521\n",
      "\ttrain loss: 0.3985158325826966\n",
      "\ttrain loss: 0.4118988220458966\n",
      "\ttrain loss: 0.4432579531018965\n",
      "\ttrain loss: 0.5385051192595405\n",
      "\ttrain loss: 0.4752173989007743\n",
      "\ttrain loss: 0.49062805670536014\n",
      "\ttrain loss: 0.6041133654740283\n",
      "\ttrain loss: 0.37904318155404737\n",
      "\ttrain loss: 0.33506375456727255\n",
      "\ttrain loss: 0.484080402969611\n",
      "\ttrain loss: 0.3995686085441216\n",
      "\ttrain loss: 0.5235352574965041\n",
      "\ttrain loss: 0.4513659384333705\n",
      "\ttrain loss: 0.5746902832994378\n",
      "\ttrain loss: 0.31732287878180093\n",
      "\ttrain loss: 0.6123621789026953\n",
      "\ttrain loss: 0.534072777887352\n",
      "\ttrain loss: 0.5067824895958705\n",
      "\ttrain loss: 0.4606769627677339\n",
      "\ttrain loss: 0.5029892715208568\n",
      "\ttrain loss: 0.38685184478039575\n",
      "\ttrain loss: 0.3952905528758912\n",
      "\ttrain loss: 0.3896179091963538\n",
      "\ttrain loss: 0.7212273216051073\n",
      "\ttrain loss: 0.601349003709427\n",
      "\ttrain loss: 0.36505876050173663\n",
      "\ttrain loss: 0.3086767517215414\n",
      "\ttrain loss: 0.31685604094774267\n",
      "\ttrain loss: 0.47354225830065033\n",
      "\ttrain loss: 0.599142148671705\n",
      "\ttrain loss: 0.400025289053547\n",
      "\ttrain loss: 0.41073126727664894\n",
      "\ttrain loss: 0.5472257137731263\n",
      "\ttrain loss: 0.5981123874587587\n",
      "\ttrain loss: 0.4670785165298653\n",
      "\ttrain loss: 0.33071080398737507\n",
      "\ttrain loss: 0.7741535216679243\n",
      "\ttrain loss: 0.4192059905031028\n",
      "\ttrain loss: 0.34601132875114315\n",
      "\ttrain loss: 0.5169265720550831\n",
      "\ttrain loss: 0.49661764209396075\n",
      "\ttrain loss: 0.7070580525258243\n",
      "\ttrain loss: 0.5136857523779369\n",
      "\ttrain loss: 0.6049827260327406\n",
      "\ttrain loss: 0.39339933040883446\n",
      "\ttrain loss: 0.2897726028807377\n",
      "\ttrain loss: 0.5719369323897344\n",
      "\ttrain loss: 0.4149470534738929\n",
      "\ttrain loss: 0.3083903645558403\n",
      "\ttrain loss: 0.4811185876106991\n",
      "\ttrain loss: 0.39295187618930316\n",
      "\ttrain loss: 0.41484137637569707\n",
      "\ttrain loss: 0.3539452147840805\n",
      "\ttrain loss: 0.32237788132273826\n",
      "\ttrain loss: 0.5103579017624622\n",
      "\ttrain loss: 0.36718689684405115\n",
      "\ttrain loss: 0.4669827147313146\n",
      "\ttrain loss: 0.670677662725939\n",
      "\ttrain loss: 0.4098928333724098\n",
      "\ttrain loss: 0.620438096083098\n",
      "\ttrain loss: 0.4554531959188859\n",
      "\ttrain loss: 0.653531652640247\n",
      "\ttrain loss: 0.6057137560146794\n",
      "\ttrain loss: 0.4525964605734988\n",
      "\ttrain loss: 0.4694941556229518\n",
      "\ttrain loss: 0.2859364249943209\n",
      "\ttrain loss: 0.32028139868417604\n",
      "\ttrain loss: 0.4100537907913183\n",
      "\ttrain loss: 0.3185969675163804\n",
      "\ttrain loss: 0.4725275211515597\n",
      "\ttrain loss: 0.367531884476038\n",
      "\ttrain loss: 0.6160965451933819\n",
      "\ttrain loss: 0.5526068505347388\n",
      "\ttrain loss: 0.3535748888917174\n",
      "\ttrain loss: 0.3790480266170905\n",
      "\ttrain loss: 0.6712229852880988\n",
      "\ttrain loss: 0.37727267854689284\n",
      "\ttrain loss: 0.5085521313253754\n",
      "\ttrain loss: 0.452140185533974\n",
      "\ttrain loss: 0.5073169245826119\n",
      "\ttrain loss: 0.3059336779421866\n",
      "\ttrain loss: 0.34687213981880466\n",
      "\ttrain loss: 0.3944443976177395\n",
      "\ttrain loss: 0.39430214911649836\n",
      "\ttrain loss: 0.25449545430985054\n",
      "\ttrain loss: 0.5166277065723964\n",
      "\ttrain loss: 0.38429477344045837\n",
      "\ttrain loss: 0.38551953079838963\n",
      "\ttrain loss: 0.4055662276474399\n",
      "\ttrain loss: 0.37376379459617926\n",
      "\ttrain loss: 0.30229159302566\n",
      "\ttrain loss: 0.6625654238405481\n",
      "\ttrain loss: 0.37613375916794944\n",
      "\ttrain loss: 0.5228813151569214\n",
      "\ttrain loss: 0.39965019011131764\n",
      "\ttrain loss: 0.2921263777473054\n",
      "\ttrain loss: 0.5752623793668945\n",
      "\ttrain loss: 0.40864972757072815\n",
      "\ttrain loss: 0.5047012800558601\n",
      "\ttrain loss: 0.48651373324062225\n",
      "\ttrain loss: 0.38117182627909585\n",
      "\ttrain loss: 0.5214303317925354\n",
      "\ttrain loss: 0.34696445763826245\n",
      "\ttrain loss: 0.45483814103622333\n",
      "\ttrain loss: 0.4274356243610808\n",
      "\ttrain loss: 0.4168014664747303\n",
      "\ttrain loss: 0.5203468369470436\n",
      "\ttrain loss: 0.5809534610349543\n",
      "\ttrain loss: 0.5505095933791027\n",
      "\ttrain loss: 0.4490115848698543\n",
      "\ttrain loss: 0.5665395206679615\n",
      "\ttrain loss: 0.4527564370788807\n",
      "\ttrain loss: 0.37002482421962773\n",
      "\ttrain loss: 0.4204204257808042\n",
      "\ttrain loss: 0.5697516013828003\n",
      "\ttrain loss: 0.38443914583530925\n",
      "\ttrain loss: 0.4389968806444883\n",
      "\ttrain loss: 0.33111985097030894\n",
      "\ttrain loss: 0.3569764351637217\n",
      "\ttrain loss: 0.500281484919916\n",
      "\ttrain loss: 0.4591841452201521\n",
      "\ttrain loss: 0.5057015483617567\n",
      "\ttrain loss: 0.5209890568947849\n",
      "\ttrain loss: 0.3910352963571575\n",
      "\ttrain loss: 0.3024834699974742\n",
      "\ttrain loss: 0.4173437875338447\n",
      "\ttrain loss: 0.3252913502159534\n",
      "\ttrain loss: 0.41106235733320484\n",
      "\ttrain loss: 0.3643190444144896\n",
      "\ttrain loss: 0.3527920698129232\n",
      "\ttrain loss: 0.406247120666516\n",
      "\ttrain loss: 0.41989431202599664\n",
      "\ttrain loss: 0.6181662172969731\n",
      "\ttrain loss: 0.39848503460918694\n",
      "\ttrain loss: 0.5540370674862718\n",
      "\ttrain loss: 0.552311368854722\n",
      "\ttrain loss: 0.4954638510150192\n",
      "\ttrain loss: 0.38434571507874804\n",
      "\ttrain loss: 0.5374235705439774\n",
      "\ttrain loss: 0.5242028172977946\n",
      "\ttrain loss: 0.653637122608954\n",
      "\ttrain loss: 0.6630610671349872\n",
      "\ttrain loss: 0.4028679665191604\n",
      "\ttrain loss: 0.28554366821952143\n",
      "\ttrain loss: 0.48951655252058873\n",
      "\ttrain loss: 0.402002708850964\n",
      "\ttrain loss: 0.4746323941146539\n",
      "\ttrain loss: 0.4453447093239758\n",
      "\ttrain loss: 0.7091282698058579\n",
      "\ttrain loss: 0.6292054411222847\n",
      "\ttrain loss: 0.4807161330075391\n",
      "\ttrain loss: 0.6384484819310465\n",
      "\ttrain loss: 0.4082573183404142\n",
      "\ttrain loss: 0.39235800459375353\n",
      "\ttrain loss: 0.48387932633327113\n",
      "\ttrain loss: 0.38634060980059504\n",
      "\ttrain loss: 0.4078817385373383\n",
      "\ttrain loss: 0.3749554509189358\n",
      "\ttrain loss: 0.4580558405490592\n",
      "\ttrain loss: 0.5112961483927365\n",
      "\ttrain loss: 0.4234715198472449\n",
      "\ttrain loss: 0.44199908878544436\n",
      "\ttrain loss: 0.4625050809618956\n",
      "\ttrain loss: 0.3691384630014267\n",
      "\ttrain loss: 0.5414269133145311\n",
      "\ttrain loss: 0.6292549567139851\n",
      "\ttrain loss: 0.4474627140714959\n",
      "\ttrain loss: 0.41016146567427764\n",
      "\ttrain loss: 0.44264571006223896\n",
      "\ttrain loss: 0.3511523434406162\n",
      "\ttrain loss: 0.313376721291342\n",
      "\ttrain loss: 0.5103771575831553\n",
      "\ttrain loss: 0.30959349681692944\n",
      "\ttrain loss: 0.44738671609923153\n",
      "\ttrain loss: 0.48414172326834337\n",
      "\ttrain loss: 0.3855437155693132\n",
      "\ttrain loss: 0.45313403120263823\n",
      "\ttrain loss: 0.4495509435393136\n",
      "\ttrain loss: 0.4331499886466687\n",
      "\ttrain loss: 0.29222516902600687\n",
      "\ttrain loss: 0.5620400166509096\n",
      "\ttrain loss: 0.40163308461536806\n",
      "\ttrain loss: 0.42479132936281727\n",
      "\ttrain loss: 0.33391547729724635\n",
      "\ttrain loss: 0.46234059686354356\n",
      "\ttrain loss: 0.3525240975377021\n",
      "\ttrain loss: 0.3745625275694671\n",
      "\ttrain loss: 0.4771715113901486\n",
      "\ttrain loss: 0.560228593748163\n",
      "\ttrain loss: 0.37277988949469204\n",
      "\ttrain loss: 0.889894573687037\n",
      "\ttrain loss: 0.4338934773532546\n",
      "\ttrain loss: 0.36146717582144694\n",
      "\ttrain loss: 0.6001006111431284\n",
      "\ttrain loss: 0.6252842367100113\n",
      "\ttrain loss: 0.2981758766964178\n",
      "\ttrain loss: 0.5878507698556068\n",
      "\ttrain loss: 0.8604041629235968\n",
      "\ttrain loss: 0.8962000845536668\n",
      "\ttrain loss: 0.3194582301788673\n",
      "\ttrain loss: 0.40720903995389374\n",
      "\ttrain loss: 0.4618821664970629\n",
      "\ttrain loss: 0.574263332340353\n",
      "\ttrain loss: 0.3377716903345963\n",
      "\ttrain loss: 0.3904573445043323\n",
      "\ttrain loss: 0.5367047759406736\n",
      "\ttrain loss: 0.4484283322406973\n",
      "\ttrain loss: 0.4625095947656288\n",
      "\ttrain loss: 0.44938320250096464\n",
      "\ttrain loss: 0.4882582892917989\n",
      "\ttrain loss: 0.6567360603562826\n",
      "\ttrain loss: 0.31332110449830497\n",
      "\ttrain loss: 0.38490245675278734\n",
      "\ttrain loss: 0.40296864539284216\n",
      "\ttrain loss: 0.5573548394914873\n",
      "\ttrain loss: 0.48974944278688887\n",
      "\ttrain loss: 0.3686239707793003\n",
      "\ttrain loss: 0.49165961096548483\n",
      "\ttrain loss: 0.42290613938424804\n",
      "\ttrain loss: 0.3988820552213784\n",
      "\ttrain loss: 0.5748668348813603\n",
      "\ttrain loss: 0.3039989537541773\n",
      "\ttrain loss: 0.4284218641063179\n",
      "\ttrain loss: 0.3930509862082094\n",
      "\ttrain loss: 0.5207964504963093\n",
      "\ttrain loss: 0.4793268270756129\n",
      "\ttrain loss: 0.5179148007295764\n",
      "\ttrain loss: 0.37312851261375357\n",
      "\ttrain loss: 0.4203268717156242\n",
      "\ttrain loss: 0.4726350667892594\n",
      "\ttrain loss: 0.47017429266713584\n",
      "\ttrain loss: 0.48281600688901766\n",
      "\ttrain loss: 0.3823343544525053\n",
      "\ttrain loss: 0.49004828704482273\n",
      "\ttrain loss: 0.5589292455493808\n",
      "\ttrain loss: 0.3496981366969064\n",
      "\ttrain loss: 0.40758946656046857\n",
      "\ttrain loss: 0.3716016856053086\n",
      "\ttrain loss: 0.4379938953101217\n",
      "\ttrain loss: 0.5770820931364493\n",
      "\ttrain loss: 0.39663432306572505\n",
      "\ttrain loss: 0.2953892800222315\n",
      "\ttrain loss: 0.308403282207066\n",
      "\ttrain loss: 0.37504077915701367\n",
      "\ttrain loss: 0.4042671194481401\n",
      "\ttrain loss: 0.4260862858402756\n",
      "\ttrain loss: 0.4378280192615719\n",
      "\ttrain loss: 0.23705801924400455\n",
      "\ttrain loss: 0.3968298608276207\n",
      "\ttrain loss: 0.5585151175628059\n",
      "\ttrain loss: 0.6291881155856303\n",
      "\ttrain loss: 0.39757287838083377\n",
      "\ttrain loss: 0.40327607443924607\n",
      "\ttrain loss: 0.3910007666589251\n",
      "\ttrain loss: 0.5252829671538478\n",
      "\ttrain loss: 0.44060579469053796\n",
      "\ttrain loss: 0.3888501817998702\n",
      "\ttrain loss: 0.4900795918736799\n",
      "\ttrain loss: 0.30087990238865003\n",
      "\ttrain loss: 0.403831135283769\n",
      "\ttrain loss: 0.699627984059701\n",
      "\ttrain loss: 0.524237733234465\n",
      "\ttrain loss: 0.5319241619486281\n",
      "\ttrain loss: 0.49986140867318546\n",
      "\ttrain loss: 0.45959862163289616\n",
      "\ttrain loss: 0.591551107366455\n",
      "\ttrain loss: 0.537888113161453\n",
      "\ttrain loss: 0.42824357525753753\n",
      "\ttrain loss: 0.5300524968035467\n",
      "\ttrain loss: 0.44265146823668944\n",
      "\ttrain loss: 0.36926492527083166\n",
      "\ttrain loss: 0.4890805129288589\n",
      "\ttrain loss: 0.5161083820511784\n",
      "\ttrain loss: 0.34021203065938266\n",
      "\ttrain loss: 0.27988919463082923\n",
      "\ttrain loss: 0.40457661766966624\n",
      "\ttrain loss: 0.5468695830796758\n",
      "\ttrain loss: 0.7145602064656288\n",
      "\ttrain loss: 0.18204105633766765\n",
      "\ttrain loss: 0.46573187276106653\n",
      "\ttrain loss: 0.4577892880364979\n",
      "\ttrain loss: 0.4314969790943104\n",
      "\ttrain loss: 0.645340132467742\n",
      "\ttrain loss: 0.3124760353589463\n",
      "\ttrain loss: 0.3267923295315307\n",
      "\ttrain loss: 0.3862511302561956\n",
      "\ttrain loss: 0.3139132602709117\n",
      "\ttrain loss: 0.4253283274024734\n",
      "\ttrain loss: 0.2946785983830408\n",
      "\ttrain loss: 0.3741890555344626\n",
      "\ttrain loss: 0.3571502321343043\n",
      "\ttrain loss: 0.40368216207765967\n",
      "\ttrain loss: 0.3864922105879155\n",
      "\ttrain loss: 0.4094683400002406\n",
      "\ttrain loss: 0.3976263840213795\n",
      "\ttrain loss: 0.6642754266587374\n",
      "\ttrain loss: 0.5384332367465124\n",
      "\ttrain loss: 0.26076448011338427\n",
      "\ttrain loss: 0.5685180556238634\n",
      "\ttrain loss: 0.4988462690737028\n",
      "\ttrain loss: 0.43086886570727506\n",
      "\ttrain loss: 0.5681646430347312\n",
      "\ttrain loss: 0.726631956971955\n",
      "\ttrain loss: 0.2983193000540493\n",
      "\ttrain loss: 0.5381094656964931\n",
      "\ttrain loss: 0.45473975364359864\n",
      "\ttrain loss: 0.35357426233459344\n",
      "\ttrain loss: 0.3001776128045758\n",
      "\ttrain loss: 0.48362734109478883\n",
      "\ttrain loss: 0.5795833546638858\n",
      "\ttrain loss: 0.4455438692427196\n",
      "\ttrain loss: 0.261745735143126\n",
      "\ttrain loss: 0.5912692423652233\n",
      "\ttrain loss: 0.43534557005477525\n",
      "\ttrain loss: 0.371047595237239\n",
      "\ttrain loss: 0.2893349804609515\n",
      "\ttrain loss: 0.29280491812185194\n",
      "\ttrain loss: 0.405962383678424\n",
      "\ttrain loss: 0.3547514767103591\n",
      "\ttrain loss: 0.676834955770978\n",
      "\ttrain loss: 0.2259545751285936\n",
      "\ttrain loss: 0.5893244297857968\n",
      "\ttrain loss: 0.47420997442138524\n",
      "\ttrain loss: 0.3054200877139373\n",
      "\ttrain loss: 0.41472809736999117\n",
      "\ttrain loss: 0.4001693616881935\n",
      "\ttrain loss: 0.5358667703173424\n",
      "\ttrain loss: 0.37435886597755197\n",
      "\ttrain loss: 0.6727899747899184\n",
      "\ttrain loss: 0.4515940720519657\n",
      "\ttrain loss: 0.32186364173261744\n",
      "\ttrain loss: 0.3254628851148985\n",
      "\ttrain loss: 0.2978082475052139\n",
      "\ttrain loss: 0.3776241259906522\n",
      "\ttrain loss: 0.335168507065511\n",
      "\ttrain loss: 0.31332928767140056\n",
      "\ttrain loss: 0.5755409296945473\n",
      "\ttrain loss: 0.33649742630173535\n",
      "\ttrain loss: 0.4004418205446671\n",
      "\ttrain loss: 0.4372521479312379\n",
      "\ttrain loss: 0.6977173295260144\n",
      "\ttrain loss: 0.29741826614379263\n",
      "\ttrain loss: 0.44783138624492547\n",
      "\ttrain loss: 0.4041152059584955\n",
      "\ttrain loss: 0.2632351185900398\n",
      "\ttrain loss: 0.4536285030722399\n",
      "\ttrain loss: 0.48674083088487996\n",
      "\ttrain loss: 0.4000405319617732\n",
      "\ttrain loss: 0.5095739609381553\n",
      "\ttrain loss: 0.42698838631557073\n",
      "\ttrain loss: 0.5936109613616515\n",
      "\ttrain loss: 0.3143057582031132\n",
      "\ttrain loss: 0.510167289457538\n",
      "\ttrain loss: 0.42618819700589083\n",
      "\ttrain loss: 0.43359723745007084\n",
      "\ttrain loss: 0.6479665267648378\n",
      "\ttrain loss: 0.5633259153887007\n",
      "\ttrain loss: 0.4408081341739867\n",
      "\ttrain loss: 0.4191086688365817\n",
      "\ttrain loss: 0.5097868943842554\n",
      "\ttrain loss: 0.404589720744181\n",
      "\ttrain loss: 0.4932611225423064\n",
      "\ttrain loss: 0.8260481003621194\n",
      "\ttrain loss: 0.4250311892076867\n",
      "\ttrain loss: 0.44279270599754306\n",
      "\ttrain loss: 0.38768019266462506\n",
      "\ttrain loss: 0.501705753447339\n",
      "\ttrain loss: 0.4980849140583351\n",
      "\ttrain loss: 0.4812930084170418\n",
      "\ttrain loss: 0.48045639607289303\n",
      "\ttrain loss: 0.4389646804051904\n",
      "\ttrain loss: 0.482884400691892\n",
      "\ttrain loss: 0.41937737585735285\n",
      "\ttrain loss: 0.33253447874959663\n",
      "\ttrain loss: 0.3575246292569635\n",
      "\ttrain loss: 0.4219001028346233\n",
      "\ttrain loss: 0.493377265628089\n",
      "\ttrain loss: 0.38943455167574714\n",
      "\ttrain loss: 0.37078602029025953\n",
      "\ttrain loss: 0.5001410259916966\n",
      "\ttrain loss: 0.35924438642493844\n",
      "\ttrain loss: 0.521128107943622\n",
      "\ttrain loss: 0.3790235814996775\n",
      "\ttrain loss: 0.5228520898035476\n",
      "\ttrain loss: 0.39199008040619837\n",
      "\ttrain loss: 0.49562537625649405\n",
      "\ttrain loss: 0.4377514760471606\n",
      "\ttrain loss: 0.33691953064753444\n",
      "\ttrain loss: 0.39654882833460686\n",
      "\ttrain loss: 0.31942254924756763\n",
      "\ttrain loss: 0.4556661369038552\n",
      "\ttrain loss: 0.4629728475222016\n",
      "\ttrain loss: 0.4325717374598198\n",
      "\ttrain loss: 0.40912397880201784\n",
      "\ttrain loss: 0.6811514731563775\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(4/15) is saved!\n",
      "\ttrain loss: 0.38989242413079617\n",
      "\ttrain loss: 0.5874155014785063\n",
      "\ttrain loss: 0.5535110926351755\n",
      "\ttrain loss: 0.5109627515849097\n",
      "\ttrain loss: 0.5517298122176664\n",
      "\ttrain loss: 0.47727383816981694\n",
      "\ttrain loss: 0.3717560823638941\n",
      "\ttrain loss: 0.43309209475730837\n",
      "\ttrain loss: 0.3748731805946812\n",
      "\ttrain loss: 0.4257656731657986\n",
      "\ttrain loss: 0.493440201576013\n",
      "\ttrain loss: 0.4445498463962605\n",
      "\ttrain loss: 0.3758469433160455\n",
      "\ttrain loss: 0.4640049486115557\n",
      "\ttrain loss: 0.5271566409404381\n",
      "\ttrain loss: 0.510110204887504\n",
      "\ttrain loss: 0.6067302597482453\n",
      "\ttrain loss: 0.39555255049889926\n",
      "\ttrain loss: 0.38640016082771844\n",
      "\ttrain loss: 0.24842423129021093\n",
      "\ttrain loss: 0.49242020798844544\n",
      "\ttrain loss: 0.5239890472929143\n",
      "\ttrain loss: 0.6493287352056135\n",
      "\ttrain loss: 0.5674312765273793\n",
      "\ttrain loss: 0.7395407629393722\n",
      "\ttrain loss: 0.32948668544535553\n",
      "\ttrain loss: 0.4445504747085172\n",
      "\ttrain loss: 0.35537149199725954\n",
      "\ttrain loss: 0.5241708470845664\n",
      "\ttrain loss: 0.4253010603347324\n",
      "\ttrain loss: 0.4234907330992551\n",
      "\ttrain loss: 0.4021531451012734\n",
      "\ttrain loss: 0.4226103946244615\n",
      "\ttrain loss: 0.4923570310431804\n",
      "\ttrain loss: 0.5495539618304957\n",
      "\ttrain loss: 0.46379118276507136\n",
      "\ttrain loss: 0.4450149765136171\n",
      "\ttrain loss: 0.37148650254845994\n",
      "\ttrain loss: 0.28747226143405996\n",
      "\ttrain loss: 0.1953638984320534\n",
      "\ttrain loss: 0.4540413876211469\n",
      "\ttrain loss: 0.5140458032507715\n",
      "\ttrain loss: 0.4792290308005153\n",
      "\ttrain loss: 0.45839549498660137\n",
      "\ttrain loss: 0.3461228404825345\n",
      "\ttrain loss: 0.4073393544436677\n",
      "\ttrain loss: 0.3945251773835363\n",
      "\ttrain loss: 0.40807929638955287\n",
      "\ttrain loss: 0.3998081081880959\n",
      "\ttrain loss: 0.5130175692479444\n",
      "\ttrain loss: 0.21574739906722823\n",
      "\ttrain loss: 0.5219520640514217\n",
      "\ttrain loss: 0.5401207284301643\n",
      "\ttrain loss: 0.26796866891456844\n",
      "\ttrain loss: 0.25361034504385466\n",
      "\ttrain loss: 0.3891163765407275\n",
      "\ttrain loss: 0.40291662954287916\n",
      "\ttrain loss: 0.2252800699553293\n",
      "\ttrain loss: 0.5117271453895429\n",
      "\ttrain loss: 0.4033991891691998\n",
      "\ttrain loss: 0.6793921063103442\n",
      "\ttrain loss: 0.4673219642868201\n",
      "\ttrain loss: 0.5032459731420558\n",
      "\ttrain loss: 0.36259405599232\n",
      "\ttrain loss: 0.5616383698136709\n",
      "\ttrain loss: 0.4585753170510835\n",
      "\ttrain loss: 0.29442924542423304\n",
      "\ttrain loss: 0.2699210066881359\n",
      "\ttrain loss: 0.4791161692483131\n",
      "\ttrain loss: 0.3480111501488401\n",
      "\ttrain loss: 0.2837296677704202\n",
      "\ttrain loss: 0.5328216092234013\n",
      "\ttrain loss: 0.30909635119634454\n",
      "\ttrain loss: 0.569127401648484\n",
      "\ttrain loss: 0.53539614726159\n",
      "\ttrain loss: 0.5975280282025427\n",
      "\ttrain loss: 0.45432472518281564\n",
      "\ttrain loss: 0.4422903758057509\n",
      "\ttrain loss: 0.49317723952550774\n",
      "\ttrain loss: 0.368851153098261\n",
      "\ttrain loss: 0.2874964476763377\n",
      "\ttrain loss: 0.32306746199662084\n",
      "\ttrain loss: 0.4961379750178143\n",
      "\ttrain loss: 0.4274013791970578\n",
      "\ttrain loss: 0.3754009605979203\n",
      "\ttrain loss: 0.4191312626749494\n",
      "\ttrain loss: 0.4369964106642612\n",
      "\ttrain loss: 0.4105367519360909\n",
      "\ttrain loss: 0.5036204234065698\n",
      "\ttrain loss: 0.20755932934072732\n",
      "\ttrain loss: 0.3599730031745016\n",
      "\ttrain loss: 0.34659757461296925\n",
      "\ttrain loss: 0.6114153976061499\n",
      "\ttrain loss: 0.47047531924670294\n",
      "\ttrain loss: 0.7330809312618525\n",
      "\ttrain loss: 0.32973640526821413\n",
      "\ttrain loss: 0.5181912873825926\n",
      "\ttrain loss: 0.32613095811126924\n",
      "\ttrain loss: 0.4470030934926444\n",
      "\ttrain loss: 0.26855521483137057\n",
      "\ttrain loss: 0.41235610596351896\n",
      "\ttrain loss: 0.3920916064386627\n",
      "\ttrain loss: 0.9076056230841374\n",
      "\ttrain loss: 0.560906758237393\n",
      "\ttrain loss: 0.343790149114963\n",
      "\ttrain loss: 0.27475935161394743\n",
      "\ttrain loss: 0.6198206755610705\n",
      "\ttrain loss: 0.4290149129142237\n",
      "\ttrain loss: 0.5509750370614059\n",
      "\ttrain loss: 0.3928726920568278\n",
      "\ttrain loss: 0.6633838337278318\n",
      "\ttrain loss: 0.3882649648936978\n",
      "\ttrain loss: 0.4162825789945117\n",
      "\ttrain loss: 0.3717831147559028\n",
      "\ttrain loss: 0.37249371696928085\n",
      "\ttrain loss: 0.5270187219603037\n",
      "\ttrain loss: 0.46795562159809506\n",
      "\ttrain loss: 0.45324108929219603\n",
      "\ttrain loss: 0.529516627509782\n",
      "\ttrain loss: 0.31925733895297936\n",
      "\ttrain loss: 0.5845967854340757\n",
      "\ttrain loss: 0.35762834219841\n",
      "\ttrain loss: 0.4206752644208256\n",
      "\ttrain loss: 0.3962825945381042\n",
      "\ttrain loss: 0.48861588780989373\n",
      "\ttrain loss: 0.3469948298145782\n",
      "\ttrain loss: 0.4573205547637067\n",
      "\ttrain loss: 0.3589086957528108\n",
      "\ttrain loss: 0.46701428092764274\n",
      "\ttrain loss: 0.3955307626051723\n",
      "\ttrain loss: 0.5241840574514164\n",
      "\ttrain loss: 0.36841772384172666\n",
      "\ttrain loss: 0.34960884902725026\n",
      "\ttrain loss: 0.5222522131938332\n",
      "\ttrain loss: 0.409587754537833\n",
      "\ttrain loss: 0.3491797372728306\n",
      "\ttrain loss: 0.2748243229522263\n",
      "\ttrain loss: 0.23659320020630337\n",
      "\ttrain loss: 0.5942287151411152\n",
      "\ttrain loss: 0.35011535100773095\n",
      "\ttrain loss: 0.5242414775351826\n",
      "\ttrain loss: 0.19798844157561007\n",
      "\ttrain loss: 0.4948340576424099\n",
      "\ttrain loss: 0.6488522751888297\n",
      "\ttrain loss: 0.4663977830406237\n",
      "\ttrain loss: 0.3767244523676506\n",
      "\ttrain loss: 0.3424629546135531\n",
      "\ttrain loss: 0.48280211297639825\n",
      "\ttrain loss: 0.5537501203890064\n",
      "\ttrain loss: 0.38840595334658146\n",
      "\ttrain loss: 0.49110818037606147\n",
      "\ttrain loss: 0.33521863365148463\n",
      "\ttrain loss: 0.3565332448844911\n",
      "\ttrain loss: 0.43268742344319044\n",
      "\ttrain loss: 0.31875789532403664\n",
      "\ttrain loss: 0.47315130725928106\n",
      "\ttrain loss: 0.428892599983701\n",
      "\ttrain loss: 0.46225533153042514\n",
      "\ttrain loss: 0.3466866532709584\n",
      "\ttrain loss: 0.3492681127085008\n",
      "\ttrain loss: 0.22776997783756514\n",
      "\ttrain loss: 0.3747249163836988\n",
      "\ttrain loss: 0.41801206989697937\n",
      "\ttrain loss: 0.7005388014400198\n",
      "\ttrain loss: 0.3213733844689134\n",
      "\ttrain loss: 0.3858275204477405\n",
      "\ttrain loss: 0.5473809518277096\n",
      "\ttrain loss: 0.3559616102262749\n",
      "\ttrain loss: 0.44334834233080406\n",
      "\ttrain loss: 0.4705039790787693\n",
      "\ttrain loss: 0.5760360623071858\n",
      "\ttrain loss: 0.26070994588199337\n",
      "\ttrain loss: 0.3927509369395772\n",
      "\ttrain loss: 0.2560168759024594\n",
      "\ttrain loss: 0.43956412854252447\n",
      "\ttrain loss: 0.32513874115218755\n",
      "\ttrain loss: 0.46599865699686843\n",
      "\ttrain loss: 0.4169426946592246\n",
      "\ttrain loss: 0.6962065488971174\n",
      "\ttrain loss: 0.37487010442105484\n",
      "\ttrain loss: 0.6044296986432658\n",
      "\ttrain loss: 0.25693889228305256\n",
      "\ttrain loss: 0.27622363518160037\n",
      "\ttrain loss: 0.3860870771658571\n",
      "\ttrain loss: 0.5518162155105835\n",
      "\ttrain loss: 0.3150887151572867\n",
      "\ttrain loss: 0.5823849547219181\n",
      "\ttrain loss: 0.3461731015926801\n",
      "\ttrain loss: 0.3967799909204005\n",
      "\ttrain loss: 0.41215727573585687\n",
      "\ttrain loss: 0.44240825525465155\n",
      "\ttrain loss: 0.35848143210743144\n",
      "\ttrain loss: 0.504158212573623\n",
      "\ttrain loss: 0.30239817584134826\n",
      "\ttrain loss: 0.2634623161860948\n",
      "\ttrain loss: 0.4546374481565211\n",
      "\ttrain loss: 0.5094903870973173\n",
      "\ttrain loss: 0.479622341555046\n",
      "\ttrain loss: 0.32313676628754195\n",
      "\ttrain loss: 0.5656797666388055\n",
      "\ttrain loss: 0.36056903679172525\n",
      "\ttrain loss: 0.48929558040781196\n",
      "\ttrain loss: 0.35015802107266986\n",
      "\ttrain loss: 0.42398625334383166\n",
      "\ttrain loss: 0.5858229690742396\n",
      "\ttrain loss: 0.49057913980468637\n",
      "\ttrain loss: 0.43257589068914515\n",
      "\ttrain loss: 0.500739642125759\n",
      "\ttrain loss: 0.39231469595972734\n",
      "\ttrain loss: 0.6281998014331008\n",
      "\ttrain loss: 0.4962874886818107\n",
      "\ttrain loss: 0.43867127051808885\n",
      "\ttrain loss: 0.30603141005262324\n",
      "\ttrain loss: 0.3975106151866478\n",
      "\ttrain loss: 0.554333618063326\n",
      "\ttrain loss: 0.47514767305626837\n",
      "\ttrain loss: 0.6802583327642656\n",
      "\ttrain loss: 0.34933458648411686\n",
      "\ttrain loss: 0.5007261536644585\n",
      "\ttrain loss: 0.4806111472665564\n",
      "\ttrain loss: 0.3782882878917024\n",
      "\ttrain loss: 0.5319213995251646\n",
      "\ttrain loss: 0.4088679191970642\n",
      "\ttrain loss: 0.6966754128416703\n",
      "\ttrain loss: 0.5300348849473244\n",
      "\ttrain loss: 0.3393197289328813\n",
      "\ttrain loss: 0.42865544464110167\n",
      "\ttrain loss: 0.4290478967046133\n",
      "\ttrain loss: 0.5146127097497574\n",
      "\ttrain loss: 0.5062919430398776\n",
      "\ttrain loss: 0.532970105352123\n",
      "\ttrain loss: 0.3461465925512571\n",
      "\ttrain loss: 0.31477633197340205\n",
      "\ttrain loss: 0.6134982562906899\n",
      "\ttrain loss: 0.5493476719000944\n",
      "\ttrain loss: 0.40158559873548\n",
      "\ttrain loss: 0.5652970421487662\n",
      "\ttrain loss: 0.3822645536073821\n",
      "\ttrain loss: 0.40932881628144036\n",
      "\ttrain loss: 0.3624433347930204\n",
      "\ttrain loss: 0.7073078885616366\n",
      "\ttrain loss: 0.381790836456659\n",
      "\ttrain loss: 0.4363145424084961\n",
      "\ttrain loss: 0.3183962012043232\n",
      "\ttrain loss: 0.2763388371565361\n",
      "\ttrain loss: 0.5357035447228036\n",
      "\ttrain loss: 0.40313828921519446\n",
      "\ttrain loss: 0.396263997560971\n",
      "\ttrain loss: 0.40999899703449266\n",
      "\ttrain loss: 0.38263547102623274\n",
      "\ttrain loss: 0.5292004721311951\n",
      "\ttrain loss: 0.42132927633447315\n",
      "\ttrain loss: 0.4261298590464157\n",
      "\ttrain loss: 0.44474050763719697\n",
      "\ttrain loss: 0.38717019723090973\n",
      "\ttrain loss: 0.35138399572272677\n",
      "\ttrain loss: 0.2992702780883774\n",
      "\ttrain loss: 0.326298674067762\n",
      "\ttrain loss: 0.5524734910144959\n",
      "\ttrain loss: 0.33581435138510995\n",
      "\ttrain loss: 0.5146420528548525\n",
      "\ttrain loss: 0.3433172240233897\n",
      "\ttrain loss: 0.35774461607502184\n",
      "\ttrain loss: 0.3614963179770033\n",
      "\ttrain loss: 0.345184497570672\n",
      "\ttrain loss: 0.39051268646794446\n",
      "\ttrain loss: 0.3262983617256669\n",
      "\ttrain loss: 0.5061893795464869\n",
      "\ttrain loss: 0.5927438187358014\n",
      "\ttrain loss: 0.28858717133529155\n",
      "\ttrain loss: 0.6044961955564815\n",
      "\ttrain loss: 0.31046095482860236\n",
      "\ttrain loss: 0.2978676524483934\n",
      "\ttrain loss: 0.2691675466739569\n",
      "\ttrain loss: 0.36567900924113217\n",
      "\ttrain loss: 0.39798474855972493\n",
      "\ttrain loss: 0.39954866339941797\n",
      "\ttrain loss: 0.6593180848948361\n",
      "\ttrain loss: 0.24910404740682746\n",
      "\ttrain loss: 0.6374886828919996\n",
      "\ttrain loss: 0.4339151548427166\n",
      "\ttrain loss: 0.5186652617467478\n",
      "\ttrain loss: 0.42971702252450394\n",
      "\ttrain loss: 0.5109273968079824\n",
      "\ttrain loss: 0.4436205755513208\n",
      "\ttrain loss: 0.7031889981995192\n",
      "\ttrain loss: 0.3148265292933085\n",
      "\ttrain loss: 0.6084458214635856\n",
      "\ttrain loss: 0.3980445439431291\n",
      "\ttrain loss: 0.4503461977136749\n",
      "\ttrain loss: 0.6248034416597217\n",
      "\ttrain loss: 0.5936208805478456\n",
      "\ttrain loss: 0.3668247804989493\n",
      "\ttrain loss: 0.2188594069228366\n",
      "\ttrain loss: 0.6165480858769036\n",
      "\ttrain loss: 0.5852758379760752\n",
      "\ttrain loss: 0.26525511962417\n",
      "\ttrain loss: 0.44194434828318946\n",
      "\ttrain loss: 0.5132755958850287\n",
      "\ttrain loss: 0.20985450123944221\n",
      "\ttrain loss: 0.43748516763053547\n",
      "\ttrain loss: 0.42185678161620765\n",
      "\ttrain loss: 0.5025930331757053\n",
      "\ttrain loss: 0.4503088201969779\n",
      "\ttrain loss: 0.5381415488576851\n",
      "\ttrain loss: 0.4973912037220741\n",
      "\ttrain loss: 0.4860705150582483\n",
      "\ttrain loss: 0.37969095606426884\n",
      "\ttrain loss: 0.4460951538744079\n",
      "\ttrain loss: 0.356950316832396\n",
      "\ttrain loss: 0.5516139985420219\n",
      "\ttrain loss: 0.5315605447797772\n",
      "\ttrain loss: 0.3421528220197284\n",
      "\ttrain loss: 0.5378182438509246\n",
      "\ttrain loss: 0.5790576214112642\n",
      "\ttrain loss: 0.28658634961125684\n",
      "\ttrain loss: 0.6118838409264092\n",
      "\ttrain loss: 0.35572874866776255\n",
      "\ttrain loss: 0.39408236173062905\n",
      "\ttrain loss: 0.2987878198098477\n",
      "\ttrain loss: 0.45256319281312957\n",
      "\ttrain loss: 0.3876959379035573\n",
      "\ttrain loss: 0.28746138317126935\n",
      "\ttrain loss: 0.6189535670014433\n",
      "\ttrain loss: 0.44712539682740654\n",
      "\ttrain loss: 0.5820109349232774\n",
      "\ttrain loss: 0.33774907506880414\n",
      "\ttrain loss: 0.5934967265303135\n",
      "\ttrain loss: 0.3845386688787202\n",
      "\ttrain loss: 0.4040541255782979\n",
      "\ttrain loss: 0.3196090398890707\n",
      "\ttrain loss: 0.41116686411269576\n",
      "\ttrain loss: 0.41310118922868644\n",
      "\ttrain loss: 0.32286597338515177\n",
      "\ttrain loss: 0.411953195639685\n",
      "\ttrain loss: 0.6341235460951559\n",
      "\ttrain loss: 0.3851784172576529\n",
      "\ttrain loss: 0.47141402814620414\n",
      "\ttrain loss: 0.4072002506377451\n",
      "\ttrain loss: 0.7064424558061928\n",
      "\ttrain loss: 0.44576783168446127\n",
      "\ttrain loss: 0.4114210386004457\n",
      "\ttrain loss: 0.4362554498611251\n",
      "\ttrain loss: 0.3568390098882519\n",
      "\ttrain loss: 0.42589541068941433\n",
      "\ttrain loss: 0.3389096860274272\n",
      "\ttrain loss: 0.4976179531840485\n",
      "\ttrain loss: 0.4127036513055115\n",
      "\ttrain loss: 0.48045610574788045\n",
      "\ttrain loss: 0.43587358727944636\n",
      "\ttrain loss: 0.33267016543003214\n",
      "\ttrain loss: 0.7180013700682044\n",
      "\ttrain loss: 0.40940217951369995\n",
      "\ttrain loss: 0.3107629955119131\n",
      "\ttrain loss: 0.3447905512699545\n",
      "\ttrain loss: 0.5964694829712576\n",
      "\ttrain loss: 0.393028572931443\n",
      "\ttrain loss: 0.5085273923270712\n",
      "\ttrain loss: 0.30131836482309826\n",
      "\ttrain loss: 0.5086776791444859\n",
      "\ttrain loss: 0.5290250593842538\n",
      "\ttrain loss: 0.4338448612193717\n",
      "\ttrain loss: 0.41245551879228076\n",
      "\ttrain loss: 0.42039296579598046\n",
      "\ttrain loss: 0.6529923229463588\n",
      "\ttrain loss: 0.4591563836569929\n",
      "\ttrain loss: 0.5906886626464967\n",
      "\ttrain loss: 0.30891910940006245\n",
      "\ttrain loss: 0.4658830943631368\n",
      "\ttrain loss: 0.6664313751615799\n",
      "\ttrain loss: 0.6073043798470454\n",
      "\ttrain loss: 0.41856586887201214\n",
      "\ttrain loss: 0.3822829587504123\n",
      "\ttrain loss: 0.40563334901573767\n",
      "\ttrain loss: 0.3234472740972332\n",
      "\ttrain loss: 0.597749016277545\n",
      "\ttrain loss: 0.3731373138208618\n",
      "\ttrain loss: 0.6919077926122359\n",
      "\ttrain loss: 0.4224905036118941\n",
      "\ttrain loss: 0.38494938384165756\n",
      "\ttrain loss: 0.4215393542048922\n",
      "\ttrain loss: 0.3794690927834852\n",
      "\ttrain loss: 0.42435913151831817\n",
      "\ttrain loss: 0.35134884798331745\n",
      "\ttrain loss: 0.3781205618866481\n",
      "\ttrain loss: 0.4306108977867603\n",
      "\ttrain loss: 0.4450312749656343\n",
      "\ttrain loss: 0.32318860541602257\n",
      "\ttrain loss: 0.5396417868754874\n",
      "\ttrain loss: 0.4893387728279049\n",
      "\ttrain loss: 0.24171246585870515\n",
      "\ttrain loss: 0.37462101379221013\n",
      "\ttrain loss: 0.25897493309305775\n",
      "\ttrain loss: 0.6806630499171171\n",
      "\ttrain loss: 0.444754389336786\n",
      "\ttrain loss: 0.47064312296299404\n",
      "\ttrain loss: 0.39677774706040053\n",
      "\ttrain loss: 0.30298452149392896\n",
      "\ttrain loss: 0.39983430363434586\n",
      "\ttrain loss: 0.4650746481042163\n",
      "\ttrain loss: 0.5258877263737625\n",
      "\ttrain loss: 0.26912357971006284\n",
      "\ttrain loss: 0.5268400269692814\n",
      "\ttrain loss: 0.30714094253708907\n",
      "\ttrain loss: 0.47829707631019425\n",
      "\ttrain loss: 0.5433705464054155\n",
      "\ttrain loss: 0.4746566920551766\n",
      "\ttrain loss: 0.554017079855915\n",
      "\ttrain loss: 0.695253711806858\n",
      "\ttrain loss: 0.550086668294933\n",
      "\ttrain loss: 0.3458731744198275\n",
      "\ttrain loss: 0.35179649808156255\n",
      "\ttrain loss: 0.43242321516634574\n",
      "\ttrain loss: 0.43831557968464463\n",
      "\ttrain loss: 0.538000299686137\n",
      "\ttrain loss: 0.6355036455728009\n",
      "\ttrain loss: 0.4346515093142507\n",
      "\ttrain loss: 0.4278151075820788\n",
      "\ttrain loss: 0.4995133051117608\n",
      "\ttrain loss: 0.39409097280624017\n",
      "\ttrain loss: 0.5262394959205376\n",
      "\ttrain loss: 0.5766375060521478\n",
      "\ttrain loss: 0.5874970949208578\n",
      "\ttrain loss: 0.5375447259679007\n",
      "\ttrain loss: 0.35759607867064164\n",
      "\ttrain loss: 0.5470863146962843\n",
      "\ttrain loss: 0.5120577079961182\n",
      "\ttrain loss: 0.5541265640179878\n",
      "\ttrain loss: 0.462919245460509\n",
      "\ttrain loss: 0.31577941055953085\n",
      "\ttrain loss: 0.46227443653934464\n",
      "\ttrain loss: 0.34808544653358053\n",
      "\ttrain loss: 0.41746662029808673\n",
      "\ttrain loss: 0.7249133868853985\n",
      "\ttrain loss: 0.4861409739141953\n",
      "\ttrain loss: 0.5150480959305939\n",
      "\ttrain loss: 0.5948287051094276\n",
      "\ttrain loss: 0.4755094721647639\n",
      "\ttrain loss: 0.4105139087571964\n",
      "\ttrain loss: 0.5055046651599779\n",
      "\ttrain loss: 0.35638839232414865\n",
      "\ttrain loss: 0.669704688841401\n",
      "\ttrain loss: 0.4829476578406915\n",
      "\ttrain loss: 0.4816087008581787\n",
      "\ttrain loss: 0.47263692732720675\n",
      "\ttrain loss: 0.46250969480255577\n",
      "\ttrain loss: 0.4129597139022532\n",
      "\ttrain loss: 0.4630693116915251\n",
      "\ttrain loss: 0.750367432306954\n",
      "\ttrain loss: 0.5557597121340655\n",
      "\ttrain loss: 0.38895264200917995\n",
      "\ttrain loss: 0.5301218241938641\n",
      "\ttrain loss: 0.6921817800986212\n",
      "\ttrain loss: 0.4878124789326188\n",
      "\ttrain loss: 0.3809281510451076\n",
      "\ttrain loss: 0.483602764694864\n",
      "\ttrain loss: 0.4895731229163063\n",
      "\ttrain loss: 0.4538985988251031\n",
      "\ttrain loss: 0.3998363209102471\n",
      "\ttrain loss: 0.455728366628778\n",
      "\ttrain loss: 0.3293370179618554\n",
      "\ttrain loss: 0.4546519903876176\n",
      "\ttrain loss: 0.37287243316663343\n",
      "\ttrain loss: 0.4212104440116985\n",
      "\ttrain loss: 0.3716484149825951\n",
      "\ttrain loss: 0.3438212021241017\n",
      "\ttrain loss: 0.4488578015462685\n",
      "\ttrain loss: 0.51621060814081\n",
      "\ttrain loss: 0.46523812262302117\n",
      "\ttrain loss: 0.5058098564849222\n",
      "\ttrain loss: 0.3755911971026935\n",
      "\ttrain loss: 0.5066456948911306\n",
      "\ttrain loss: 0.4078016920898654\n",
      "\ttrain loss: 0.3396022683786568\n",
      "\ttrain loss: 0.4489883000869348\n",
      "\ttrain loss: 0.603504112344591\n",
      "\ttrain loss: 0.2540757539542157\n",
      "\ttrain loss: 0.5856601673868069\n",
      "\ttrain loss: 0.3794284179836967\n",
      "\ttrain loss: 0.46016687874165385\n",
      "\ttrain loss: 0.46183991846366784\n",
      "\ttrain loss: 0.44434659489690465\n",
      "\ttrain loss: 0.5422775919271935\n",
      "\ttrain loss: 0.2858082208494007\n",
      "\ttrain loss: 0.5346672810351167\n",
      "\ttrain loss: 0.5362697157566358\n",
      "\ttrain loss: 0.5078268720263233\n",
      "\ttrain loss: 0.5269801024012599\n",
      "\ttrain loss: 0.3178977923585078\n",
      "\ttrain loss: 0.6453225601247707\n",
      "\ttrain loss: 0.2917304612071144\n",
      "\ttrain loss: 0.48166032775089984\n",
      "\ttrain loss: 0.37155163642371786\n",
      "\ttrain loss: 0.5057399009674717\n",
      "\ttrain loss: 0.4966242796678457\n",
      "\ttrain loss: 0.42790126579372606\n",
      "\ttrain loss: 0.30438286039454987\n",
      "\ttrain loss: 0.3732386759411416\n",
      "\ttrain loss: 0.4715019726722437\n",
      "\ttrain loss: 0.3680608185992542\n",
      "\ttrain loss: 0.32611759431890563\n",
      "\ttrain loss: 0.3333719850244391\n",
      "\ttrain loss: 0.4094409907523613\n",
      "\ttrain loss: 0.3455426149877739\n",
      "\ttrain loss: 0.43105409227061\n",
      "\ttrain loss: 0.4646736473313444\n",
      "\ttrain loss: 0.36828514702744247\n",
      "\ttrain loss: 0.3513101783390912\n",
      "\ttrain loss: 0.24017660694884274\n",
      "\ttrain loss: 0.2852395653141461\n",
      "\ttrain loss: 0.4493415649690796\n",
      "\ttrain loss: 0.3896677754799428\n",
      "\ttrain loss: 0.32296559711526784\n",
      "\ttrain loss: 0.42099182987960676\n",
      "\ttrain loss: 0.30199133706307907\n",
      "\ttrain loss: 0.6381916430884608\n",
      "\ttrain loss: 0.553492192194201\n",
      "\ttrain loss: 0.27517177165628204\n",
      "\ttrain loss: 0.29463409830317394\n",
      "\ttrain loss: 0.2917076778339863\n",
      "\ttrain loss: 0.4157612065752093\n",
      "\ttrain loss: 0.5961058482442115\n",
      "\ttrain loss: 0.3890909821580074\n",
      "\ttrain loss: 0.6183927554673083\n",
      "\ttrain loss: 0.350037890391923\n",
      "\ttrain loss: 0.3846647224131424\n",
      "\ttrain loss: 0.4257642416501778\n",
      "\ttrain loss: 0.5248351398474667\n",
      "\ttrain loss: 0.59164591431647\n",
      "\ttrain loss: 0.3701698627901282\n",
      "\ttrain loss: 0.3895322168230808\n",
      "\ttrain loss: 0.2711353698501473\n",
      "\ttrain loss: 0.6140814051923343\n",
      "\ttrain loss: 0.4327742509125172\n",
      "\ttrain loss: 0.45314401771266555\n",
      "\ttrain loss: 0.48357306734037336\n",
      "\ttrain loss: 0.3304222021747008\n",
      "\ttrain loss: 0.4344872911447809\n",
      "\ttrain loss: 0.3442201520721724\n",
      "\ttrain loss: 0.4553583735335873\n",
      "\ttrain loss: 0.5617048653280898\n",
      "\ttrain loss: 0.4264851557335394\n",
      "\ttrain loss: 0.4866515259055792\n",
      "\ttrain loss: 0.4559870394011686\n",
      "\ttrain loss: 0.5046344879771755\n",
      "\ttrain loss: 0.3035411945903943\n",
      "\ttrain loss: 0.76860953877739\n",
      "\ttrain loss: 0.8339904456898912\n",
      "\ttrain loss: 0.3829176151797863\n",
      "\ttrain loss: 0.35346941489326855\n",
      "\ttrain loss: 0.356282812042909\n",
      "\ttrain loss: 0.32713031899277456\n",
      "\ttrain loss: 0.2407256943999468\n",
      "\ttrain loss: 0.3795437851067452\n",
      "\ttrain loss: 0.5513528264153122\n",
      "\ttrain loss: 0.5236221623282552\n",
      "\ttrain loss: 0.5391645733287086\n",
      "\ttrain loss: 0.6794587474320618\n",
      "\ttrain loss: 0.5354481073616031\n",
      "\ttrain loss: 0.4467346203513224\n",
      "\ttrain loss: 0.5603194489906059\n",
      "\ttrain loss: 0.5083563234748193\n",
      "\ttrain loss: 0.6241615024554499\n",
      "\ttrain loss: 0.39586728939477145\n",
      "\ttrain loss: 0.2828482043976792\n",
      "\ttrain loss: 0.5940566885697085\n",
      "\ttrain loss: 0.2957417529987355\n",
      "\ttrain loss: 0.41979024786106833\n",
      "\ttrain loss: 0.4711474828448826\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(5/15) is saved!\n",
      "\ttrain loss: 0.4560844764696465\n",
      "\ttrain loss: 0.527698102493301\n",
      "\ttrain loss: 0.35227228668860855\n",
      "\ttrain loss: 0.5329222419587358\n",
      "\ttrain loss: 0.475520879397091\n",
      "\ttrain loss: 0.3262001045297521\n",
      "\ttrain loss: 0.5686593283088608\n",
      "\ttrain loss: 0.3542527882888964\n",
      "\ttrain loss: 0.48911775785700273\n",
      "\ttrain loss: 0.4481147251680849\n",
      "\ttrain loss: 0.5303028453496192\n",
      "\ttrain loss: 0.412791679561323\n",
      "\ttrain loss: 0.3055727248747777\n",
      "\ttrain loss: 0.35971795175490917\n",
      "\ttrain loss: 0.4782903400740246\n",
      "\ttrain loss: 0.3007380510374849\n",
      "\ttrain loss: 0.4073032410803718\n",
      "\ttrain loss: 0.3862777036700086\n",
      "\ttrain loss: 0.40021805960128565\n",
      "\ttrain loss: 0.5025433234613487\n",
      "\ttrain loss: 0.3599676919783237\n",
      "\ttrain loss: 0.4756244714185825\n",
      "\ttrain loss: 0.40127269607794547\n",
      "\ttrain loss: 0.2825873407920799\n",
      "\ttrain loss: 0.3787472351693437\n",
      "\ttrain loss: 0.4983722752955715\n",
      "\ttrain loss: 0.37115887865935904\n",
      "\ttrain loss: 0.43419599898452194\n",
      "\ttrain loss: 0.5352343369234003\n",
      "\ttrain loss: 0.35384707799805926\n",
      "\ttrain loss: 0.36599814207619796\n",
      "\ttrain loss: 0.41096221616597367\n",
      "\ttrain loss: 0.28442628601841174\n",
      "\ttrain loss: 0.7788673184929309\n",
      "\ttrain loss: 0.5776415231173976\n",
      "\ttrain loss: 0.3037059884348193\n",
      "\ttrain loss: 0.4200153291875436\n",
      "\ttrain loss: 0.4546650148135683\n",
      "\ttrain loss: 0.4734376064038501\n",
      "\ttrain loss: 0.4232746797891893\n",
      "\ttrain loss: 0.25839550671527856\n",
      "\ttrain loss: 0.4253207415340645\n",
      "\ttrain loss: 0.6439160584151427\n",
      "\ttrain loss: 0.7835344012973797\n",
      "\ttrain loss: 0.6788165988579484\n",
      "\ttrain loss: 0.36127284195494325\n",
      "\ttrain loss: 0.4787527100297029\n",
      "\ttrain loss: 0.531274946266235\n",
      "\ttrain loss: 0.44002047547246276\n",
      "\ttrain loss: 0.3477122657023952\n",
      "\ttrain loss: 0.4019186517117524\n",
      "\ttrain loss: 0.4792914667365135\n",
      "\ttrain loss: 0.40574009606796285\n",
      "\ttrain loss: 0.47526409488187105\n",
      "\ttrain loss: 0.436682023577114\n",
      "\ttrain loss: 0.412272852111349\n",
      "\ttrain loss: 0.4471676812450823\n",
      "\ttrain loss: 0.44814272006229716\n",
      "\ttrain loss: 0.6392108811640639\n",
      "\ttrain loss: 0.4774719628200619\n",
      "\ttrain loss: 0.36788422086311934\n",
      "\ttrain loss: 0.3306509474721243\n",
      "\ttrain loss: 0.514848832739019\n",
      "\ttrain loss: 0.5074689501202743\n",
      "\ttrain loss: 0.4321766229801579\n",
      "\ttrain loss: 0.32229912435502617\n",
      "\ttrain loss: 0.39681240512825205\n",
      "\ttrain loss: 0.5425852146230169\n",
      "\ttrain loss: 0.30554930087847365\n",
      "\ttrain loss: 0.493382656061692\n",
      "\ttrain loss: 0.3626830685413934\n",
      "\ttrain loss: 0.36241284333700613\n",
      "\ttrain loss: 0.9341868970596949\n",
      "\ttrain loss: 0.6858551085929467\n",
      "\ttrain loss: 0.4130308487593129\n",
      "\ttrain loss: 0.36207005486220584\n",
      "\ttrain loss: 0.44307388152787774\n",
      "\ttrain loss: 0.31210261288337865\n",
      "\ttrain loss: 0.42273194418638\n",
      "\ttrain loss: 0.3159036173103633\n",
      "\ttrain loss: 0.3379591547959814\n",
      "\ttrain loss: 0.3629016806047505\n",
      "\ttrain loss: 0.3141849921498068\n",
      "\ttrain loss: 0.5074027071203008\n",
      "\ttrain loss: 0.36023819262646456\n",
      "\ttrain loss: 0.5072752091261179\n",
      "\ttrain loss: 0.4594697212151547\n",
      "\ttrain loss: 0.49977068283964976\n",
      "\ttrain loss: 0.533686139610628\n",
      "\ttrain loss: 0.37951970092805853\n",
      "\ttrain loss: 0.28907420413016666\n",
      "\ttrain loss: 0.3395927538361073\n",
      "\ttrain loss: 0.4657834170723475\n",
      "\ttrain loss: 0.5840995185793147\n",
      "\ttrain loss: 0.3825899448970263\n",
      "\ttrain loss: 0.3482964027941789\n",
      "\ttrain loss: 0.44308676401750996\n",
      "\ttrain loss: 0.37725493753283734\n",
      "\ttrain loss: 0.3146906114284198\n",
      "\ttrain loss: 0.4523343137289174\n",
      "\ttrain loss: 0.3748949565028673\n",
      "\ttrain loss: 0.3664775251022485\n",
      "\ttrain loss: 0.35766473657782094\n",
      "\ttrain loss: 0.35229079880227016\n",
      "\ttrain loss: 0.27960732305950586\n",
      "\ttrain loss: 0.3672741353553749\n",
      "\ttrain loss: 0.3243482506704839\n",
      "\ttrain loss: 0.4149515740479202\n",
      "\ttrain loss: 0.36902276211556084\n",
      "\ttrain loss: 0.4661577241984193\n",
      "\ttrain loss: 0.3422348661010154\n",
      "\ttrain loss: 0.46230507249182096\n",
      "\ttrain loss: 0.3401744945268477\n",
      "\ttrain loss: 0.4216502701662102\n",
      "\ttrain loss: 0.45020446497283956\n",
      "\ttrain loss: 0.4507751203027488\n",
      "\ttrain loss: 0.3597662647088679\n",
      "\ttrain loss: 0.5349142223651173\n",
      "\ttrain loss: 0.6058947723795489\n",
      "\ttrain loss: 0.41933549662227243\n",
      "\ttrain loss: 0.39826069470951986\n",
      "\ttrain loss: 0.608740275267533\n",
      "\ttrain loss: 0.6448499187339684\n",
      "\ttrain loss: 0.4735169308385697\n",
      "\ttrain loss: 0.39894500257982013\n",
      "\ttrain loss: 0.2688349188628933\n",
      "\ttrain loss: 0.3146735951095201\n",
      "\ttrain loss: 0.41455875716243173\n",
      "\ttrain loss: 0.4710894298679975\n",
      "\ttrain loss: 0.4402629115783342\n",
      "\ttrain loss: 0.3707791588159265\n",
      "\ttrain loss: 0.44002992438746574\n",
      "\ttrain loss: 0.37637817700238885\n",
      "\ttrain loss: 0.43112763120963116\n",
      "\ttrain loss: 0.44467119713748193\n",
      "\ttrain loss: 0.3963514887570506\n",
      "\ttrain loss: 0.42050514441700604\n",
      "\ttrain loss: 0.3348665073710331\n",
      "\ttrain loss: 0.4011319794953331\n",
      "\ttrain loss: 0.6078009269734286\n",
      "\ttrain loss: 0.251705071601114\n",
      "\ttrain loss: 0.2926991629364321\n",
      "\ttrain loss: 0.31000954002047754\n",
      "\ttrain loss: 0.631657304546724\n",
      "\ttrain loss: 0.440327809530371\n",
      "\ttrain loss: 0.6496424553506255\n",
      "\ttrain loss: 0.5572260548394848\n",
      "\ttrain loss: 0.44214655442567047\n",
      "\ttrain loss: 0.34914791135868706\n",
      "\ttrain loss: 0.5833171151520293\n",
      "\ttrain loss: 0.4484213493540858\n",
      "\ttrain loss: 0.502094122581764\n",
      "\ttrain loss: 0.4906984096782288\n",
      "\ttrain loss: 0.5422056851738132\n",
      "\ttrain loss: 0.36061585615834113\n",
      "\ttrain loss: 0.34231892310521894\n",
      "\ttrain loss: 0.5385799777188379\n",
      "\ttrain loss: 0.3739829762064065\n",
      "\ttrain loss: 0.3900836622350639\n",
      "\ttrain loss: 0.3718191622703595\n",
      "\ttrain loss: 0.5111099302542157\n",
      "\ttrain loss: 0.5525264626665307\n",
      "\ttrain loss: 0.5132370681982961\n",
      "\ttrain loss: 0.43853147957341265\n",
      "\ttrain loss: 0.4002097919337975\n",
      "\ttrain loss: 0.2517480815573493\n",
      "\ttrain loss: 0.4200673197313075\n",
      "\ttrain loss: 0.5745337518278896\n",
      "\ttrain loss: 0.5234585512940853\n",
      "\ttrain loss: 0.3220955533271031\n",
      "\ttrain loss: 0.3311384513804424\n",
      "\ttrain loss: 0.41013158501191893\n",
      "\ttrain loss: 0.48964274164082\n",
      "\ttrain loss: 0.3782324437618567\n",
      "\ttrain loss: 0.5429018651581093\n",
      "\ttrain loss: 0.3614550941956652\n",
      "\ttrain loss: 0.4033104887930138\n",
      "\ttrain loss: 0.37538646734739556\n",
      "\ttrain loss: 0.4720388722776436\n",
      "\ttrain loss: 0.38942683619983526\n",
      "\ttrain loss: 0.42488657379520856\n",
      "\ttrain loss: 0.5865316962303656\n",
      "\ttrain loss: 0.4214387186515331\n",
      "\ttrain loss: 0.4539073622281381\n",
      "\ttrain loss: 0.447465539583573\n",
      "\ttrain loss: 0.3829603718809086\n",
      "\ttrain loss: 0.5284512310231604\n",
      "\ttrain loss: 0.3861092358301134\n",
      "\ttrain loss: 0.3589605527912203\n",
      "\ttrain loss: 0.434669605022375\n",
      "\ttrain loss: 0.4117788317884382\n",
      "\ttrain loss: 0.40211415609771495\n",
      "\ttrain loss: 0.4505598524776408\n",
      "\ttrain loss: 0.2970834497907313\n",
      "\ttrain loss: 0.521850798366599\n",
      "\ttrain loss: 0.3742227661801011\n",
      "\ttrain loss: 0.431830691704219\n",
      "\ttrain loss: 0.43232101962728553\n",
      "\ttrain loss: 0.3652023095639194\n",
      "\ttrain loss: 0.5115438237969442\n",
      "\ttrain loss: 0.548507368840953\n",
      "\ttrain loss: 0.39915965707899925\n",
      "\ttrain loss: 0.8145975082101231\n",
      "\ttrain loss: 0.40045144318312925\n",
      "\ttrain loss: 0.5685450858209299\n",
      "\ttrain loss: 0.519612613232161\n",
      "\ttrain loss: 0.3207886008458844\n",
      "\ttrain loss: 0.31694845390274695\n",
      "\ttrain loss: 0.3165288880224605\n",
      "\ttrain loss: 0.3679714592023524\n",
      "\ttrain loss: 0.3594788779112829\n",
      "\ttrain loss: 0.48639995383477064\n",
      "\ttrain loss: 0.4624727643108959\n",
      "\ttrain loss: 0.39843996541732024\n",
      "\ttrain loss: 0.5591401578321868\n",
      "\ttrain loss: 0.34817830147145334\n",
      "\ttrain loss: 0.4206779904655745\n",
      "\ttrain loss: 0.2701570748619283\n",
      "\ttrain loss: 0.42897932093767677\n",
      "\ttrain loss: 0.4750154503703854\n",
      "\ttrain loss: 0.4955381200306609\n",
      "\ttrain loss: 0.44524888428872356\n",
      "\ttrain loss: 0.554148182254631\n",
      "\ttrain loss: 0.3486207624817176\n",
      "\ttrain loss: 0.6903529196378082\n",
      "\ttrain loss: 0.4767624834340404\n",
      "\ttrain loss: 0.6500694534243526\n",
      "\ttrain loss: 0.5318160323357489\n",
      "\ttrain loss: 0.3942331590076465\n",
      "\ttrain loss: 0.49429641215458425\n",
      "\ttrain loss: 0.7486311703809987\n",
      "\ttrain loss: 0.24407825543949416\n",
      "\ttrain loss: 0.2350530680334315\n",
      "\ttrain loss: 0.43640847086912904\n",
      "\ttrain loss: 0.4640095807853264\n",
      "\ttrain loss: 0.32748818526543777\n",
      "\ttrain loss: 0.37957437573995323\n",
      "\ttrain loss: 0.2968106464599642\n",
      "\ttrain loss: 0.4916624387400782\n",
      "\ttrain loss: 0.29198073957527876\n",
      "\ttrain loss: 0.3065215296479987\n",
      "\ttrain loss: 0.561581653231854\n",
      "\ttrain loss: 0.3655766133152369\n",
      "\ttrain loss: 0.3724082124413763\n",
      "\ttrain loss: 0.3655838472655787\n",
      "\ttrain loss: 0.5122711225817208\n",
      "\ttrain loss: 0.32547612536347165\n",
      "\ttrain loss: 0.38844418690079663\n",
      "\ttrain loss: 0.3604965225199006\n",
      "\ttrain loss: 0.3230625282077986\n",
      "\ttrain loss: 0.35240616727993357\n",
      "\ttrain loss: 0.5527665617730433\n",
      "\ttrain loss: 0.39477816720038406\n",
      "\ttrain loss: 0.45669745855202853\n",
      "\ttrain loss: 0.2835456695558249\n",
      "\ttrain loss: 0.40231126864924693\n",
      "\ttrain loss: 0.28964110786094216\n",
      "\ttrain loss: 0.47568871925683115\n",
      "\ttrain loss: 0.5076093215354753\n",
      "\ttrain loss: 0.3817445740112282\n",
      "\ttrain loss: 0.3254921954911731\n",
      "\ttrain loss: 0.45689186379995617\n",
      "\ttrain loss: 0.4487219932780495\n",
      "\ttrain loss: 0.4947222587271748\n",
      "\ttrain loss: 0.43107805928615844\n",
      "\ttrain loss: 0.36705558362501833\n",
      "\ttrain loss: 0.32229603507149407\n",
      "\ttrain loss: 0.37834386916673474\n",
      "\ttrain loss: 0.47225676818365503\n",
      "\ttrain loss: 0.5327515568836643\n",
      "\ttrain loss: 0.4783836344052762\n",
      "\ttrain loss: 0.3328156789479093\n",
      "\ttrain loss: 0.39185403368920224\n",
      "\ttrain loss: 0.40188802547306735\n",
      "\ttrain loss: 0.37909080918347404\n",
      "\ttrain loss: 0.28648921144557704\n",
      "\ttrain loss: 0.6004476998078885\n",
      "\ttrain loss: 0.314322778709612\n",
      "\ttrain loss: 0.5572981713946027\n",
      "\ttrain loss: 0.276557886666393\n",
      "\ttrain loss: 0.4192963075609766\n",
      "\ttrain loss: 0.40733197061234305\n",
      "\ttrain loss: 0.4717348971254819\n",
      "\ttrain loss: 0.37326407992588556\n",
      "\ttrain loss: 0.38765459579348693\n",
      "\ttrain loss: 0.3389090065870599\n",
      "\ttrain loss: 0.5798548012532335\n",
      "\ttrain loss: 0.4205893855235712\n",
      "\ttrain loss: 0.5005608967203403\n",
      "\ttrain loss: 0.4323129329841482\n",
      "\ttrain loss: 0.5199762554084786\n",
      "\ttrain loss: 0.24162184600091544\n",
      "\ttrain loss: 0.4319323930630204\n",
      "\ttrain loss: 0.5963294739147305\n",
      "\ttrain loss: 0.3803930812356908\n",
      "\ttrain loss: 0.4730039257832662\n",
      "\ttrain loss: 0.4078335936460778\n",
      "\ttrain loss: 0.49469579353831683\n",
      "\ttrain loss: 0.29176706704263095\n",
      "\ttrain loss: 0.4123659196358517\n",
      "\ttrain loss: 0.3610176445608404\n",
      "\ttrain loss: 0.40212073797981535\n",
      "\ttrain loss: 0.38607736225167777\n",
      "\ttrain loss: 0.27346899038925176\n",
      "\ttrain loss: 0.39086849959043307\n",
      "\ttrain loss: 0.3631653429374453\n",
      "\ttrain loss: 0.3290430152291193\n",
      "\ttrain loss: 0.38378223676310047\n",
      "\ttrain loss: 0.4185689438284018\n",
      "\ttrain loss: 0.37926793652949775\n",
      "\ttrain loss: 0.28650101404213013\n",
      "\ttrain loss: 0.4165571291361513\n",
      "\ttrain loss: 0.31026836955277093\n",
      "\ttrain loss: 0.3372422880857308\n",
      "\ttrain loss: 0.3928594671256336\n",
      "\ttrain loss: 0.5135723686264216\n",
      "\ttrain loss: 0.40904730277808493\n",
      "\ttrain loss: 0.3932691461302271\n",
      "\ttrain loss: 0.2904005879882383\n",
      "\ttrain loss: 0.3566408817553327\n",
      "\ttrain loss: 0.5056573899469456\n",
      "\ttrain loss: 0.6891715984906831\n",
      "\ttrain loss: 0.34230395710098993\n",
      "\ttrain loss: 0.4724799214804976\n",
      "\ttrain loss: 0.41049236914941994\n",
      "\ttrain loss: 0.3601291813416087\n",
      "\ttrain loss: 0.4159941957225555\n",
      "\ttrain loss: 0.47368968787231225\n",
      "\ttrain loss: 0.8076971631763841\n",
      "\ttrain loss: 0.6353476302603076\n",
      "\ttrain loss: 0.5328201552660831\n",
      "\ttrain loss: 0.4417552821615864\n",
      "\ttrain loss: 0.45137854053035414\n",
      "\ttrain loss: 0.2976686118110458\n",
      "\ttrain loss: 0.6358286905361685\n",
      "\ttrain loss: 0.3660788110101782\n",
      "\ttrain loss: 0.5773807960092641\n",
      "\ttrain loss: 0.27890961441975404\n",
      "\ttrain loss: 0.5380445430947105\n",
      "\ttrain loss: 0.42713400455423645\n",
      "\ttrain loss: 0.41185154005565155\n",
      "\ttrain loss: 0.3805673078410713\n",
      "\ttrain loss: 0.40384161785280326\n",
      "\ttrain loss: 0.4845615822249213\n",
      "\ttrain loss: 0.4378304039338463\n",
      "\ttrain loss: 0.6145098627850429\n",
      "\ttrain loss: 0.4139851174439854\n",
      "\ttrain loss: 0.3335950668897328\n",
      "\ttrain loss: 0.4485296939918047\n",
      "\ttrain loss: 0.2930653070471979\n",
      "\ttrain loss: 0.43026718820058785\n",
      "\ttrain loss: 0.5015977398547453\n",
      "\ttrain loss: 0.31577724005760494\n",
      "\ttrain loss: 0.41023258524168205\n",
      "\ttrain loss: 0.4527919163951142\n",
      "\ttrain loss: 0.34137957640357103\n",
      "\ttrain loss: 0.36654362773260907\n",
      "\ttrain loss: 0.40607828144230856\n",
      "\ttrain loss: 0.42243348028360805\n",
      "\ttrain loss: 0.5112010254409982\n",
      "\ttrain loss: 0.327229120418568\n",
      "\ttrain loss: 0.5270277559022699\n",
      "\ttrain loss: 0.45161169956201275\n",
      "\ttrain loss: 0.31294919646766794\n",
      "\ttrain loss: 0.4154836275796303\n",
      "\ttrain loss: 0.4224554625964544\n",
      "\ttrain loss: 0.2343848249308597\n",
      "\ttrain loss: 0.4876418781466254\n",
      "\ttrain loss: 0.5118514785670347\n",
      "\ttrain loss: 0.598215577470223\n",
      "\ttrain loss: 0.2943735325197296\n",
      "\ttrain loss: 0.3419483539647889\n",
      "\ttrain loss: 0.39591514346584555\n",
      "\ttrain loss: 0.40714462352506864\n",
      "\ttrain loss: 0.5937098833424477\n",
      "\ttrain loss: 0.533588545963837\n",
      "\ttrain loss: 0.357662290243872\n",
      "\ttrain loss: 0.6016937500949757\n",
      "\ttrain loss: 0.3361982268357458\n",
      "\ttrain loss: 0.37251819900137095\n",
      "\ttrain loss: 0.47113744509099553\n",
      "\ttrain loss: 0.5434816733309267\n",
      "\ttrain loss: 0.26050567051223084\n",
      "\ttrain loss: 0.4197162480768536\n",
      "\ttrain loss: 0.5349436981536418\n",
      "\ttrain loss: 0.3547075579517186\n",
      "\ttrain loss: 0.33815727913139093\n",
      "\ttrain loss: 0.4230033065256979\n",
      "\ttrain loss: 0.3150532042736152\n",
      "\ttrain loss: 0.28207254036256635\n",
      "\ttrain loss: 0.4081353644841592\n",
      "\ttrain loss: 0.5081692966568899\n",
      "\ttrain loss: 0.4242024334787983\n",
      "\ttrain loss: 0.4247154214146486\n",
      "\ttrain loss: 0.46927860435677426\n",
      "\ttrain loss: 0.33949921011860174\n",
      "\ttrain loss: 0.36149921933788365\n",
      "\ttrain loss: 0.40566008246342145\n",
      "\ttrain loss: 0.5848030716734082\n",
      "\ttrain loss: 0.40307582123332303\n",
      "\ttrain loss: 0.383260079267013\n",
      "\ttrain loss: 0.44260993589282005\n",
      "\ttrain loss: 0.4518031956992623\n",
      "\ttrain loss: 0.7192734062813175\n",
      "\ttrain loss: 0.29198131693420726\n",
      "\ttrain loss: 0.37376699430083826\n",
      "\ttrain loss: 0.2834392539959842\n",
      "\ttrain loss: 0.45809897498961427\n",
      "\ttrain loss: 0.45651569867923336\n",
      "\ttrain loss: 0.4236778639949018\n",
      "\ttrain loss: 0.3773199174346433\n",
      "\ttrain loss: 0.45694967682238335\n",
      "\ttrain loss: 0.4049388474743826\n",
      "\ttrain loss: 0.2875703155092516\n",
      "\ttrain loss: 0.7718858245980686\n",
      "\ttrain loss: 0.46872222311821654\n",
      "\ttrain loss: 0.4540607107039194\n",
      "\ttrain loss: 0.33795596755760676\n",
      "\ttrain loss: 0.5050465755278969\n",
      "\ttrain loss: 0.3846667758808473\n",
      "\ttrain loss: 0.4499875207339323\n",
      "\ttrain loss: 0.7959267232057372\n",
      "\ttrain loss: 0.42711745122454436\n",
      "\ttrain loss: 0.6364198046683092\n",
      "\ttrain loss: 0.4794343116451721\n",
      "\ttrain loss: 0.44582480461192775\n",
      "\ttrain loss: 0.5184287644759403\n",
      "\ttrain loss: 0.40595223313175766\n",
      "\ttrain loss: 0.3582775848507702\n",
      "\ttrain loss: 0.43814528909173933\n",
      "\ttrain loss: 0.31458428090567847\n",
      "\ttrain loss: 0.4132358899231776\n",
      "\ttrain loss: 0.46172423762730586\n",
      "\ttrain loss: 0.5131550215678315\n",
      "\ttrain loss: 0.5140640573586824\n",
      "\ttrain loss: 0.4883113465759489\n",
      "\ttrain loss: 0.4224108665104349\n",
      "\ttrain loss: 0.4190807234679947\n",
      "\ttrain loss: 0.514395589754524\n",
      "\ttrain loss: 0.5012946757438711\n",
      "\ttrain loss: 0.39367125314530693\n",
      "\ttrain loss: 0.4381271775358068\n",
      "\ttrain loss: 0.40104949717133365\n",
      "\ttrain loss: 0.4241102588722058\n",
      "\ttrain loss: 0.46842223645586956\n",
      "\ttrain loss: 0.34001627296621\n",
      "\ttrain loss: 0.2997525148014628\n",
      "\ttrain loss: 0.3201233198047655\n",
      "\ttrain loss: 0.41802421085694985\n",
      "\ttrain loss: 0.4140841765750949\n",
      "\ttrain loss: 0.6621066363037179\n",
      "\ttrain loss: 0.4175413169025085\n",
      "\ttrain loss: 0.5062079386859646\n",
      "\ttrain loss: 0.6166401035019677\n",
      "\ttrain loss: 0.2656092161693596\n",
      "\ttrain loss: 0.4439793402029572\n",
      "\ttrain loss: 0.34296747887990275\n",
      "\ttrain loss: 0.3947958377240607\n",
      "\ttrain loss: 0.5664764871378588\n",
      "\ttrain loss: 0.49131017135175814\n",
      "\ttrain loss: 0.6199760908718017\n",
      "\ttrain loss: 0.36984024828133477\n",
      "\ttrain loss: 0.34969173305420764\n",
      "\ttrain loss: 0.31042435951469777\n",
      "\ttrain loss: 0.3629499155230981\n",
      "\ttrain loss: 0.5522575640497618\n",
      "\ttrain loss: 0.5210223596255636\n",
      "\ttrain loss: 0.47225951957271084\n",
      "\ttrain loss: 0.42615928588448315\n",
      "\ttrain loss: 0.2837786630115268\n",
      "\ttrain loss: 0.4651892374330791\n",
      "\ttrain loss: 0.4629751688628443\n",
      "\ttrain loss: 0.355358911238834\n",
      "\ttrain loss: 0.44795915662851205\n",
      "\ttrain loss: 0.600560512129876\n",
      "\ttrain loss: 0.5174049957809967\n",
      "\ttrain loss: 0.4489616496566974\n",
      "\ttrain loss: 0.4900414082036864\n",
      "\ttrain loss: 0.4770435819067347\n",
      "\ttrain loss: 0.3329348059283639\n",
      "\ttrain loss: 0.4597072395877668\n",
      "\ttrain loss: 0.3263287159329199\n",
      "\ttrain loss: 0.3114622374175151\n",
      "\ttrain loss: 0.29838622461575404\n",
      "\ttrain loss: 0.4382312286679152\n",
      "\ttrain loss: 0.394309150801767\n",
      "\ttrain loss: 0.4483463000401582\n",
      "\ttrain loss: 0.3147818702897856\n",
      "\ttrain loss: 0.36325372747969653\n",
      "\ttrain loss: 0.4250807519814591\n",
      "\ttrain loss: 0.47898777523192726\n",
      "\ttrain loss: 0.20744809411348042\n",
      "\ttrain loss: 0.46435133978781645\n",
      "\ttrain loss: 0.39553087641012064\n",
      "\ttrain loss: 0.5155143388176833\n",
      "\ttrain loss: 0.3768130325443333\n",
      "\ttrain loss: 0.34050927626125516\n",
      "\ttrain loss: 0.35510848259756\n",
      "\ttrain loss: 0.5005059373940214\n",
      "\ttrain loss: 0.4396269120159715\n",
      "\ttrain loss: 0.32718500635490266\n",
      "\ttrain loss: 0.6321021222726024\n",
      "\ttrain loss: 0.3801793031380278\n",
      "\ttrain loss: 0.3464976357918924\n",
      "\ttrain loss: 0.3763507954038938\n",
      "\ttrain loss: 0.548046729276903\n",
      "\ttrain loss: 0.4418607543569488\n",
      "\ttrain loss: 0.4172034581116815\n",
      "\ttrain loss: 0.36906849747496506\n",
      "\ttrain loss: 0.3375117208189868\n",
      "\ttrain loss: 0.3277379954999141\n",
      "\ttrain loss: 0.2804864671878321\n",
      "\ttrain loss: 0.2927299386815223\n",
      "\ttrain loss: 0.4170570020058429\n",
      "\ttrain loss: 0.38938220744710245\n",
      "\ttrain loss: 0.49235990234004356\n",
      "\ttrain loss: 0.4848469869092098\n",
      "\ttrain loss: 0.4726458399219939\n",
      "\ttrain loss: 0.4381814375016085\n",
      "\ttrain loss: 0.4222982925262375\n",
      "\ttrain loss: 0.37861785336977777\n",
      "\ttrain loss: 0.4194301039483863\n",
      "\ttrain loss: 0.29387944193168974\n",
      "\ttrain loss: 0.5102897411154653\n",
      "\ttrain loss: 0.5972702021356876\n",
      "\ttrain loss: 0.3999846947447395\n",
      "\ttrain loss: 0.4452515344099603\n",
      "\ttrain loss: 0.4381931601988157\n",
      "\ttrain loss: 0.3351270805945237\n",
      "\ttrain loss: 0.4030451058690374\n",
      "\ttrain loss: 0.23987042458618152\n",
      "\ttrain loss: 0.385204722080046\n",
      "\ttrain loss: 0.5535500338062357\n",
      "\ttrain loss: 0.263549056385047\n",
      "\ttrain loss: 0.34211036820423146\n",
      "\ttrain loss: 0.4257435500938426\n",
      "\ttrain loss: 0.449335119654108\n",
      "\ttrain loss: 0.35615986240177344\n",
      "\ttrain loss: 0.4197779698939385\n",
      "\ttrain loss: 0.6332593214620514\n",
      "\ttrain loss: 0.4885510259366575\n",
      "\ttrain loss: 0.5069852209938089\n",
      "\ttrain loss: 0.4407205507254214\n",
      "\ttrain loss: 0.4266070707413637\n",
      "\ttrain loss: 0.46369502136158786\n",
      "\ttrain loss: 0.600945760555367\n",
      "\ttrain loss: 0.3465240594126572\n",
      "\ttrain loss: 0.30162408751523967\n",
      "\ttrain loss: 0.4313222075808988\n",
      "\ttrain loss: 0.4319433219502733\n",
      "\ttrain loss: 0.38714825111146967\n",
      "\ttrain loss: 0.41394963405034113\n",
      "\ttrain loss: 0.5959914130957291\n",
      "\ttrain loss: 0.5391883283270129\n",
      "\ttrain loss: 0.28991395132755954\n",
      "\ttrain loss: 0.44690191441409655\n",
      "\ttrain loss: 0.45016362878169935\n",
      "\ttrain loss: 0.5081685458906672\n",
      "\ttrain loss: 0.36475491457982223\n",
      "\ttrain loss: 0.35752948949304153\n",
      "\ttrain loss: 0.38212981034578386\n",
      "\ttrain loss: 0.4312121441738492\n",
      "\ttrain loss: 0.4840229501342843\n",
      "\ttrain loss: 0.41797655562668157\n",
      "\ttrain loss: 0.3947789511675118\n",
      "\ttrain loss: 0.31154796576004606\n",
      "\ttrain loss: 0.4095300006873742\n",
      "\ttrain loss: 0.39155627046437214\n",
      "\ttrain loss: 0.2960032530616624\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(6/15) is saved!\n",
      "\ttrain loss: 0.5708020126627726\n",
      "\ttrain loss: 0.30106091255454837\n",
      "\ttrain loss: 0.34523809703078406\n",
      "\ttrain loss: 0.38210122798740476\n",
      "\ttrain loss: 0.3458493898901387\n",
      "\ttrain loss: 0.38261880738790516\n",
      "\ttrain loss: 0.5760384017742584\n",
      "\ttrain loss: 0.35949514285473\n",
      "\ttrain loss: 0.4742451779297211\n",
      "\ttrain loss: 0.3523504877206872\n",
      "\ttrain loss: 0.5172505903181427\n",
      "\ttrain loss: 0.5382641767880575\n",
      "\ttrain loss: 0.3593515084306944\n",
      "\ttrain loss: 0.23880799067482536\n",
      "\ttrain loss: 0.38052753197122646\n",
      "\ttrain loss: 0.30372061076949547\n",
      "\ttrain loss: 0.3835021859314971\n",
      "\ttrain loss: 0.2577067002434692\n",
      "\ttrain loss: 0.6845859916824967\n",
      "\ttrain loss: 0.4406779357181569\n",
      "\ttrain loss: 0.3344111032121966\n",
      "\ttrain loss: 0.4276049549403602\n",
      "\ttrain loss: 0.5616521257216509\n",
      "\ttrain loss: 0.460066948501927\n",
      "\ttrain loss: 0.37498088707381205\n",
      "\ttrain loss: 0.5306179819799298\n",
      "\ttrain loss: 0.3521502419652768\n",
      "\ttrain loss: 0.4519945509826133\n",
      "\ttrain loss: 0.4782002871126274\n",
      "\ttrain loss: 0.532188312543548\n",
      "\ttrain loss: 0.29565434063211976\n",
      "\ttrain loss: 0.39177193872611416\n",
      "\ttrain loss: 0.41153150727935545\n",
      "\ttrain loss: 0.4753891143260116\n",
      "\ttrain loss: 0.4570323995313623\n",
      "\ttrain loss: 0.44423997970853607\n",
      "\ttrain loss: 0.5594288083213712\n",
      "\ttrain loss: 0.25814648493343845\n",
      "\ttrain loss: 0.3937772992840004\n",
      "\ttrain loss: 0.2624505020603136\n",
      "\ttrain loss: 0.35909291593711656\n",
      "\ttrain loss: 0.4568272012695469\n",
      "\ttrain loss: 0.49751314733438035\n",
      "\ttrain loss: 0.5080467028474847\n",
      "\ttrain loss: 0.31278515417551167\n",
      "\ttrain loss: 0.38821037371462264\n",
      "\ttrain loss: 0.45774059520301824\n",
      "\ttrain loss: 0.3940780085778678\n",
      "\ttrain loss: 0.27852917321679016\n",
      "\ttrain loss: 0.35251605019290166\n",
      "\ttrain loss: 0.3235237580520184\n",
      "\ttrain loss: 0.3738275908051557\n",
      "\ttrain loss: 0.49166020272003824\n",
      "\ttrain loss: 0.32632580350684176\n",
      "\ttrain loss: 0.34327463149311355\n",
      "\ttrain loss: 0.4604512612739545\n",
      "\ttrain loss: 0.3532108753555103\n",
      "\ttrain loss: 0.3149438701916113\n",
      "\ttrain loss: 0.3373031365604063\n",
      "\ttrain loss: 0.49102420741601926\n",
      "\ttrain loss: 0.5943412935325805\n",
      "\ttrain loss: 0.42199349619812343\n",
      "\ttrain loss: 0.3532565622492763\n",
      "\ttrain loss: 0.5417480588524509\n",
      "\ttrain loss: 0.3975603808720004\n",
      "\ttrain loss: 0.4278484746755711\n",
      "\ttrain loss: 0.3813280058664635\n",
      "\ttrain loss: 0.3239744484382074\n",
      "\ttrain loss: 0.3218576436973579\n",
      "\ttrain loss: 0.36801349521363264\n",
      "\ttrain loss: 0.6986566706602493\n",
      "\ttrain loss: 0.5071918078614484\n",
      "\ttrain loss: 0.33493739926000365\n",
      "\ttrain loss: 0.6315362290085078\n",
      "\ttrain loss: 0.3863324968313355\n",
      "\ttrain loss: 0.32123849543080185\n",
      "\ttrain loss: 0.41052890203831605\n",
      "\ttrain loss: 0.42847476079493285\n",
      "\ttrain loss: 0.41124101413752073\n",
      "\ttrain loss: 0.35947406253543834\n",
      "\ttrain loss: 0.5034507488839761\n",
      "\ttrain loss: 0.4003418897304199\n",
      "\ttrain loss: 0.4466769472259865\n",
      "\ttrain loss: 0.3942290740053147\n",
      "\ttrain loss: 0.48669472782636647\n",
      "\ttrain loss: 0.400975057722206\n",
      "\ttrain loss: 0.5507355957604798\n",
      "\ttrain loss: 0.5676697996866135\n",
      "\ttrain loss: 0.4112714635218757\n",
      "\ttrain loss: 0.41412242854157466\n",
      "\ttrain loss: 0.42371111313871457\n",
      "\ttrain loss: 0.4368066104281399\n",
      "\ttrain loss: 0.3898960923667387\n",
      "\ttrain loss: 0.5586034085668041\n",
      "\ttrain loss: 0.4336573286054927\n",
      "\ttrain loss: 0.46114727162227775\n",
      "\ttrain loss: 0.3238643653086538\n",
      "\ttrain loss: 0.40123803317800677\n",
      "\ttrain loss: 0.3788960532658765\n",
      "\ttrain loss: 0.301698685958187\n",
      "\ttrain loss: 0.2832049557639959\n",
      "\ttrain loss: 0.4376237575681726\n",
      "\ttrain loss: 0.4226473950235893\n",
      "\ttrain loss: 0.2646673158366496\n",
      "\ttrain loss: 0.3578773583718983\n",
      "\ttrain loss: 0.4935444135598912\n",
      "\ttrain loss: 0.3159650897043411\n",
      "\ttrain loss: 0.2960485662143451\n",
      "\ttrain loss: 0.40778035702452253\n",
      "\ttrain loss: 0.35900508520861446\n",
      "\ttrain loss: 0.4725417395799354\n",
      "\ttrain loss: 0.37639322331038627\n",
      "\ttrain loss: 0.3713818249693568\n",
      "\ttrain loss: 0.4334933969521475\n",
      "\ttrain loss: 0.44939260781790674\n",
      "\ttrain loss: 0.42117819613360996\n",
      "\ttrain loss: 0.33353907647429626\n",
      "\ttrain loss: 0.3975614526147104\n",
      "\ttrain loss: 0.3375323044127355\n",
      "\ttrain loss: 0.3539099281743606\n",
      "\ttrain loss: 0.594598309725169\n",
      "\ttrain loss: 0.3940416122724711\n",
      "\ttrain loss: 0.4358431930689798\n",
      "\ttrain loss: 0.38151452766493865\n",
      "\ttrain loss: 0.44105965588379004\n",
      "\ttrain loss: 0.4673719951108156\n",
      "\ttrain loss: 0.5471252891209412\n",
      "\ttrain loss: 0.4146894925218264\n",
      "\ttrain loss: 0.4652371120731054\n",
      "\ttrain loss: 0.4599056686162281\n",
      "\ttrain loss: 0.32765252524060606\n",
      "\ttrain loss: 0.627474935583865\n",
      "\ttrain loss: 0.43579263142015795\n",
      "\ttrain loss: 0.40164533057474844\n",
      "\ttrain loss: 0.4266428995091298\n",
      "\ttrain loss: 0.319599758055497\n",
      "\ttrain loss: 0.5352257621170607\n",
      "\ttrain loss: 0.32786526726973864\n",
      "\ttrain loss: 0.4738287626040947\n",
      "\ttrain loss: 0.30074701161450257\n",
      "\ttrain loss: 0.503323817101669\n",
      "\ttrain loss: 0.6623553263089168\n",
      "\ttrain loss: 0.38994241397468776\n",
      "\ttrain loss: 0.47124010029072577\n",
      "\ttrain loss: 0.4867813562866576\n",
      "\ttrain loss: 0.363827214289553\n",
      "\ttrain loss: 0.3820393313119831\n",
      "\ttrain loss: 0.4442360051325491\n",
      "\ttrain loss: 0.5691667210779574\n",
      "\ttrain loss: 0.5074046563790775\n",
      "\ttrain loss: 0.41659658098502633\n",
      "\ttrain loss: 0.42621470201026157\n",
      "\ttrain loss: 0.30603517646150036\n",
      "\ttrain loss: 0.5964826992584612\n",
      "\ttrain loss: 0.3687042003288\n",
      "\ttrain loss: 0.3973834360978893\n",
      "\ttrain loss: 0.4874648414084365\n",
      "\ttrain loss: 0.33451812374810797\n",
      "\ttrain loss: 0.35267197360942604\n",
      "\ttrain loss: 0.32224258078715284\n",
      "\ttrain loss: 0.4100568733415892\n",
      "\ttrain loss: 0.35185710792308583\n",
      "\ttrain loss: 0.5085037156627474\n",
      "\ttrain loss: 0.3250825881439409\n",
      "\ttrain loss: 0.49051392928489074\n",
      "\ttrain loss: 0.4476807604254888\n",
      "\ttrain loss: 0.3525476047952727\n",
      "\ttrain loss: 0.4492310743089022\n",
      "\ttrain loss: 0.5068657819693725\n",
      "\ttrain loss: 0.3929004287104755\n",
      "\ttrain loss: 0.5114422572454609\n",
      "\ttrain loss: 0.37460877078916155\n",
      "\ttrain loss: 0.4176229879352714\n",
      "\ttrain loss: 0.3736053261165665\n",
      "\ttrain loss: 0.4011082859610371\n",
      "\ttrain loss: 0.5374725548939313\n",
      "\ttrain loss: 0.37850331590220687\n",
      "\ttrain loss: 0.4477740772063509\n",
      "\ttrain loss: 0.46066964255393156\n",
      "\ttrain loss: 0.4583632192551182\n",
      "\ttrain loss: 0.3170158551060428\n",
      "\ttrain loss: 0.3527743324352437\n",
      "\ttrain loss: 0.41717783250359675\n",
      "\ttrain loss: 0.4533307606567372\n",
      "\ttrain loss: 0.2984940074468044\n",
      "\ttrain loss: 0.6419597948360013\n",
      "\ttrain loss: 0.4628655940469819\n",
      "\ttrain loss: 0.38904450477273467\n",
      "\ttrain loss: 0.29471435411061736\n",
      "\ttrain loss: 0.4076199048574768\n",
      "\ttrain loss: 0.5780273605821753\n",
      "\ttrain loss: 0.6534848097245064\n",
      "\ttrain loss: 0.4072207962985118\n",
      "\ttrain loss: 0.4039983568952582\n",
      "\ttrain loss: 0.35781338014807623\n",
      "\ttrain loss: 0.5734686003515258\n",
      "\ttrain loss: 0.35335885096002523\n",
      "\ttrain loss: 0.39545780184074963\n",
      "\ttrain loss: 0.3094013968076984\n",
      "\ttrain loss: 0.3804175955561624\n",
      "\ttrain loss: 0.43847118741577806\n",
      "\ttrain loss: 0.4178181632708613\n",
      "\ttrain loss: 0.3679742640656872\n",
      "\ttrain loss: 0.34196196504184506\n",
      "\ttrain loss: 0.2508349714663266\n",
      "\ttrain loss: 0.6038204656139292\n",
      "\ttrain loss: 0.35126847738353423\n",
      "\ttrain loss: 0.5627471119156611\n",
      "\ttrain loss: 0.49578467797334336\n",
      "\ttrain loss: 0.5938996041624534\n",
      "\ttrain loss: 0.3405668974254671\n",
      "\ttrain loss: 0.24853054622680928\n",
      "\ttrain loss: 0.6127340976397542\n",
      "\ttrain loss: 0.3648494625139947\n",
      "\ttrain loss: 0.2890185881405939\n",
      "\ttrain loss: 0.37051011058082384\n",
      "\ttrain loss: 0.4770444317941709\n",
      "\ttrain loss: 0.37076722441573723\n",
      "\ttrain loss: 0.5082643776401892\n",
      "\ttrain loss: 0.30415959716108143\n",
      "\ttrain loss: 0.3091392452342163\n",
      "\ttrain loss: 0.48231696890047654\n",
      "\ttrain loss: 0.4515098123552297\n",
      "\ttrain loss: 0.4477722211917383\n",
      "\ttrain loss: 0.37635646475604134\n",
      "\ttrain loss: 0.46501918754313043\n",
      "\ttrain loss: 0.40199613393001854\n",
      "\ttrain loss: 0.3623164313364764\n",
      "\ttrain loss: 0.7461716395285091\n",
      "\ttrain loss: 0.36429799214331227\n",
      "\ttrain loss: 0.40524499862055774\n",
      "\ttrain loss: 0.33044694365527316\n",
      "\ttrain loss: 0.2327736979426449\n",
      "\ttrain loss: 0.4868676020810072\n",
      "\ttrain loss: 0.47902469212373366\n",
      "\ttrain loss: 0.3242055603722628\n",
      "\ttrain loss: 0.583963929027415\n",
      "\ttrain loss: 0.8627258776732276\n",
      "\ttrain loss: 0.41192253432628273\n",
      "\ttrain loss: 0.47424957487307906\n",
      "\ttrain loss: 0.3177047235198636\n",
      "\ttrain loss: 0.4400196626082057\n",
      "\ttrain loss: 0.39212390803990327\n",
      "\ttrain loss: 0.5898285614248656\n",
      "\ttrain loss: 0.5529836788304388\n",
      "\ttrain loss: 0.5035850389567627\n",
      "\ttrain loss: 0.46700872028624996\n",
      "\ttrain loss: 0.293465435407417\n",
      "\ttrain loss: 0.4129387484838315\n",
      "\ttrain loss: 0.4935976050236168\n",
      "\ttrain loss: 0.8015792038457561\n",
      "\ttrain loss: 0.4325697492502923\n",
      "\ttrain loss: 0.46766438723767534\n",
      "\ttrain loss: 0.5658359358426149\n",
      "\ttrain loss: 0.43187055862312795\n",
      "\ttrain loss: 0.3976114551110807\n",
      "\ttrain loss: 0.2777534937085986\n",
      "\ttrain loss: 0.4859980578114686\n",
      "\ttrain loss: 0.9633468747372963\n",
      "\ttrain loss: 0.4464679363595858\n",
      "\ttrain loss: 0.47007152632973603\n",
      "\ttrain loss: 0.5072303039139635\n",
      "\ttrain loss: 0.38026105292648854\n",
      "\ttrain loss: 0.46563925381419097\n",
      "\ttrain loss: 0.38627855162184704\n",
      "\ttrain loss: 0.5647946639936472\n",
      "\ttrain loss: 0.3323301378873246\n",
      "\ttrain loss: 0.41323584653782697\n",
      "\ttrain loss: 0.44509267307698885\n",
      "\ttrain loss: 0.36825624857875944\n",
      "\ttrain loss: 0.4517136744707172\n",
      "\ttrain loss: 0.42486163263444654\n",
      "\ttrain loss: 0.44133637857244595\n",
      "\ttrain loss: 0.6246400932978854\n",
      "\ttrain loss: 0.5180697498710525\n",
      "\ttrain loss: 0.48375186442791707\n",
      "\ttrain loss: 0.6066228323013914\n",
      "\ttrain loss: 0.31087898779193873\n",
      "\ttrain loss: 0.448594952873313\n",
      "\ttrain loss: 0.3548689982791531\n",
      "\ttrain loss: 0.47441838687076454\n",
      "\ttrain loss: 0.3221336800828065\n",
      "\ttrain loss: 0.3863726238278885\n",
      "\ttrain loss: 0.32281643759926826\n",
      "\ttrain loss: 0.4617303696201044\n",
      "\ttrain loss: 0.2943660556639126\n",
      "\ttrain loss: 0.4021142296444867\n",
      "\ttrain loss: 0.4155725302451526\n",
      "\ttrain loss: 0.37674997300433904\n",
      "\ttrain loss: 0.2668839206909175\n",
      "\ttrain loss: 0.3492948315487378\n",
      "\ttrain loss: 0.5399306645532371\n",
      "\ttrain loss: 0.3478009426007689\n",
      "\ttrain loss: 0.353821024769742\n",
      "\ttrain loss: 0.2657221200699168\n",
      "\ttrain loss: 0.499216149301817\n",
      "\ttrain loss: 0.32425810826577894\n",
      "\ttrain loss: 0.38833914661405006\n",
      "\ttrain loss: 0.4342798250530553\n",
      "\ttrain loss: 0.556770778760411\n",
      "\ttrain loss: 0.2687899630819979\n",
      "\ttrain loss: 0.39386175254855554\n",
      "\ttrain loss: 0.44619590737769566\n",
      "\ttrain loss: 0.32570724429361864\n",
      "\ttrain loss: 0.42722579904137814\n",
      "\ttrain loss: 0.5033291833972615\n",
      "\ttrain loss: 0.4172654457781423\n",
      "\ttrain loss: 0.6467832825881732\n",
      "\ttrain loss: 0.427443528843951\n",
      "\ttrain loss: 0.5946065089537043\n",
      "\ttrain loss: 0.49354718581950197\n",
      "\ttrain loss: 0.6497368162306055\n",
      "\ttrain loss: 0.3865963238642962\n",
      "\ttrain loss: 0.4799716886835736\n",
      "\ttrain loss: 0.4697049532821491\n",
      "\ttrain loss: 0.34057675645917285\n",
      "\ttrain loss: 0.41590036174208633\n",
      "\ttrain loss: 0.45297638307348387\n",
      "\ttrain loss: 0.40437478080579037\n",
      "\ttrain loss: 0.44179232216536646\n",
      "\ttrain loss: 0.33995057962504044\n",
      "\ttrain loss: 0.42597622059910856\n",
      "\ttrain loss: 0.227591323135696\n",
      "\ttrain loss: 0.59286602536145\n",
      "\ttrain loss: 0.3731673995276467\n",
      "\ttrain loss: 0.43773407652732776\n",
      "\ttrain loss: 0.3677366181240702\n",
      "\ttrain loss: 0.47568303522329214\n",
      "\ttrain loss: 0.37030104876623876\n",
      "\ttrain loss: 0.3426264128336691\n",
      "\ttrain loss: 0.41334988460884853\n",
      "\ttrain loss: 0.7696628880621861\n",
      "\ttrain loss: 0.3753949250406102\n",
      "\ttrain loss: 0.4264847726777287\n",
      "\ttrain loss: 0.48220548180323997\n",
      "\ttrain loss: 0.354329354208473\n",
      "\ttrain loss: 0.32384661355702216\n",
      "\ttrain loss: 0.5243756834598847\n",
      "\ttrain loss: 0.36857107587441434\n",
      "\ttrain loss: 0.45190053695548504\n",
      "\ttrain loss: 0.3946043068918778\n",
      "\ttrain loss: 0.386529229654336\n",
      "\ttrain loss: 0.2661388590260532\n",
      "\ttrain loss: 0.47815975924849924\n",
      "\ttrain loss: 0.5282151956826197\n",
      "\ttrain loss: 0.3953219671345475\n",
      "\ttrain loss: 0.6368250563455814\n",
      "\ttrain loss: 0.49043155711886866\n",
      "\ttrain loss: 0.4020856479485564\n",
      "\ttrain loss: 0.6773430717158508\n",
      "\ttrain loss: 0.35432375053842613\n",
      "\ttrain loss: 0.4381337926363828\n",
      "\ttrain loss: 0.3772610444639791\n",
      "\ttrain loss: 0.508083556194675\n",
      "\ttrain loss: 0.4492888663663818\n",
      "\ttrain loss: 0.2984592608927882\n",
      "\ttrain loss: 0.45240090055678944\n",
      "\ttrain loss: 0.4465084937939677\n",
      "\ttrain loss: 0.37275278337911405\n",
      "\ttrain loss: 0.46477556189904684\n",
      "\ttrain loss: 0.45400484830257004\n",
      "\ttrain loss: 0.3166706087735295\n",
      "\ttrain loss: 0.3856323699710082\n",
      "\ttrain loss: 0.40085876123634584\n",
      "\ttrain loss: 0.3490010727508621\n",
      "\ttrain loss: 0.2792452456894163\n",
      "\ttrain loss: 0.5208309542214382\n",
      "\ttrain loss: 0.2151209151165812\n",
      "\ttrain loss: 0.333613846428195\n",
      "\ttrain loss: 0.42675850565888346\n",
      "\ttrain loss: 0.46027644139436513\n",
      "\ttrain loss: 0.57063857606844\n",
      "\ttrain loss: 0.48468513337874797\n",
      "\ttrain loss: 0.434843703336144\n",
      "\ttrain loss: 0.4540243490258078\n",
      "\ttrain loss: 0.2511021467564709\n",
      "\ttrain loss: 0.40586464717365045\n",
      "\ttrain loss: 0.3312223799602534\n",
      "\ttrain loss: 0.4728491037395819\n",
      "\ttrain loss: 0.45332856450719883\n",
      "\ttrain loss: 0.35462883111982535\n",
      "\ttrain loss: 0.6577331331242047\n",
      "\ttrain loss: 0.3901835426367589\n",
      "\ttrain loss: 0.31209580024391675\n",
      "\ttrain loss: 0.31216490780480555\n",
      "\ttrain loss: 0.5798666876558685\n",
      "\ttrain loss: 0.4183813552032807\n",
      "\ttrain loss: 0.37682786835068893\n",
      "\ttrain loss: 0.4369606022404463\n",
      "\ttrain loss: 0.3329685831099521\n",
      "\ttrain loss: 0.3696775168532211\n",
      "\ttrain loss: 0.41132919510445987\n",
      "\ttrain loss: 0.4589310402683996\n",
      "\ttrain loss: 0.5296212121282164\n",
      "\ttrain loss: 0.422323101927838\n",
      "\ttrain loss: 0.30787673670776744\n",
      "\ttrain loss: 0.45927435776845904\n",
      "\ttrain loss: 0.42482190512626883\n",
      "\ttrain loss: 0.3804428887151177\n",
      "\ttrain loss: 0.34052700307725364\n",
      "\ttrain loss: 0.42673713217058207\n",
      "\ttrain loss: 0.3629590769954761\n",
      "\ttrain loss: 0.4528723206416035\n",
      "\ttrain loss: 0.3139590938407222\n",
      "\ttrain loss: 0.5359238652500924\n",
      "\ttrain loss: 0.3370836209799183\n",
      "\ttrain loss: 0.28302880837709044\n",
      "\ttrain loss: 0.29835664305628695\n",
      "\ttrain loss: 0.47289617431948927\n",
      "\ttrain loss: 0.2932383542964369\n",
      "\ttrain loss: 0.3090160496210088\n",
      "\ttrain loss: 0.43913286330157675\n",
      "\ttrain loss: 0.3100387670493149\n",
      "\ttrain loss: 0.4973377854613152\n",
      "\ttrain loss: 0.28694071506887664\n",
      "\ttrain loss: 0.28867190856046193\n",
      "\ttrain loss: 0.5291199381900058\n",
      "\ttrain loss: 0.47708942430755086\n",
      "\ttrain loss: 0.2414881790861664\n",
      "\ttrain loss: 0.5487098797573364\n",
      "\ttrain loss: 0.3356591211622679\n",
      "\ttrain loss: 0.33925976237555666\n",
      "\ttrain loss: 0.4004706189160142\n",
      "\ttrain loss: 0.3746509762576389\n",
      "\ttrain loss: 0.44571224557776545\n",
      "\ttrain loss: 0.42161921402860303\n",
      "\ttrain loss: 0.4117574837305229\n",
      "\ttrain loss: 0.3550041929986567\n",
      "\ttrain loss: 0.5460099088498822\n",
      "\ttrain loss: 0.47064661825215615\n",
      "\ttrain loss: 0.3961485628557555\n",
      "\ttrain loss: 0.24337551753156372\n",
      "\ttrain loss: 0.3669060069059251\n",
      "\ttrain loss: 0.3953634611433724\n",
      "\ttrain loss: 0.2111111796174438\n",
      "\ttrain loss: 0.3880258014961294\n",
      "\ttrain loss: 0.37712502911990564\n",
      "\ttrain loss: 0.6774856776359819\n",
      "\ttrain loss: 0.4324561127865889\n",
      "\ttrain loss: 0.535113598812957\n",
      "\ttrain loss: 0.4011696774426404\n",
      "\ttrain loss: 0.5001079268793471\n",
      "\ttrain loss: 0.5242221904169578\n",
      "\ttrain loss: 0.3505167047273603\n",
      "\ttrain loss: 0.3237660045915709\n",
      "\ttrain loss: 0.315447443492624\n",
      "\ttrain loss: 0.31480981147445497\n",
      "\ttrain loss: 0.3517746650394652\n",
      "\ttrain loss: 0.3245404707114329\n",
      "\ttrain loss: 0.37562777537167535\n",
      "\ttrain loss: 0.3839784917993422\n",
      "\ttrain loss: 0.5022632592755186\n",
      "\ttrain loss: 0.5578708573581743\n",
      "\ttrain loss: 0.3387235965855779\n",
      "\ttrain loss: 0.6573938544201099\n",
      "\ttrain loss: 0.470554396048983\n",
      "\ttrain loss: 0.4063203270706336\n",
      "\ttrain loss: 0.4517294848906098\n",
      "\ttrain loss: 0.4602726892969933\n",
      "\ttrain loss: 0.33720690185267876\n",
      "\ttrain loss: 0.2793578996927619\n",
      "\ttrain loss: 0.48477398232987506\n",
      "\ttrain loss: 0.40331927306116444\n",
      "\ttrain loss: 0.29126177531569963\n",
      "\ttrain loss: 0.3353529033062112\n",
      "\ttrain loss: 0.373786308362557\n",
      "\ttrain loss: 0.33806079212795803\n",
      "\ttrain loss: 0.34149280237284696\n",
      "\ttrain loss: 0.6132509583638459\n",
      "\ttrain loss: 0.3724205228166281\n",
      "\ttrain loss: 0.29549114494683215\n",
      "\ttrain loss: 0.4232176573803651\n",
      "\ttrain loss: 0.5247207084360126\n",
      "\ttrain loss: 0.3824177193265158\n",
      "\ttrain loss: 0.6077544239607402\n",
      "\ttrain loss: 0.39128855615983\n",
      "\ttrain loss: 0.5069844453765127\n",
      "\ttrain loss: 0.4181883881634156\n",
      "\ttrain loss: 0.330795395155038\n",
      "\ttrain loss: 0.28859168594065976\n",
      "\ttrain loss: 0.4556984257473046\n",
      "\ttrain loss: 0.4882285551302613\n",
      "\ttrain loss: 0.45962931996516476\n",
      "\ttrain loss: 0.4886725613920364\n",
      "\ttrain loss: 0.43003665590245754\n",
      "\ttrain loss: 0.2613783541493292\n",
      "\ttrain loss: 0.38902127941797726\n",
      "\ttrain loss: 0.31956979248941053\n",
      "\ttrain loss: 0.5808504013671069\n",
      "\ttrain loss: 0.3781335233033444\n",
      "\ttrain loss: 0.25373974016498607\n",
      "\ttrain loss: 0.6460758790403422\n",
      "\ttrain loss: 0.44079371942948037\n",
      "\ttrain loss: 0.35186201979481563\n",
      "\ttrain loss: 0.36070407633370516\n",
      "\ttrain loss: 0.4064237885412708\n",
      "\ttrain loss: 0.5316501799208245\n",
      "\ttrain loss: 0.5236777240422521\n",
      "\ttrain loss: 0.54595040623999\n",
      "\ttrain loss: 0.33914945289658915\n",
      "\ttrain loss: 0.4185530518937566\n",
      "\ttrain loss: 0.4712501037879816\n",
      "\ttrain loss: 0.6386152911033345\n",
      "\ttrain loss: 0.32594584694798334\n",
      "\ttrain loss: 0.38773964610027956\n",
      "\ttrain loss: 0.3540514572618782\n",
      "\ttrain loss: 0.3035697990393848\n",
      "\ttrain loss: 0.3057656702868029\n",
      "\ttrain loss: 0.398843484876696\n",
      "\ttrain loss: 0.4380397503055866\n",
      "\ttrain loss: 0.30842466515388545\n",
      "\ttrain loss: 0.4076539705738896\n",
      "\ttrain loss: 0.5557897819189995\n",
      "\ttrain loss: 0.3826920447111497\n",
      "\ttrain loss: 0.39817217676069017\n",
      "\ttrain loss: 0.49483412117017544\n",
      "\ttrain loss: 0.4296837667749499\n",
      "\ttrain loss: 0.3155633406551056\n",
      "\ttrain loss: 0.5881431872239933\n",
      "\ttrain loss: 0.3855243948515085\n",
      "\ttrain loss: 0.5834187151384775\n",
      "\ttrain loss: 0.2404748274264348\n",
      "\ttrain loss: 0.5110573613636158\n",
      "\ttrain loss: 0.43975397250336423\n",
      "\ttrain loss: 0.3321247928325095\n",
      "\ttrain loss: 0.5514649674831614\n",
      "\ttrain loss: 0.3538201954434964\n",
      "\ttrain loss: 0.5157973067454937\n",
      "\ttrain loss: 0.5611268610821624\n",
      "\ttrain loss: 0.29334887267829346\n",
      "\ttrain loss: 0.2645481256245503\n",
      "\ttrain loss: 0.3060279536055083\n",
      "\ttrain loss: 0.43977501505776917\n",
      "\ttrain loss: 0.5009690043850579\n",
      "\ttrain loss: 0.4277555218803775\n",
      "\ttrain loss: 0.42170873669101827\n",
      "\ttrain loss: 0.2909343025851596\n",
      "\ttrain loss: 0.3365371474721666\n",
      "\ttrain loss: 0.3369113568439427\n",
      "\ttrain loss: 0.3738731735906473\n",
      "\ttrain loss: 0.4120301616637013\n",
      "\ttrain loss: 0.45846921099967164\n",
      "\ttrain loss: 0.3816171262384099\n",
      "\ttrain loss: 0.4538445977660117\n",
      "\ttrain loss: 0.3165551176505418\n",
      "\ttrain loss: 0.3708998702420867\n",
      "\ttrain loss: 0.4836041682539144\n",
      "\ttrain loss: 0.3100358526825141\n",
      "\ttrain loss: 0.38655368381444016\n",
      "\ttrain loss: 0.3860788410303494\n",
      "\ttrain loss: 0.35193871130398763\n",
      "\ttrain loss: 0.4729915907486714\n",
      "\ttrain loss: 0.4067175072277024\n",
      "\ttrain loss: 0.4343965562346796\n",
      "\ttrain loss: 0.34630994513196767\n",
      "\ttrain loss: 0.48317111060167384\n",
      "\ttrain loss: 0.6496833989530835\n",
      "\ttrain loss: 0.3961234973608962\n",
      "\ttrain loss: 0.4451462530415171\n",
      "\ttrain loss: 0.3151100369314276\n",
      "\ttrain loss: 0.4763497896910923\n",
      "\ttrain loss: 0.241615617240738\n",
      "\ttrain loss: 0.42703857587760385\n",
      "\ttrain loss: 0.31024288226489327\n",
      "\ttrain loss: 0.40925201443987347\n",
      "\ttrain loss: 0.47552282165885557\n",
      "\ttrain loss: 0.43719714767316037\n",
      "\ttrain loss: 0.3969827575618351\n",
      "\ttrain loss: 0.3552690144151933\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(7/15) is saved!\n",
      "\ttrain loss: 0.3927324568665198\n",
      "\ttrain loss: 0.4341458012123456\n",
      "\ttrain loss: 0.38342327688761024\n",
      "\ttrain loss: 0.323615151564629\n",
      "\ttrain loss: 0.47791960987980936\n",
      "\ttrain loss: 0.26968798518597714\n",
      "\ttrain loss: 0.29919521970047475\n",
      "\ttrain loss: 0.48106450264252587\n",
      "\ttrain loss: 0.4675505632567155\n",
      "\ttrain loss: 0.5196122519224805\n",
      "\ttrain loss: 0.404170618112382\n",
      "\ttrain loss: 0.47161383562550585\n",
      "\ttrain loss: 0.38063148783164263\n",
      "\ttrain loss: 0.5605854174785225\n",
      "\ttrain loss: 0.36043030847928026\n",
      "\ttrain loss: 0.302594548713117\n",
      "\ttrain loss: 0.5719525745045061\n",
      "\ttrain loss: 0.37351179717164534\n",
      "\ttrain loss: 0.30412663082250324\n",
      "\ttrain loss: 0.483138783724259\n",
      "\ttrain loss: 0.45637525501349796\n",
      "\ttrain loss: 0.25992665345119365\n",
      "\ttrain loss: 0.327150740079475\n",
      "\ttrain loss: 0.7707601238394459\n",
      "\ttrain loss: 0.637094473049572\n",
      "\ttrain loss: 0.4560215677712823\n",
      "\ttrain loss: 0.49560741673613806\n",
      "\ttrain loss: 0.250348573300736\n",
      "\ttrain loss: 0.3783562602064766\n",
      "\ttrain loss: 0.5977157954646597\n",
      "\ttrain loss: 0.4148685001007739\n",
      "\ttrain loss: 0.38843145143603575\n",
      "\ttrain loss: 0.2983556208810403\n",
      "\ttrain loss: 0.46179864224684797\n",
      "\ttrain loss: 0.37117821159934666\n",
      "\ttrain loss: 0.5263938040429395\n",
      "\ttrain loss: 0.3799698045249113\n",
      "\ttrain loss: 0.4591580573975007\n",
      "\ttrain loss: 0.590266571402547\n",
      "\ttrain loss: 0.4292144985695017\n",
      "\ttrain loss: 0.3783476798262457\n",
      "\ttrain loss: 0.35314584054143644\n",
      "\ttrain loss: 0.3926666687212175\n",
      "\ttrain loss: 0.531177902251706\n",
      "\ttrain loss: 0.4709703897913369\n",
      "\ttrain loss: 0.5879737335582853\n",
      "\ttrain loss: 0.42227014951495423\n",
      "\ttrain loss: 0.2891329149873809\n",
      "\ttrain loss: 0.5545167601724927\n",
      "\ttrain loss: 0.5814699692353099\n",
      "\ttrain loss: 0.39970803350525647\n",
      "\ttrain loss: 0.6253630364082816\n",
      "\ttrain loss: 0.40124375807968254\n",
      "\ttrain loss: 0.5000629113036764\n",
      "\ttrain loss: 0.4474994846958944\n",
      "\ttrain loss: 0.316545764070865\n",
      "\ttrain loss: 0.4426485880210719\n",
      "\ttrain loss: 0.4639785256590623\n",
      "\ttrain loss: 0.5317647164144716\n",
      "\ttrain loss: 0.4114004181915309\n",
      "\ttrain loss: 0.30877991836524343\n",
      "\ttrain loss: 0.34972025577484356\n",
      "\ttrain loss: 0.33787692350937004\n",
      "\ttrain loss: 0.29934811054650035\n",
      "\ttrain loss: 0.4654802538244962\n",
      "\ttrain loss: 0.41864639584444957\n",
      "\ttrain loss: 0.4103958695500052\n",
      "\ttrain loss: 0.5643997109578465\n",
      "\ttrain loss: 0.22726285455047485\n",
      "\ttrain loss: 0.5374416844979096\n",
      "\ttrain loss: 0.3767348008826824\n",
      "\ttrain loss: 0.5466090254625429\n",
      "\ttrain loss: 0.3706769622694974\n",
      "\ttrain loss: 0.47985016779915707\n",
      "\ttrain loss: 0.6635995150769971\n",
      "\ttrain loss: 0.48404370209158887\n",
      "\ttrain loss: 0.4397830785746222\n",
      "\ttrain loss: 0.4426055179298549\n",
      "\ttrain loss: 0.5092743559903591\n",
      "\ttrain loss: 0.406518725991615\n",
      "\ttrain loss: 0.48978011591099324\n",
      "\ttrain loss: 0.6060103091275546\n",
      "\ttrain loss: 0.45706812640063055\n",
      "\ttrain loss: 0.4068257127775824\n",
      "\ttrain loss: 0.3789609219202206\n",
      "\ttrain loss: 0.30089453618075795\n",
      "\ttrain loss: 0.4943959054633638\n",
      "\ttrain loss: 0.4502114987425536\n",
      "\ttrain loss: 0.2849601289011433\n",
      "\ttrain loss: 0.4041250321992662\n",
      "\ttrain loss: 0.4969599527826654\n",
      "\ttrain loss: 0.33733016402426946\n",
      "\ttrain loss: 0.4138110280653494\n",
      "\ttrain loss: 0.2749941544477332\n",
      "\ttrain loss: 0.45251108736076273\n",
      "\ttrain loss: 0.616806816886234\n",
      "\ttrain loss: 0.2736045554262571\n",
      "\ttrain loss: 0.5500973986563544\n",
      "\ttrain loss: 0.40004734814255444\n",
      "\ttrain loss: 0.32904931724699993\n",
      "\ttrain loss: 0.415170208671353\n",
      "\ttrain loss: 0.31915549349317307\n",
      "\ttrain loss: 0.3630623990356092\n",
      "\ttrain loss: 0.48698767822529787\n",
      "\ttrain loss: 0.5601439392692077\n",
      "\ttrain loss: 0.5316597257174107\n",
      "\ttrain loss: 0.4493363713615477\n",
      "\ttrain loss: 0.4748326493553493\n",
      "\ttrain loss: 0.4374732627847697\n",
      "\ttrain loss: 0.39273174970762653\n",
      "\ttrain loss: 0.32081659092407433\n",
      "\ttrain loss: 0.5251974018679002\n",
      "\ttrain loss: 0.5530365005350665\n",
      "\ttrain loss: 0.3684961236768325\n",
      "\ttrain loss: 0.5652612931118333\n",
      "\ttrain loss: 0.3601716044912007\n",
      "\ttrain loss: 0.6104123509586289\n",
      "\ttrain loss: 0.25243036126690416\n",
      "\ttrain loss: 0.38100904913154976\n",
      "\ttrain loss: 0.40708293557638664\n",
      "\ttrain loss: 0.33980562838376654\n",
      "\ttrain loss: 0.3667969724165032\n",
      "\ttrain loss: 0.46395365755230067\n",
      "\ttrain loss: 0.38412843760792614\n",
      "\ttrain loss: 0.4023146617381794\n",
      "\ttrain loss: 0.3458397718423422\n",
      "\ttrain loss: 0.5287541283567054\n",
      "\ttrain loss: 0.6127076418162536\n",
      "\ttrain loss: 0.32378712925669506\n",
      "\ttrain loss: 0.30386956254728203\n",
      "\ttrain loss: 0.3332940651828753\n",
      "\ttrain loss: 0.4642429275709858\n",
      "\ttrain loss: 0.3488274380911148\n",
      "\ttrain loss: 0.7658756090667076\n",
      "\ttrain loss: 0.4713295633325461\n",
      "\ttrain loss: 0.5472487217713251\n",
      "\ttrain loss: 0.2926326205870753\n",
      "\ttrain loss: 0.4980303098609879\n",
      "\ttrain loss: 0.4139815610603811\n",
      "\ttrain loss: 0.6022029981942958\n",
      "\ttrain loss: 0.23978374010006645\n",
      "\ttrain loss: 0.4787519533157566\n",
      "\ttrain loss: 0.27636278542947157\n",
      "\ttrain loss: 0.39343910724999254\n",
      "\ttrain loss: 0.4074351784778718\n",
      "\ttrain loss: 0.43426823485982047\n",
      "\ttrain loss: 0.41233386023198093\n",
      "\ttrain loss: 0.36978912182930124\n",
      "\ttrain loss: 0.5975195561120913\n",
      "\ttrain loss: 0.4440689287688644\n",
      "\ttrain loss: 0.4687992006185589\n",
      "\ttrain loss: 0.4557844670672634\n",
      "\ttrain loss: 0.44199286730069964\n",
      "\ttrain loss: 0.2974555212810008\n",
      "\ttrain loss: 0.4120899250284955\n",
      "\ttrain loss: 0.47110613465410034\n",
      "\ttrain loss: 0.2976037278074154\n",
      "\ttrain loss: 0.3945555857185108\n",
      "\ttrain loss: 0.3114866079152709\n",
      "\ttrain loss: 0.5969705699531369\n",
      "\ttrain loss: 0.6025839392586787\n",
      "\ttrain loss: 0.3291862243102252\n",
      "\ttrain loss: 0.3867880770588945\n",
      "\ttrain loss: 0.47030992072437194\n",
      "\ttrain loss: 0.5219534546010749\n",
      "\ttrain loss: 0.40622136174245094\n",
      "\ttrain loss: 0.4314150853107356\n",
      "\ttrain loss: 0.3715036339142913\n",
      "\ttrain loss: 0.5932878253560845\n",
      "\ttrain loss: 0.6752193616193967\n",
      "\ttrain loss: 0.2952799456309718\n",
      "\ttrain loss: 0.4878254832430689\n",
      "\ttrain loss: 0.46217218068885\n",
      "\ttrain loss: 0.2896476661270453\n",
      "\ttrain loss: 0.6805212135567045\n",
      "\ttrain loss: 0.5027627828787662\n",
      "\ttrain loss: 0.27401443977101203\n",
      "\ttrain loss: 0.335562871014481\n",
      "\ttrain loss: 0.2572450321900397\n",
      "\ttrain loss: 0.3039102605871364\n",
      "\ttrain loss: 0.5936514984607687\n",
      "\ttrain loss: 0.40383877356293235\n",
      "\ttrain loss: 0.3342499585415636\n",
      "\ttrain loss: 0.3756220493525922\n",
      "\ttrain loss: 0.7258149101518941\n",
      "\ttrain loss: 0.34197832489946206\n",
      "\ttrain loss: 0.40894810152038175\n",
      "\ttrain loss: 0.2493306859780508\n",
      "\ttrain loss: 0.40757638376870003\n",
      "\ttrain loss: 0.5697177742012205\n",
      "\ttrain loss: 0.4230545348006082\n",
      "\ttrain loss: 0.34221906293772486\n",
      "\ttrain loss: 0.4657016966168211\n",
      "\ttrain loss: 0.36781797849639497\n",
      "\ttrain loss: 0.4933909486134047\n",
      "\ttrain loss: 0.2936621727755419\n",
      "\ttrain loss: 0.3822927494951783\n",
      "\ttrain loss: 0.4541990235703295\n",
      "\ttrain loss: 0.49234780285886076\n",
      "\ttrain loss: 0.3912894790188154\n",
      "\ttrain loss: 0.48834724125192536\n",
      "\ttrain loss: 0.49622810860619604\n",
      "\ttrain loss: 0.5463837406466748\n",
      "\ttrain loss: 0.6344897174482342\n",
      "\ttrain loss: 0.4980488900895853\n",
      "\ttrain loss: 0.5315595345678321\n",
      "\ttrain loss: 0.4056398456416141\n",
      "\ttrain loss: 0.5377389676066711\n",
      "\ttrain loss: 0.6034246345456991\n",
      "\ttrain loss: 0.3759528722825177\n",
      "\ttrain loss: 0.34638382808760115\n",
      "\ttrain loss: 0.26401831006804966\n",
      "\ttrain loss: 0.26740843279566573\n",
      "\ttrain loss: 0.4031270289332216\n",
      "\ttrain loss: 0.503811893526174\n",
      "\ttrain loss: 0.4524819194818038\n",
      "\ttrain loss: 0.35076646440187376\n",
      "\ttrain loss: 0.58459442363978\n",
      "\ttrain loss: 0.7125239890691895\n",
      "\ttrain loss: 0.41976211876015473\n",
      "\ttrain loss: 0.33417908504235466\n",
      "\ttrain loss: 0.5374700779498928\n",
      "\ttrain loss: 0.5064492941025287\n",
      "\ttrain loss: 0.43712263159078724\n",
      "\ttrain loss: 0.5479773230293138\n",
      "\ttrain loss: 0.527135547702524\n",
      "\ttrain loss: 0.36260730124099916\n",
      "\ttrain loss: 0.5803164315864395\n",
      "\ttrain loss: 0.35504382113301464\n",
      "\ttrain loss: 0.3146081628490113\n",
      "\ttrain loss: 0.4240217219364426\n",
      "\ttrain loss: 0.3597109366801067\n",
      "\ttrain loss: 0.40971855861536\n",
      "\ttrain loss: 0.30662667054616666\n",
      "\ttrain loss: 0.3332108510715055\n",
      "\ttrain loss: 0.46701923100835396\n",
      "\ttrain loss: 0.3453842404905334\n",
      "\ttrain loss: 0.49563075526134187\n",
      "\ttrain loss: 0.4548121931023528\n",
      "\ttrain loss: 0.41408991205242063\n",
      "\ttrain loss: 0.45198708819914424\n",
      "\ttrain loss: 0.412936850266316\n",
      "\ttrain loss: 0.4623863383735222\n",
      "\ttrain loss: 0.37489168012853913\n",
      "\ttrain loss: 0.373129045503955\n",
      "\ttrain loss: 0.39876891840957174\n",
      "\ttrain loss: 0.41628645901629546\n",
      "\ttrain loss: 0.323148322557468\n",
      "\ttrain loss: 0.44397852933582815\n",
      "\ttrain loss: 0.2902972518563762\n",
      "\ttrain loss: 0.5253152965318069\n",
      "\ttrain loss: 0.5807848712371113\n",
      "\ttrain loss: 0.4185612394015452\n",
      "\ttrain loss: 0.21984439241644865\n",
      "\ttrain loss: 0.5435551391115483\n",
      "\ttrain loss: 0.43181687248280437\n",
      "\ttrain loss: 0.44769526861486375\n",
      "\ttrain loss: 0.23469251661453971\n",
      "\ttrain loss: 0.2611317107039043\n",
      "\ttrain loss: 0.39664446494354955\n",
      "\ttrain loss: 0.3577147730493131\n",
      "\ttrain loss: 0.4448889602701752\n",
      "\ttrain loss: 0.31472308338205374\n",
      "\ttrain loss: 0.32066722877269843\n",
      "\ttrain loss: 0.5339645502413538\n",
      "\ttrain loss: 0.42567656913348506\n",
      "\ttrain loss: 0.6341386982748497\n",
      "\ttrain loss: 0.6548712226710887\n",
      "\ttrain loss: 0.41331261481306103\n",
      "\ttrain loss: 0.44576110931898605\n",
      "\ttrain loss: 0.5096117214493754\n",
      "\ttrain loss: 0.27907235004483877\n",
      "\ttrain loss: 0.48089360302648543\n",
      "\ttrain loss: 0.4442172709465155\n",
      "\ttrain loss: 0.531738814028534\n",
      "\ttrain loss: 0.4199809012336645\n",
      "\ttrain loss: 0.42207342631198186\n",
      "\ttrain loss: 0.31585671709664065\n",
      "\ttrain loss: 0.4143947420642312\n",
      "\ttrain loss: 0.4734183661885071\n",
      "\ttrain loss: 0.3699495068739587\n",
      "\ttrain loss: 0.4704145186421991\n",
      "\ttrain loss: 0.4581674961519243\n",
      "\ttrain loss: 0.2894081122016293\n",
      "\ttrain loss: 0.44463726992303004\n",
      "\ttrain loss: 0.37541475787015993\n",
      "\ttrain loss: 0.5339482787120663\n",
      "\ttrain loss: 0.3373892486944342\n",
      "\ttrain loss: 0.36386359705492605\n",
      "\ttrain loss: 0.3490850585498825\n",
      "\ttrain loss: 0.32772557852831474\n",
      "\ttrain loss: 0.4215614525188741\n",
      "\ttrain loss: 0.5075208156543167\n",
      "\ttrain loss: 0.4485587757249472\n",
      "\ttrain loss: 0.43637673892953166\n",
      "\ttrain loss: 0.4614508556991028\n",
      "\ttrain loss: 0.4457055916515446\n",
      "\ttrain loss: 0.4194925055289\n",
      "\ttrain loss: 0.6461361548970348\n",
      "\ttrain loss: 0.5493332860547238\n",
      "\ttrain loss: 0.2594832282854419\n",
      "\ttrain loss: 0.22503817503804252\n",
      "\ttrain loss: 0.3658483161996131\n",
      "\ttrain loss: 0.5134925940526164\n",
      "\ttrain loss: 0.5006739646865643\n",
      "\ttrain loss: 0.34961040390036274\n",
      "\ttrain loss: 0.3932134240163554\n",
      "\ttrain loss: 0.29586961040042387\n",
      "\ttrain loss: 0.3663481862554793\n",
      "\ttrain loss: 0.427233480684486\n",
      "\ttrain loss: 0.30566072665999\n",
      "\ttrain loss: 0.296691124350717\n",
      "\ttrain loss: 0.4142595109686348\n",
      "\ttrain loss: 0.42559428649306286\n",
      "\ttrain loss: 0.4241379897233548\n",
      "\ttrain loss: 0.41376564749599004\n",
      "\ttrain loss: 0.5497950888931484\n",
      "\ttrain loss: 0.3557920767757488\n",
      "\ttrain loss: 0.3565559667790674\n",
      "\ttrain loss: 0.32549952801307397\n",
      "\ttrain loss: 0.4263704747027688\n",
      "\ttrain loss: 0.6147944421156699\n",
      "\ttrain loss: 0.3226295010825986\n",
      "\ttrain loss: 0.3878013252660515\n",
      "\ttrain loss: 0.36541226976849484\n",
      "\ttrain loss: 0.35345476993760355\n",
      "\ttrain loss: 0.30299201682462523\n",
      "\ttrain loss: 0.46742098273052524\n",
      "\ttrain loss: 0.3985325677274646\n",
      "\ttrain loss: 0.5848623367320722\n",
      "\ttrain loss: 0.45649504202513913\n",
      "\ttrain loss: 0.5038668750120446\n",
      "\ttrain loss: 0.4703614168446933\n",
      "\ttrain loss: 0.40066928567224\n",
      "\ttrain loss: 0.32431897564384016\n",
      "\ttrain loss: 0.47831915362768285\n",
      "\ttrain loss: 0.3871199184018814\n",
      "\ttrain loss: 0.41155842205983295\n",
      "\ttrain loss: 0.39655458725840187\n",
      "\ttrain loss: 0.4582574726314709\n",
      "\ttrain loss: 0.5872443174861831\n",
      "\ttrain loss: 0.5009518961241491\n",
      "\ttrain loss: 0.4174608392376682\n",
      "\ttrain loss: 0.6469844418451233\n",
      "\ttrain loss: 0.405912261788438\n",
      "\ttrain loss: 0.4403043072997985\n",
      "\ttrain loss: 0.4565953171740211\n",
      "\ttrain loss: 0.44418731490386476\n",
      "\ttrain loss: 0.45389964149146755\n",
      "\ttrain loss: 0.38866417874331427\n",
      "\ttrain loss: 0.5205383673955027\n",
      "\ttrain loss: 0.32525741742334957\n",
      "\ttrain loss: 0.39047173366084637\n",
      "\ttrain loss: 0.40065952371885516\n",
      "\ttrain loss: 0.33401810007522553\n",
      "\ttrain loss: 0.759270242325173\n",
      "\ttrain loss: 0.3244058300400769\n",
      "\ttrain loss: 0.42149130818420955\n",
      "\ttrain loss: 0.47750042748449406\n",
      "\ttrain loss: 0.5034755587519331\n",
      "\ttrain loss: 0.5610975138445424\n",
      "\ttrain loss: 0.4278574208961248\n",
      "\ttrain loss: 0.37714093084969524\n",
      "\ttrain loss: 0.4177669342773504\n",
      "\ttrain loss: 0.3858146028818583\n",
      "\ttrain loss: 0.5198790304677718\n",
      "\ttrain loss: 0.47617293809860484\n",
      "\ttrain loss: 0.4145214347203582\n",
      "\ttrain loss: 0.4044379988658531\n",
      "\ttrain loss: 0.3520827612840456\n",
      "\ttrain loss: 0.483090653837292\n",
      "\ttrain loss: 0.40529846291509325\n",
      "\ttrain loss: 0.5475781661772345\n",
      "\ttrain loss: 0.5240313288357394\n",
      "\ttrain loss: 0.49921429615961277\n",
      "\ttrain loss: 0.4187811183695608\n",
      "\ttrain loss: 0.4919831122955969\n",
      "\ttrain loss: 0.4129378033491987\n",
      "\ttrain loss: 0.3290363775972641\n",
      "\ttrain loss: 0.35181743786118136\n",
      "\ttrain loss: 0.2826346044973113\n",
      "\ttrain loss: 0.32442205363558607\n",
      "\ttrain loss: 0.38753399564581814\n",
      "\ttrain loss: 0.294287399260259\n",
      "\ttrain loss: 0.49269320638820757\n",
      "\ttrain loss: 0.3508804613108579\n",
      "\ttrain loss: 0.3283803143679723\n",
      "\ttrain loss: 0.4145969873544462\n",
      "\ttrain loss: 0.4444131765546452\n",
      "\ttrain loss: 0.3859292533240808\n",
      "\ttrain loss: 0.5336060819276511\n",
      "\ttrain loss: 0.39331465373943575\n",
      "\ttrain loss: 0.556839097998675\n",
      "\ttrain loss: 0.5988397251438277\n",
      "\ttrain loss: 0.4445627784009134\n",
      "\ttrain loss: 0.37775692005077954\n",
      "\ttrain loss: 0.44370317706690654\n",
      "\ttrain loss: 0.23493320633429093\n",
      "\ttrain loss: 0.29575491249536157\n",
      "\ttrain loss: 0.5014660424785372\n",
      "\ttrain loss: 0.485304145904518\n",
      "\ttrain loss: 0.23956759373041744\n",
      "\ttrain loss: 0.3764009343098733\n",
      "\ttrain loss: 0.4313071353060386\n",
      "\ttrain loss: 0.3203263369024936\n",
      "\ttrain loss: 0.6950441132941082\n",
      "\ttrain loss: 0.26672640943088766\n",
      "\ttrain loss: 0.5177802141850805\n",
      "\ttrain loss: 0.34838236826054986\n",
      "\ttrain loss: 0.4989205463137108\n",
      "\ttrain loss: 0.5687854962024456\n",
      "\ttrain loss: 0.4339307464288873\n",
      "\ttrain loss: 0.6255046140495001\n",
      "\ttrain loss: 0.3825775096489418\n",
      "\ttrain loss: 0.4028508431983494\n",
      "\ttrain loss: 0.4292879551744345\n",
      "\ttrain loss: 0.4187396794956367\n",
      "\ttrain loss: 0.3424915131304152\n",
      "\ttrain loss: 0.5119112422594241\n",
      "\ttrain loss: 0.43720938966398304\n",
      "\ttrain loss: 0.26940426615007096\n",
      "\ttrain loss: 0.4825969207789734\n",
      "\ttrain loss: 0.6017366261665569\n",
      "\ttrain loss: 0.4377480939532006\n",
      "\ttrain loss: 0.3805457670448723\n",
      "\ttrain loss: 0.39633325593214497\n",
      "\ttrain loss: 0.3079365675099718\n",
      "\ttrain loss: 0.2548989904191371\n",
      "\ttrain loss: 0.3954811191132398\n",
      "\ttrain loss: 0.28937197164423334\n",
      "\ttrain loss: 0.5170903927672486\n",
      "\ttrain loss: 0.47387672472463005\n",
      "\ttrain loss: 0.3853400584994304\n",
      "\ttrain loss: 0.38849597177742473\n",
      "\ttrain loss: 0.3239910218380601\n",
      "\ttrain loss: 0.4077254512822456\n",
      "\ttrain loss: 0.5631752035544313\n",
      "\ttrain loss: 0.5275314474818489\n",
      "\ttrain loss: 0.4932464385475653\n",
      "\ttrain loss: 0.49280363134290966\n",
      "\ttrain loss: 0.3982531460599745\n",
      "\ttrain loss: 0.26837632304545017\n",
      "\ttrain loss: 0.5978615558963574\n",
      "\ttrain loss: 0.34348078804357657\n",
      "\ttrain loss: 0.4143556871863046\n",
      "\ttrain loss: 0.26843072753777253\n",
      "\ttrain loss: 0.5051523034445423\n",
      "\ttrain loss: 0.28475124825187925\n",
      "\ttrain loss: 0.505077420634085\n",
      "\ttrain loss: 0.503591059214146\n",
      "\ttrain loss: 0.5688336892982482\n",
      "\ttrain loss: 0.3710387272115502\n",
      "\ttrain loss: 0.23482225822598823\n",
      "\ttrain loss: 0.3934926256494388\n",
      "\ttrain loss: 0.5155269396726975\n",
      "\ttrain loss: 0.433540757153701\n",
      "\ttrain loss: 0.4446635087073746\n",
      "\ttrain loss: 0.40188994465340155\n",
      "\ttrain loss: 0.35740876483494055\n",
      "\ttrain loss: 0.42702642020202025\n",
      "\ttrain loss: 0.434894845005134\n",
      "\ttrain loss: 0.4569917887406518\n",
      "\ttrain loss: 0.46900966583104853\n",
      "\ttrain loss: 0.507422190467923\n",
      "\ttrain loss: 0.5170164821822669\n",
      "\ttrain loss: 0.3637082364769781\n",
      "\ttrain loss: 0.5737947122987822\n",
      "\ttrain loss: 0.3331111709399221\n",
      "\ttrain loss: 0.32688630687301934\n",
      "\ttrain loss: 0.362591897318003\n",
      "\ttrain loss: 0.5003340897291402\n",
      "\ttrain loss: 0.33293339231661373\n",
      "\ttrain loss: 0.44379892002224786\n",
      "\ttrain loss: 0.3693669699401464\n",
      "\ttrain loss: 0.27433720346224766\n",
      "\ttrain loss: 0.36270421212472004\n",
      "\ttrain loss: 0.41015535172764556\n",
      "\ttrain loss: 0.4122199144490456\n",
      "\ttrain loss: 0.5006605565094916\n",
      "\ttrain loss: 0.6577729751310439\n",
      "\ttrain loss: 0.31546582208380286\n",
      "\ttrain loss: 0.3664445323653756\n",
      "\ttrain loss: 0.434932251704155\n",
      "\ttrain loss: 0.3963472575190947\n",
      "\ttrain loss: 0.3396889398780172\n",
      "\ttrain loss: 0.33099608970415667\n",
      "\ttrain loss: 0.3480192391536566\n",
      "\ttrain loss: 0.34238281786012065\n",
      "\ttrain loss: 0.37815991252210324\n",
      "\ttrain loss: 0.4238617695088065\n",
      "\ttrain loss: 0.27017805298108205\n",
      "\ttrain loss: 0.33383854571444355\n",
      "\ttrain loss: 0.4346808633632241\n",
      "\ttrain loss: 0.3058370837449026\n",
      "\ttrain loss: 0.4129240552483237\n",
      "\ttrain loss: 0.34833643196623265\n",
      "\ttrain loss: 0.3749186423492125\n",
      "\ttrain loss: 0.40509509297366\n",
      "\ttrain loss: 0.40867395598136785\n",
      "\ttrain loss: 0.43941120303922393\n",
      "\ttrain loss: 0.4981631254676996\n",
      "\ttrain loss: 0.4140948486841681\n",
      "\ttrain loss: 0.3715968739257814\n",
      "\ttrain loss: 0.3435198465810262\n",
      "\ttrain loss: 0.3245977288806493\n",
      "\ttrain loss: 0.4457415559851068\n",
      "\ttrain loss: 0.5496049514670371\n",
      "\ttrain loss: 0.6547199663996774\n",
      "\ttrain loss: 0.28439232712689094\n",
      "\ttrain loss: 0.5747193601821933\n",
      "\ttrain loss: 0.4327580270476309\n",
      "\ttrain loss: 0.2928481681578365\n",
      "\ttrain loss: 0.4402675771386385\n",
      "\ttrain loss: 0.3490425643330576\n",
      "\ttrain loss: 0.5163728430316653\n",
      "\ttrain loss: 0.5572099007700657\n",
      "\ttrain loss: 0.3353856611573721\n",
      "\ttrain loss: 0.48715045174399674\n",
      "\ttrain loss: 0.3651773877121939\n",
      "\ttrain loss: 0.41581558224635606\n",
      "\ttrain loss: 0.40934991137039295\n",
      "\ttrain loss: 0.381273789548611\n",
      "\ttrain loss: 0.5843538742191989\n",
      "\ttrain loss: 0.447858495165677\n",
      "\ttrain loss: 0.520728231998864\n",
      "\ttrain loss: 0.5779963139565834\n",
      "\ttrain loss: 0.6049188888853696\n",
      "\ttrain loss: 0.41030757127453366\n",
      "\ttrain loss: 0.4200898775381651\n",
      "\ttrain loss: 0.3025389013817684\n",
      "\ttrain loss: 0.5583948455465648\n",
      "\ttrain loss: 0.5047291305381012\n",
      "\ttrain loss: 0.49072321663742935\n",
      "\ttrain loss: 0.42935138445359533\n",
      "\ttrain loss: 0.29207507040733294\n",
      "\ttrain loss: 0.36228948793741467\n",
      "\ttrain loss: 0.4822361189760802\n",
      "\ttrain loss: 0.4024571983029579\n",
      "\ttrain loss: 0.403037740553042\n",
      "\ttrain loss: 0.6021909053525447\n",
      "\ttrain loss: 0.5969649834246377\n",
      "\ttrain loss: 0.3244365966417931\n",
      "\ttrain loss: 0.5589049629936932\n",
      "\ttrain loss: 0.5751119277834972\n",
      "\ttrain loss: 0.23751413656936027\n",
      "\ttrain loss: 0.6108986061195276\n",
      "\ttrain loss: 0.546675410323213\n",
      "\ttrain loss: 0.42416092726784743\n",
      "\ttrain loss: 0.34783415112602123\n",
      "\ttrain loss: 0.3434081633491225\n",
      "\ttrain loss: 0.45690384738938367\n",
      "\ttrain loss: 0.26472195637988116\n",
      "\ttrain loss: 0.5811871425214865\n",
      "\ttrain loss: 0.5863035174950424\n",
      "\ttrain loss: 0.17599401613433796\n",
      "\ttrain loss: 0.4473478893211954\n",
      "\ttrain loss: 0.5382305013167464\n",
      "\ttrain loss: 0.3068323590394485\n",
      "\ttrain loss: 0.30247939230699494\n",
      "\ttrain loss: 0.3926893611087978\n",
      "\ttrain loss: 0.27443417292373073\n",
      "\ttrain loss: 0.3792552243702863\n",
      "\ttrain loss: 0.5430599831103372\n",
      "\ttrain loss: 0.39742242505166536\n",
      "\ttrain loss: 0.47584475496597817\n",
      "\ttrain loss: 0.27862118503159794\n",
      "\ttrain loss: 0.27361491786200476\n",
      "\ttrain loss: 0.25922388203469504\n",
      "\ttrain loss: 0.3468969543097102\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(8/15) is saved!\n",
      "\ttrain loss: 0.33619301843745075\n",
      "\ttrain loss: 0.36522269021462034\n",
      "\ttrain loss: 0.319511845503823\n",
      "\ttrain loss: 0.3654897983475299\n",
      "\ttrain loss: 0.3878228732851051\n",
      "\ttrain loss: 0.35921976847380066\n",
      "\ttrain loss: 0.39269874000832333\n",
      "\ttrain loss: 0.5165662306049396\n",
      "\ttrain loss: 0.3135015902864826\n",
      "\ttrain loss: 0.3638686647542659\n",
      "\ttrain loss: 0.28814458369847107\n",
      "\ttrain loss: 0.3061416211812077\n",
      "\ttrain loss: 0.470310426448447\n",
      "\ttrain loss: 0.35226668723630844\n",
      "\ttrain loss: 0.5588475311689877\n",
      "\ttrain loss: 0.45295589557050636\n",
      "\ttrain loss: 0.3979957508962063\n",
      "\ttrain loss: 0.30155057859043016\n",
      "\ttrain loss: 0.274017298605725\n",
      "\ttrain loss: 0.3555093679111415\n",
      "\ttrain loss: 0.37952824624897086\n",
      "\ttrain loss: 0.3209405258230423\n",
      "\ttrain loss: 0.4402032538697934\n",
      "\ttrain loss: 0.27529966802175687\n",
      "\ttrain loss: 0.28003672617995534\n",
      "\ttrain loss: 0.47981037638683877\n",
      "\ttrain loss: 0.5166328850560266\n",
      "\ttrain loss: 0.4006248922196495\n",
      "\ttrain loss: 0.6326198878078826\n",
      "\ttrain loss: 0.6142812923045013\n",
      "\ttrain loss: 0.4006924063771275\n",
      "\ttrain loss: 0.6259506798880323\n",
      "\ttrain loss: 0.3185960604158685\n",
      "\ttrain loss: 0.41069977613464126\n",
      "\ttrain loss: 0.4599163192043705\n",
      "\ttrain loss: 0.6452102275885796\n",
      "\ttrain loss: 0.2861302178470246\n",
      "\ttrain loss: 0.35803991697417337\n",
      "\ttrain loss: 0.456424873520326\n",
      "\ttrain loss: 0.4023316498678769\n",
      "\ttrain loss: 0.428172139979183\n",
      "\ttrain loss: 0.47117960944907267\n",
      "\ttrain loss: 0.3832213543736526\n",
      "\ttrain loss: 0.43979974891965545\n",
      "\ttrain loss: 0.3938512834526572\n",
      "\ttrain loss: 0.313289773974004\n",
      "\ttrain loss: 0.6022478488564609\n",
      "\ttrain loss: 0.43795096407189227\n",
      "\ttrain loss: 0.4270749001643177\n",
      "\ttrain loss: 0.24508709861679612\n",
      "\ttrain loss: 0.4530540881484122\n",
      "\ttrain loss: 0.3629615718886656\n",
      "\ttrain loss: 0.2785280257635427\n",
      "\ttrain loss: 0.4886385046454661\n",
      "\ttrain loss: 0.28648520815777256\n",
      "\ttrain loss: 0.42065835302629184\n",
      "\ttrain loss: 0.35388161059145257\n",
      "\ttrain loss: 0.42017412338762494\n",
      "\ttrain loss: 0.3925642580196955\n",
      "\ttrain loss: 0.44041173208239814\n",
      "\ttrain loss: 0.40487233888486746\n",
      "\ttrain loss: 0.4832446782718969\n",
      "\ttrain loss: 0.27568707008220783\n",
      "\ttrain loss: 0.34294230521198255\n",
      "\ttrain loss: 0.5159394577570466\n",
      "\ttrain loss: 0.39426911534615633\n",
      "\ttrain loss: 0.4440880711684587\n",
      "\ttrain loss: 0.4042428862516627\n",
      "\ttrain loss: 0.4293878658054112\n",
      "\ttrain loss: 0.47371916990969304\n",
      "\ttrain loss: 0.3943511484811856\n",
      "\ttrain loss: 0.337025369479541\n",
      "\ttrain loss: 0.2725372364232088\n",
      "\ttrain loss: 0.5686243175259914\n",
      "\ttrain loss: 0.4465314836270248\n",
      "\ttrain loss: 0.39916751559753705\n",
      "\ttrain loss: 0.41087467112233067\n",
      "\ttrain loss: 0.4703150432526083\n",
      "\ttrain loss: 0.3146988770763894\n",
      "\ttrain loss: 0.401700321685973\n",
      "\ttrain loss: 0.3814561885948692\n",
      "\ttrain loss: 0.49291553322259785\n",
      "\ttrain loss: 0.36073790768640807\n",
      "\ttrain loss: 0.449381912873795\n",
      "\ttrain loss: 0.304437736021892\n",
      "\ttrain loss: 0.30578292200252033\n",
      "\ttrain loss: 0.26499707709107734\n",
      "\ttrain loss: 0.3714784851085562\n",
      "\ttrain loss: 0.4955340200485653\n",
      "\ttrain loss: 0.5066050430574479\n",
      "\ttrain loss: 0.36229251092978965\n",
      "\ttrain loss: 0.4492088077815734\n",
      "\ttrain loss: 0.36650732879467585\n",
      "\ttrain loss: 0.2732811146797516\n",
      "\ttrain loss: 0.44015814573228806\n",
      "\ttrain loss: 0.32873994881072793\n",
      "\ttrain loss: 0.30248851718104014\n",
      "\ttrain loss: 0.40474727625570167\n",
      "\ttrain loss: 0.49397966585493236\n",
      "\ttrain loss: 0.3672299054476263\n",
      "\ttrain loss: 0.5065310038039181\n",
      "\ttrain loss: 0.6977463912592758\n",
      "\ttrain loss: 0.4914287071095147\n",
      "\ttrain loss: 0.45543732272738363\n",
      "\ttrain loss: 0.23715898665758792\n",
      "\ttrain loss: 0.3458772178036198\n",
      "\ttrain loss: 0.4264174535576981\n",
      "\ttrain loss: 0.5951038221712038\n",
      "\ttrain loss: 0.284664569127706\n",
      "\ttrain loss: 0.7269878025701497\n",
      "\ttrain loss: 0.2693839928182028\n",
      "\ttrain loss: 0.43024365355627114\n",
      "\ttrain loss: 0.3666198142132713\n",
      "\ttrain loss: 0.46055580748184355\n",
      "\ttrain loss: 0.4548395673317252\n",
      "\ttrain loss: 0.17506915235396447\n",
      "\ttrain loss: 0.24531165392439697\n",
      "\ttrain loss: 0.31191219138956383\n",
      "\ttrain loss: 0.45025972562591865\n",
      "\ttrain loss: 0.4183959779756158\n",
      "\ttrain loss: 0.3236441146677784\n",
      "\ttrain loss: 0.6013439832466893\n",
      "\ttrain loss: 0.29504823613502795\n",
      "\ttrain loss: 0.3017475028475555\n",
      "\ttrain loss: 0.4875507852742671\n",
      "\ttrain loss: 0.4311821137917889\n",
      "\ttrain loss: 0.47374778588591004\n",
      "\ttrain loss: 0.2321000244814543\n",
      "\ttrain loss: 0.4233866682175706\n",
      "\ttrain loss: 0.35133389363480794\n",
      "\ttrain loss: 0.37588162572625794\n",
      "\ttrain loss: 0.46351410357583434\n",
      "\ttrain loss: 0.3502129034035708\n",
      "\ttrain loss: 0.5399547790833474\n",
      "\ttrain loss: 0.3982542466512562\n",
      "\ttrain loss: 0.5813551765396177\n",
      "\ttrain loss: 0.47157539080824606\n",
      "\ttrain loss: 0.3992603538096352\n",
      "\ttrain loss: 0.554974270884022\n",
      "\ttrain loss: 0.46691378182405985\n",
      "\ttrain loss: 0.35149646027814285\n",
      "\ttrain loss: 0.3319446486261872\n",
      "\ttrain loss: 0.3975121433958179\n",
      "\ttrain loss: 0.42206130426940425\n",
      "\ttrain loss: 0.3281503263998413\n",
      "\ttrain loss: 0.3365371457377047\n",
      "\ttrain loss: 0.3360371280595715\n",
      "\ttrain loss: 0.3865901252079389\n",
      "\ttrain loss: 0.39829354928506\n",
      "\ttrain loss: 0.2684466720156483\n",
      "\ttrain loss: 0.41547553619809297\n",
      "\ttrain loss: 0.3836928133410772\n",
      "\ttrain loss: 0.40856901287291236\n",
      "\ttrain loss: 0.41205820216995526\n",
      "\ttrain loss: 0.3635205045835017\n",
      "\ttrain loss: 0.4079661886166132\n",
      "\ttrain loss: 0.44515716550453477\n",
      "\ttrain loss: 0.4415980881233965\n",
      "\ttrain loss: 0.431248381147322\n",
      "\ttrain loss: 0.3919850002700477\n",
      "\ttrain loss: 0.28487187839227746\n",
      "\ttrain loss: 0.7838098271113296\n",
      "\ttrain loss: 0.3226542180121106\n",
      "\ttrain loss: 0.4984730877455964\n",
      "\ttrain loss: 0.31805998230248966\n",
      "\ttrain loss: 0.3051525934101602\n",
      "\ttrain loss: 0.41077668109804805\n",
      "\ttrain loss: 0.5525393884953166\n",
      "\ttrain loss: 0.2616830644507869\n",
      "\ttrain loss: 0.47837225549540063\n",
      "\ttrain loss: 0.4083461354420168\n",
      "\ttrain loss: 0.3917699073363323\n",
      "\ttrain loss: 0.36914473820743465\n",
      "\ttrain loss: 0.3826939852653214\n",
      "\ttrain loss: 0.506690195131277\n",
      "\ttrain loss: 0.508037827634346\n",
      "\ttrain loss: 0.26336730736399566\n",
      "\ttrain loss: 0.556874953788393\n",
      "\ttrain loss: 0.3980048576453962\n",
      "\ttrain loss: 0.42997948187646773\n",
      "\ttrain loss: 0.33592357434708264\n",
      "\ttrain loss: 0.3936571280973322\n",
      "\ttrain loss: 0.4008509227747604\n",
      "\ttrain loss: 0.2694486633199714\n",
      "\ttrain loss: 0.5309537376223454\n",
      "\ttrain loss: 0.5276738445696516\n",
      "\ttrain loss: 0.4659473832442881\n",
      "\ttrain loss: 0.3797231358645192\n",
      "\ttrain loss: 0.37820363284968983\n",
      "\ttrain loss: 0.43046192531122646\n",
      "\ttrain loss: 0.5211037217897964\n",
      "\ttrain loss: 0.37682503033936865\n",
      "\ttrain loss: 0.29533591263245573\n",
      "\ttrain loss: 0.4518592565606551\n",
      "\ttrain loss: 0.6599715853570265\n",
      "\ttrain loss: 0.5309095738281151\n",
      "\ttrain loss: 0.46828049553606443\n",
      "\ttrain loss: 0.29402409965729287\n",
      "\ttrain loss: 0.7230071587361653\n",
      "\ttrain loss: 0.4531797600970305\n",
      "\ttrain loss: 0.5022857423263968\n",
      "\ttrain loss: 0.37342536273798116\n",
      "\ttrain loss: 0.3384278625091499\n",
      "\ttrain loss: 0.29798603701278525\n",
      "\ttrain loss: 0.45502184175312654\n",
      "\ttrain loss: 0.3963003233181809\n",
      "\ttrain loss: 0.31504463270318356\n",
      "\ttrain loss: 0.2687577627775544\n",
      "\ttrain loss: 0.4059897925871205\n",
      "\ttrain loss: 0.4097855073935607\n",
      "\ttrain loss: 0.34205227686015693\n",
      "\ttrain loss: 0.50188509904184\n",
      "\ttrain loss: 0.6307842641212935\n",
      "\ttrain loss: 0.49221199163266294\n",
      "\ttrain loss: 0.28360848564842295\n",
      "\ttrain loss: 0.6060008591048123\n",
      "\ttrain loss: 0.29794947177580067\n",
      "\ttrain loss: 0.29914271480698135\n",
      "\ttrain loss: 0.399148558720106\n",
      "\ttrain loss: 0.34958143285719745\n",
      "\ttrain loss: 0.2931787266438372\n",
      "\ttrain loss: 0.43455647672706427\n",
      "\ttrain loss: 0.3028669867882419\n",
      "\ttrain loss: 0.35571936416645045\n",
      "\ttrain loss: 0.42655907698948703\n",
      "\ttrain loss: 0.3383090078974895\n",
      "\ttrain loss: 0.27759640588067747\n",
      "\ttrain loss: 0.5204444094616686\n",
      "\ttrain loss: 0.31721749786684666\n",
      "\ttrain loss: 0.2776718176161412\n",
      "\ttrain loss: 0.4502234196718935\n",
      "\ttrain loss: 0.30306780169447173\n",
      "\ttrain loss: 0.37915691769159077\n",
      "\ttrain loss: 0.441944639984276\n",
      "\ttrain loss: 0.4836207529368425\n",
      "\ttrain loss: 0.37225634767687116\n",
      "\ttrain loss: 0.5977848472211794\n",
      "\ttrain loss: 0.33057359271604597\n",
      "\ttrain loss: 0.37652291571141383\n",
      "\ttrain loss: 0.31756681902527906\n",
      "\ttrain loss: 0.569763840918886\n",
      "\ttrain loss: 0.4288842346138333\n",
      "\ttrain loss: 0.38941242065036497\n",
      "\ttrain loss: 0.690763053676527\n",
      "\ttrain loss: 0.3699031067459389\n",
      "\ttrain loss: 0.4040019960595873\n",
      "\ttrain loss: 0.4353064768332928\n",
      "\ttrain loss: 0.4381727602495962\n",
      "\ttrain loss: 0.3839713559002074\n",
      "\ttrain loss: 0.36029784400106113\n",
      "\ttrain loss: 0.4682428802015228\n",
      "\ttrain loss: 0.4128394898945632\n",
      "\ttrain loss: 0.42606236521496554\n",
      "\ttrain loss: 0.45494894899832744\n",
      "\ttrain loss: 0.5271849600586311\n",
      "\ttrain loss: 0.4135910559826258\n",
      "\ttrain loss: 0.3959056393483561\n",
      "\ttrain loss: 0.35597688467617317\n",
      "\ttrain loss: 0.39431070204660124\n",
      "\ttrain loss: 0.3839962699479411\n",
      "\ttrain loss: 0.6023498893154589\n",
      "\ttrain loss: 0.4123182699595203\n",
      "\ttrain loss: 0.4063808674223442\n",
      "\ttrain loss: 0.3580304030517929\n",
      "\ttrain loss: 0.477121865398215\n",
      "\ttrain loss: 0.549205475767156\n",
      "\ttrain loss: 0.3284548623084588\n",
      "\ttrain loss: 0.3814841806589341\n",
      "\ttrain loss: 0.4781197747520796\n",
      "\ttrain loss: 0.4105143363976292\n",
      "\ttrain loss: 0.5670588669640171\n",
      "\ttrain loss: 0.34665870988516506\n",
      "\ttrain loss: 0.38055799237055676\n",
      "\ttrain loss: 0.3585184525130435\n",
      "\ttrain loss: 0.44748056482061116\n",
      "\ttrain loss: 0.35377394253384875\n",
      "\ttrain loss: 0.3769118914300502\n",
      "\ttrain loss: 0.48006518573267093\n",
      "\ttrain loss: 0.2949849259464268\n",
      "\ttrain loss: 0.27001720069584934\n",
      "\ttrain loss: 0.5409429410729356\n",
      "\ttrain loss: 0.4395415324205214\n",
      "\ttrain loss: 0.3248954239754158\n",
      "\ttrain loss: 0.42240728043903086\n",
      "\ttrain loss: 0.504554019320854\n",
      "\ttrain loss: 0.23329029809762125\n",
      "\ttrain loss: 0.4380844195131042\n",
      "\ttrain loss: 0.4763040276223921\n",
      "\ttrain loss: 0.3967848180918502\n",
      "\ttrain loss: 0.36746518026571373\n",
      "\ttrain loss: 0.4141519273457833\n",
      "\ttrain loss: 0.4465431378311214\n",
      "\ttrain loss: 0.35647963659062043\n",
      "\ttrain loss: 0.31674811246947426\n",
      "\ttrain loss: 0.3736684200127728\n",
      "\ttrain loss: 0.454341053869973\n",
      "\ttrain loss: 0.2969214099007619\n",
      "\ttrain loss: 0.42821847113312195\n",
      "\ttrain loss: 0.45644413545925383\n",
      "\ttrain loss: 0.3876818243411313\n",
      "\ttrain loss: 0.40326964932527876\n",
      "\ttrain loss: 0.5810140101464063\n",
      "\ttrain loss: 0.33117817518514037\n",
      "\ttrain loss: 0.33168819370674085\n",
      "\ttrain loss: 0.444266334283235\n",
      "\ttrain loss: 0.5084956860957274\n",
      "\ttrain loss: 0.6662668784339562\n",
      "\ttrain loss: 0.5261345696045303\n",
      "\ttrain loss: 0.32761809322125746\n",
      "\ttrain loss: 0.5511746139470115\n",
      "\ttrain loss: 0.4161308005710399\n",
      "\ttrain loss: 0.22868989705222453\n",
      "\ttrain loss: 0.4708366429587039\n",
      "\ttrain loss: 0.5077259435252902\n",
      "\ttrain loss: 0.290249393579851\n",
      "\ttrain loss: 0.5678845633509322\n",
      "\ttrain loss: 0.5479732932299152\n",
      "\ttrain loss: 0.43209301385554383\n",
      "\ttrain loss: 0.6501830337429528\n",
      "\ttrain loss: 0.2829202363577103\n",
      "\ttrain loss: 0.5008888111885825\n",
      "\ttrain loss: 0.3848017917587187\n",
      "\ttrain loss: 0.5075596905076633\n",
      "\ttrain loss: 0.39637361156223816\n",
      "\ttrain loss: 0.417909241573318\n",
      "\ttrain loss: 0.22767401103224563\n",
      "\ttrain loss: 0.6479055952383186\n",
      "\ttrain loss: 0.3993188313136881\n",
      "\ttrain loss: 0.3847086487692705\n",
      "\ttrain loss: 0.34573768640535874\n",
      "\ttrain loss: 0.23605998231089576\n",
      "\ttrain loss: 0.4304716833597929\n",
      "\ttrain loss: 0.2795229261070656\n",
      "\ttrain loss: 0.3512523927615855\n",
      "\ttrain loss: 0.35372888667117397\n",
      "\ttrain loss: 0.573316026204558\n",
      "\ttrain loss: 0.35933937289060425\n",
      "\ttrain loss: 0.4547928250144854\n",
      "\ttrain loss: 0.4586729517986869\n",
      "\ttrain loss: 0.3405180583625832\n",
      "\ttrain loss: 0.477313709932154\n",
      "\ttrain loss: 0.37884868330146215\n",
      "\ttrain loss: 0.5139250401954509\n",
      "\ttrain loss: 0.6239889944881851\n",
      "\ttrain loss: 0.3902012640788513\n",
      "\ttrain loss: 0.37707001343438556\n",
      "\ttrain loss: 0.3299704036501495\n",
      "\ttrain loss: 0.3940152959718456\n",
      "\ttrain loss: 0.4372904473545235\n",
      "\ttrain loss: 0.35696387624265447\n",
      "\ttrain loss: 0.5844976588461235\n",
      "\ttrain loss: 0.5658215361269519\n",
      "\ttrain loss: 0.5682940158590224\n",
      "\ttrain loss: 0.4685495379397741\n",
      "\ttrain loss: 0.4626572081114319\n",
      "\ttrain loss: 0.5176589176551473\n",
      "\ttrain loss: 0.35490266991898767\n",
      "\ttrain loss: 0.6513720736783869\n",
      "\ttrain loss: 0.3802732427787202\n",
      "\ttrain loss: 0.5289079869897484\n",
      "\ttrain loss: 0.4482637568649945\n",
      "\ttrain loss: 0.4186068042875709\n",
      "\ttrain loss: 0.43015592594190616\n",
      "\ttrain loss: 0.4266420987461077\n",
      "\ttrain loss: 0.3191065369357773\n",
      "\ttrain loss: 0.4327133790506532\n",
      "\ttrain loss: 0.4117906934825878\n",
      "\ttrain loss: 0.3573061121867942\n",
      "\ttrain loss: 0.3283834340195183\n",
      "\ttrain loss: 0.4306290246462135\n",
      "\ttrain loss: 0.5471205808496639\n",
      "\ttrain loss: 0.4079697832131005\n",
      "\ttrain loss: 0.48739275648835756\n",
      "\ttrain loss: 0.2925399670230905\n",
      "\ttrain loss: 0.5506834836659814\n",
      "\ttrain loss: 0.2894766582518469\n",
      "\ttrain loss: 0.3363341837027145\n",
      "\ttrain loss: 0.3650996558239066\n",
      "\ttrain loss: 0.7639174662087651\n",
      "\ttrain loss: 0.3690421967678291\n",
      "\ttrain loss: 0.4865147670442114\n",
      "\ttrain loss: 0.2731081723660654\n",
      "\ttrain loss: 0.5452122502541379\n",
      "\ttrain loss: 0.33284100632527025\n",
      "\ttrain loss: 0.4821336541763952\n",
      "\ttrain loss: 0.4215453841923198\n",
      "\ttrain loss: 0.47948920632897396\n",
      "\ttrain loss: 0.524272757113736\n",
      "\ttrain loss: 0.2932705627572987\n",
      "\ttrain loss: 0.3913546739688136\n",
      "\ttrain loss: 0.4087794673516495\n",
      "\ttrain loss: 0.38513295097378863\n",
      "\ttrain loss: 0.2891244662861573\n",
      "\ttrain loss: 0.315774415338002\n",
      "\ttrain loss: 0.3861320192782355\n",
      "\ttrain loss: 0.5815326774583333\n",
      "\ttrain loss: 0.41693341562264996\n",
      "\ttrain loss: 0.33165862986138855\n",
      "\ttrain loss: 0.38433171368522584\n",
      "\ttrain loss: 0.2403961391944477\n",
      "\ttrain loss: 0.2999039282084983\n",
      "\ttrain loss: 0.3432675184694789\n",
      "\ttrain loss: 0.26445324193813796\n",
      "\ttrain loss: 0.49938347611580713\n",
      "\ttrain loss: 0.36612955916150103\n",
      "\ttrain loss: 0.4417789398005286\n",
      "\ttrain loss: 0.40577163648163767\n",
      "\ttrain loss: 0.3942749126565106\n",
      "\ttrain loss: 0.6297354794808858\n",
      "\ttrain loss: 0.6477727866930751\n",
      "\ttrain loss: 0.43492178677409515\n",
      "\ttrain loss: 0.4208637702968232\n",
      "\ttrain loss: 0.43968181524083505\n",
      "\ttrain loss: 0.4955659604202853\n",
      "\ttrain loss: 0.3884403731075839\n",
      "\ttrain loss: 0.5033539797915081\n",
      "\ttrain loss: 0.26951966802033067\n",
      "\ttrain loss: 0.17761455698431783\n",
      "\ttrain loss: 0.2647576037780531\n",
      "\ttrain loss: 0.41890285151477286\n",
      "\ttrain loss: 0.3893019250340273\n",
      "\ttrain loss: 0.2661132025860673\n",
      "\ttrain loss: 0.6091441552714644\n",
      "\ttrain loss: 0.41358679156122047\n",
      "\ttrain loss: 0.4548881856506639\n",
      "\ttrain loss: 0.43573103645419287\n",
      "\ttrain loss: 0.5464336280189543\n",
      "\ttrain loss: 0.5606391939621672\n",
      "\ttrain loss: 0.39640582852911477\n",
      "\ttrain loss: 0.38832000611556855\n",
      "\ttrain loss: 0.5163034137136063\n",
      "\ttrain loss: 0.40492889217088196\n",
      "\ttrain loss: 0.42805088475818065\n",
      "\ttrain loss: 0.35020408680888065\n",
      "\ttrain loss: 0.49568448361883793\n",
      "\ttrain loss: 0.5200333653367774\n",
      "\ttrain loss: 0.32357651904851004\n",
      "\ttrain loss: 0.4733870999374414\n",
      "\ttrain loss: 0.3603008980575412\n",
      "\ttrain loss: 0.33372777153314037\n",
      "\ttrain loss: 0.358320299002641\n",
      "\ttrain loss: 0.37838371740070276\n",
      "\ttrain loss: 0.4768493014666677\n",
      "\ttrain loss: 0.38455373466590703\n",
      "\ttrain loss: 0.378038245150305\n",
      "\ttrain loss: 0.41116928451129037\n",
      "\ttrain loss: 0.3130704607192996\n",
      "\ttrain loss: 0.5163476941460066\n",
      "\ttrain loss: 0.48814901274218664\n",
      "\ttrain loss: 0.3658570957779439\n",
      "\ttrain loss: 0.3829441580175885\n",
      "\ttrain loss: 0.4215166085754499\n",
      "\ttrain loss: 0.4938090489556905\n",
      "\ttrain loss: 0.3434719632654921\n",
      "\ttrain loss: 0.3492024673448898\n",
      "\ttrain loss: 0.319225564403511\n",
      "\ttrain loss: 0.3708878413189155\n",
      "\ttrain loss: 0.43269811982138806\n",
      "\ttrain loss: 0.5340196396736776\n",
      "\ttrain loss: 0.3768814512361334\n",
      "\ttrain loss: 0.4726637520499228\n",
      "\ttrain loss: 0.3710175536791389\n",
      "\ttrain loss: 0.45197449818918123\n",
      "\ttrain loss: 0.41792091282581656\n",
      "\ttrain loss: 0.45002772673998026\n",
      "\ttrain loss: 0.40401847668809976\n",
      "\ttrain loss: 0.4936775780606081\n",
      "\ttrain loss: 0.4657975787272735\n",
      "\ttrain loss: 0.4118196199161155\n",
      "\ttrain loss: 0.33612384501665515\n",
      "\ttrain loss: 0.32220601627420575\n",
      "\ttrain loss: 0.4371721347666638\n",
      "\ttrain loss: 0.5029464966676158\n",
      "\ttrain loss: 0.4218296319210292\n",
      "\ttrain loss: 0.5008273007358648\n",
      "\ttrain loss: 0.4456498686595192\n",
      "\ttrain loss: 0.4208515918075295\n",
      "\ttrain loss: 0.28796423621655415\n",
      "\ttrain loss: 0.38745833830680965\n",
      "\ttrain loss: 0.4593787572218452\n",
      "\ttrain loss: 0.3539881425506395\n",
      "\ttrain loss: 0.4756835436263603\n",
      "\ttrain loss: 0.3877748216499024\n",
      "\ttrain loss: 0.3931071504934651\n",
      "\ttrain loss: 0.5874386566163445\n",
      "\ttrain loss: 0.5870182181308998\n",
      "\ttrain loss: 0.44484563295996266\n",
      "\ttrain loss: 0.6918604976610345\n",
      "\ttrain loss: 0.35240272829504893\n",
      "\ttrain loss: 0.46566505981434814\n",
      "\ttrain loss: 0.4013040665043515\n",
      "\ttrain loss: 0.5280440280540228\n",
      "\ttrain loss: 0.5503600277628581\n",
      "\ttrain loss: 0.46029894873022065\n",
      "\ttrain loss: 0.6411511731333793\n",
      "\ttrain loss: 0.46757109411552583\n",
      "\ttrain loss: 0.41591917058153915\n",
      "\ttrain loss: 0.4937730342773501\n",
      "\ttrain loss: 0.35652706234284126\n",
      "\ttrain loss: 0.411010560770962\n",
      "\ttrain loss: 0.4234406563495979\n",
      "\ttrain loss: 0.45121536591380756\n",
      "\ttrain loss: 0.4853367724703375\n",
      "\ttrain loss: 0.44555095048288607\n",
      "\ttrain loss: 0.5435649773103399\n",
      "\ttrain loss: 0.40528812393782715\n",
      "\ttrain loss: 0.5307305305700317\n",
      "\ttrain loss: 0.41734169885425804\n",
      "\ttrain loss: 0.4353337754356349\n",
      "\ttrain loss: 0.41898178159080574\n",
      "\ttrain loss: 0.47487751768193864\n",
      "\ttrain loss: 0.46315115194134043\n",
      "\ttrain loss: 0.5138688338162523\n",
      "\ttrain loss: 0.29636643053861844\n",
      "\ttrain loss: 0.39376643374689463\n",
      "\ttrain loss: 0.3873387713222905\n",
      "\ttrain loss: 0.34431839312168544\n",
      "\ttrain loss: 0.5004921786621657\n",
      "\ttrain loss: 0.4530818799243805\n",
      "\ttrain loss: 0.4501208580207993\n",
      "\ttrain loss: 0.3127464897579143\n",
      "\ttrain loss: 0.37555467902633377\n",
      "\ttrain loss: 0.47074166562323727\n",
      "\ttrain loss: 0.4251396935515067\n",
      "\ttrain loss: 0.34335145425756\n",
      "\ttrain loss: 0.4799284762403816\n",
      "\ttrain loss: 0.41315820411823617\n",
      "\ttrain loss: 0.4421031533895\n",
      "\ttrain loss: 0.48138187203266264\n",
      "\ttrain loss: 0.2630863421853791\n",
      "\ttrain loss: 0.38514964498277954\n",
      "\ttrain loss: 0.4426990962377795\n",
      "\ttrain loss: 0.48243632567412714\n",
      "\ttrain loss: 0.27405181188804273\n",
      "\ttrain loss: 0.2942465299594555\n",
      "\ttrain loss: 0.3210344082940528\n",
      "\ttrain loss: 0.3349187312671823\n",
      "\ttrain loss: 0.4852185404402628\n",
      "\ttrain loss: 0.28420756723063506\n",
      "\ttrain loss: 0.455127544517429\n",
      "\ttrain loss: 0.3503711162217197\n",
      "\ttrain loss: 0.39434564879433914\n",
      "\ttrain loss: 0.2843836606734089\n",
      "\ttrain loss: 0.3773378000307359\n",
      "\ttrain loss: 0.357194775781736\n",
      "\ttrain loss: 0.43133898574026663\n",
      "\ttrain loss: 0.2918556207864559\n",
      "\ttrain loss: 0.4452345725665869\n",
      "\ttrain loss: 0.3126913935941785\n",
      "\ttrain loss: 0.40203654536537536\n",
      "\ttrain loss: 0.2543531430175914\n",
      "\ttrain loss: 0.3356559406057489\n",
      "\ttrain loss: 0.5660092216924925\n",
      "\ttrain loss: 0.5095064797713142\n",
      "\ttrain loss: 0.30844820017715185\n",
      "\ttrain loss: 0.38203610859377973\n",
      "\ttrain loss: 0.339670957595805\n",
      "\ttrain loss: 0.30747683502952805\n",
      "\ttrain loss: 0.36782023766135735\n",
      "\ttrain loss: 0.6213787724622815\n",
      "\ttrain loss: 0.4201135686102743\n",
      "\ttrain loss: 0.6325997956887038\n",
      "\ttrain loss: 0.27754852195781027\n",
      "\ttrain loss: 0.45952526474125066\n",
      "\ttrain loss: 0.32616659851871543\n",
      "\ttrain loss: 0.43679288552747186\n",
      "\ttrain loss: 0.36648528173843675\n",
      "\ttrain loss: 0.4673342280942041\n",
      "\ttrain loss: 0.3782388179344471\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(9/15) is saved!\n",
      "\ttrain loss: 0.3771915349128472\n",
      "\ttrain loss: 0.4030626775625635\n",
      "\ttrain loss: 0.36380609391435514\n",
      "\ttrain loss: 0.32064880018577635\n",
      "\ttrain loss: 0.3520349778892782\n",
      "\ttrain loss: 0.2574622085790974\n",
      "\ttrain loss: 0.2881299975710081\n",
      "\ttrain loss: 0.4935480415331812\n",
      "\ttrain loss: 0.36046198974367977\n",
      "\ttrain loss: 0.4247865157338176\n",
      "\ttrain loss: 0.4436458979664665\n",
      "\ttrain loss: 0.5114319912596174\n",
      "\ttrain loss: 0.4961265622389149\n",
      "\ttrain loss: 0.3993736203398022\n",
      "\ttrain loss: 0.4390741749816488\n",
      "\ttrain loss: 0.5794151292410177\n",
      "\ttrain loss: 0.46416601336147123\n",
      "\ttrain loss: 0.5754644201566406\n",
      "\ttrain loss: 0.41784151233220856\n",
      "\ttrain loss: 0.6479760520335802\n",
      "\ttrain loss: 0.38149515217397056\n",
      "\ttrain loss: 0.3495659271444751\n",
      "\ttrain loss: 0.6206620856961262\n",
      "\ttrain loss: 0.3969978462761205\n",
      "\ttrain loss: 0.4362622644918297\n",
      "\ttrain loss: 0.41204929562134085\n",
      "\ttrain loss: 0.2943148685986118\n",
      "\ttrain loss: 0.5933090865244104\n",
      "\ttrain loss: 0.403586845007118\n",
      "\ttrain loss: 0.38097312273253714\n",
      "\ttrain loss: 0.3094713592421774\n",
      "\ttrain loss: 0.4169478377227398\n",
      "\ttrain loss: 0.5527733147704341\n",
      "\ttrain loss: 0.517256854284384\n",
      "\ttrain loss: 0.26831846707971035\n",
      "\ttrain loss: 0.33373703160700957\n",
      "\ttrain loss: 0.17220437866413113\n",
      "\ttrain loss: 0.4348657895777229\n",
      "\ttrain loss: 0.450408078265985\n",
      "\ttrain loss: 0.5024140376838768\n",
      "\ttrain loss: 0.35877869686208164\n",
      "\ttrain loss: 0.4707844455241579\n",
      "\ttrain loss: 0.5306323084846507\n",
      "\ttrain loss: 0.46358692787458733\n",
      "\ttrain loss: 0.5946316569736412\n",
      "\ttrain loss: 0.3903347027010632\n",
      "\ttrain loss: 0.40760277046070403\n",
      "\ttrain loss: 0.3087603550504589\n",
      "\ttrain loss: 0.24659132388611993\n",
      "\ttrain loss: 0.35710279892285324\n",
      "\ttrain loss: 0.4309941879737764\n",
      "\ttrain loss: 0.34697616246924906\n",
      "\ttrain loss: 0.5761961680605784\n",
      "\ttrain loss: 0.3049352608643176\n",
      "\ttrain loss: 0.31171473374715436\n",
      "\ttrain loss: 0.2406949089224812\n",
      "\ttrain loss: 0.4306223512531847\n",
      "\ttrain loss: 0.36272790872618155\n",
      "\ttrain loss: 0.36154640200154137\n",
      "\ttrain loss: 0.45430516651181907\n",
      "\ttrain loss: 0.4495445166778641\n",
      "\ttrain loss: 0.24961400058673358\n",
      "\ttrain loss: 0.29391095400363687\n",
      "\ttrain loss: 0.3424071323727389\n",
      "\ttrain loss: 0.28832004759303476\n",
      "\ttrain loss: 0.4879603612767773\n",
      "\ttrain loss: 0.28721751725110506\n",
      "\ttrain loss: 0.34354106683572994\n",
      "\ttrain loss: 0.3820917016691694\n",
      "\ttrain loss: 0.4224336097177872\n",
      "\ttrain loss: 0.2844495118380347\n",
      "\ttrain loss: 0.37569282356034284\n",
      "\ttrain loss: 0.44429813742631197\n",
      "\ttrain loss: 0.3651034711499292\n",
      "\ttrain loss: 0.46640500405494734\n",
      "\ttrain loss: 0.22346268448130485\n",
      "\ttrain loss: 0.327709279185462\n",
      "\ttrain loss: 0.47199929091649595\n",
      "\ttrain loss: 0.5389604823701666\n",
      "\ttrain loss: 0.3656695469170127\n",
      "\ttrain loss: 0.4371347130793824\n",
      "\ttrain loss: 0.5095565283329607\n",
      "\ttrain loss: 0.42191790409671787\n",
      "\ttrain loss: 0.4859686678784774\n",
      "\ttrain loss: 0.3287370419455157\n",
      "\ttrain loss: 0.28380677333671034\n",
      "\ttrain loss: 0.6133425802988353\n",
      "\ttrain loss: 0.43064092226947964\n",
      "\ttrain loss: 0.3593857485425629\n",
      "\ttrain loss: 0.7015102246197709\n",
      "\ttrain loss: 0.5819299610791857\n",
      "\ttrain loss: 0.3884310621843754\n",
      "\ttrain loss: 0.3772345602293591\n",
      "\ttrain loss: 0.5196435531810969\n",
      "\ttrain loss: 0.36282481351070694\n",
      "\ttrain loss: 0.6291104503069536\n",
      "\ttrain loss: 0.5167985848272317\n",
      "\ttrain loss: 0.3895376134627498\n",
      "\ttrain loss: 0.32166010565484315\n",
      "\ttrain loss: 0.4490577867115958\n",
      "\ttrain loss: 0.42696281658229696\n",
      "\ttrain loss: 0.4295404506752613\n",
      "\ttrain loss: 0.2715942160599389\n",
      "\ttrain loss: 0.33306106462710755\n",
      "\ttrain loss: 0.5111160812859278\n",
      "\ttrain loss: 0.35637441396098934\n",
      "\ttrain loss: 0.4394136934561213\n",
      "\ttrain loss: 0.34132751150498647\n",
      "\ttrain loss: 0.27076474527393524\n",
      "\ttrain loss: 0.5919791280597785\n",
      "\ttrain loss: 0.3662571675767015\n",
      "\ttrain loss: 0.4303456515946545\n",
      "\ttrain loss: 0.39698594302181\n",
      "\ttrain loss: 0.29572356782212633\n",
      "\ttrain loss: 0.32146500714586557\n",
      "\ttrain loss: 0.3269976615225791\n",
      "\ttrain loss: 0.5834331306322174\n",
      "\ttrain loss: 0.55050545905118\n",
      "\ttrain loss: 0.4955734764425725\n",
      "\ttrain loss: 0.37654470623738745\n",
      "\ttrain loss: 0.5292818138046098\n",
      "\ttrain loss: 0.36804522961896613\n",
      "\ttrain loss: 0.4640785396026394\n",
      "\ttrain loss: 0.4666749980689999\n",
      "\ttrain loss: 0.4207094993749918\n",
      "\ttrain loss: 0.37554316926985665\n",
      "\ttrain loss: 0.41590390086222084\n",
      "\ttrain loss: 0.2349740915580572\n",
      "\ttrain loss: 0.2458265034499193\n",
      "\ttrain loss: 0.37375007999706233\n",
      "\ttrain loss: 0.42772129490815913\n",
      "\ttrain loss: 0.5535085800134092\n",
      "\ttrain loss: 0.38161407745576237\n",
      "\ttrain loss: 0.4993167312438954\n",
      "\ttrain loss: 0.3603221107047611\n",
      "\ttrain loss: 0.33924395049915373\n",
      "\ttrain loss: 0.24333243692601872\n",
      "\ttrain loss: 0.41320771035862786\n",
      "\ttrain loss: 0.3650756609198221\n",
      "\ttrain loss: 0.3754151176794543\n",
      "\ttrain loss: 0.23317588658103147\n",
      "\ttrain loss: 0.3359061603572213\n",
      "\ttrain loss: 0.42243540599586216\n",
      "\ttrain loss: 0.39233923816453997\n",
      "\ttrain loss: 0.578447877412074\n",
      "\ttrain loss: 0.3754476386000357\n",
      "\ttrain loss: 0.3969198562319337\n",
      "\ttrain loss: 0.40879620989908394\n",
      "\ttrain loss: 0.4823881021893592\n",
      "\ttrain loss: 0.6515199512606686\n",
      "\ttrain loss: 0.42840014589320063\n",
      "\ttrain loss: 0.3622440898977507\n",
      "\ttrain loss: 0.4806967546051709\n",
      "\ttrain loss: 0.4609111428466971\n",
      "\ttrain loss: 0.3975737821282837\n",
      "\ttrain loss: 0.4038377880479872\n",
      "\ttrain loss: 0.8142852108515888\n",
      "\ttrain loss: 0.3293768469534397\n",
      "\ttrain loss: 0.3795393485363338\n",
      "\ttrain loss: 0.3586842649364622\n",
      "\ttrain loss: 0.2784821215497605\n",
      "\ttrain loss: 0.3187228320514944\n",
      "\ttrain loss: 0.40849263510859163\n",
      "\ttrain loss: 0.43737788311059345\n",
      "\ttrain loss: 0.3912579382371373\n",
      "\ttrain loss: 0.31198453400081816\n",
      "\ttrain loss: 0.4092127963397643\n",
      "\ttrain loss: 0.3026640906556771\n",
      "\ttrain loss: 0.3952818234744436\n",
      "\ttrain loss: 0.3073437395543427\n",
      "\ttrain loss: 0.5393465919954884\n",
      "\ttrain loss: 0.4463228449896864\n",
      "\ttrain loss: 0.3383299636739351\n",
      "\ttrain loss: 0.6223411107726017\n",
      "\ttrain loss: 0.5258303708226902\n",
      "\ttrain loss: 0.5207200699992904\n",
      "\ttrain loss: 0.2350716934225499\n",
      "\ttrain loss: 0.449302268173587\n",
      "\ttrain loss: 0.3779760614426112\n",
      "\ttrain loss: 0.4447462200234082\n",
      "\ttrain loss: 0.3425452278110305\n",
      "\ttrain loss: 0.3220078845107538\n",
      "\ttrain loss: 0.3298127597044031\n",
      "\ttrain loss: 0.4184643827786456\n",
      "\ttrain loss: 0.2393909833850281\n",
      "\ttrain loss: 0.41083869209525337\n",
      "\ttrain loss: 0.23599600107956764\n",
      "\ttrain loss: 0.48274821989586636\n",
      "\ttrain loss: 0.33957521643463173\n",
      "\ttrain loss: 0.26773961796431944\n",
      "\ttrain loss: 0.43832400572371333\n",
      "\ttrain loss: 0.36452323951964327\n",
      "\ttrain loss: 0.36590020421923664\n",
      "\ttrain loss: 0.3518466098899546\n",
      "\ttrain loss: 0.4936226452167367\n",
      "\ttrain loss: 0.40801489856513296\n",
      "\ttrain loss: 0.3125066860678819\n",
      "\ttrain loss: 0.2501689762589767\n",
      "\ttrain loss: 0.42472174813528274\n",
      "\ttrain loss: 0.2920356606062698\n",
      "\ttrain loss: 0.436919327178058\n",
      "\ttrain loss: 0.5383445899220447\n",
      "\ttrain loss: 0.2903364728535234\n",
      "\ttrain loss: 0.6396078769175333\n",
      "\ttrain loss: 0.38027007908484645\n",
      "\ttrain loss: 0.40814065380610326\n",
      "\ttrain loss: 0.3018048501323931\n",
      "\ttrain loss: 0.2626201948007669\n",
      "\ttrain loss: 0.34892726982917055\n",
      "\ttrain loss: 0.45200828137805893\n",
      "\ttrain loss: 0.215079272816309\n",
      "\ttrain loss: 0.7147564873354235\n",
      "\ttrain loss: 0.31521112332626433\n",
      "\ttrain loss: 0.5159079260647343\n",
      "\ttrain loss: 0.5443239814381315\n",
      "\ttrain loss: 0.5181970598025789\n",
      "\ttrain loss: 0.23868930769506008\n",
      "\ttrain loss: 0.4720622065220196\n",
      "\ttrain loss: 0.47325474453914973\n",
      "\ttrain loss: 0.46331780010517987\n",
      "\ttrain loss: 0.4314060679421804\n",
      "\ttrain loss: 0.3147908986606952\n",
      "\ttrain loss: 0.2576471608026044\n",
      "\ttrain loss: 0.5285488358844811\n",
      "\ttrain loss: 0.3899373314903708\n",
      "\ttrain loss: 0.5833563065972516\n",
      "\ttrain loss: 0.3738419607371649\n",
      "\ttrain loss: 0.305905698764927\n",
      "\ttrain loss: 0.46702193212935417\n",
      "\ttrain loss: 0.43111992031701996\n",
      "\ttrain loss: 0.4763075950948645\n",
      "\ttrain loss: 0.3710511797040189\n",
      "\ttrain loss: 0.4993398718325317\n",
      "\ttrain loss: 0.6305480353446852\n",
      "\ttrain loss: 0.4621953694342594\n",
      "\ttrain loss: 0.444672917449774\n",
      "\ttrain loss: 0.3249580814322202\n",
      "\ttrain loss: 0.3665340463193967\n",
      "\ttrain loss: 0.5378899900019044\n",
      "\ttrain loss: 0.3837177509369186\n",
      "\ttrain loss: 0.29174017212692127\n",
      "\ttrain loss: 0.39814108702472517\n",
      "\ttrain loss: 0.4219438933451895\n",
      "\ttrain loss: 0.4553365358293766\n",
      "\ttrain loss: 0.3884542269002249\n",
      "\ttrain loss: 0.3879854038482736\n",
      "\ttrain loss: 0.2904623643135956\n",
      "\ttrain loss: 0.62055300279299\n",
      "\ttrain loss: 0.3423315710145227\n",
      "\ttrain loss: 0.2939866366544296\n",
      "\ttrain loss: 0.3786767898703052\n",
      "\ttrain loss: 0.3971683876822875\n",
      "\ttrain loss: 0.38331897484680866\n",
      "\ttrain loss: 0.6283726393914972\n",
      "\ttrain loss: 0.43611842204392076\n",
      "\ttrain loss: 0.4877556279253107\n",
      "\ttrain loss: 0.42258015311177044\n",
      "\ttrain loss: 0.2681694900552902\n",
      "\ttrain loss: 0.37540450020700855\n",
      "\ttrain loss: 0.5937683765859953\n",
      "\ttrain loss: 0.29971878889980547\n",
      "\ttrain loss: 0.4580377062882925\n",
      "\ttrain loss: 0.5099554091490031\n",
      "\ttrain loss: 0.5600940513444095\n",
      "\ttrain loss: 0.5148477996886833\n",
      "\ttrain loss: 0.4253872044528476\n",
      "\ttrain loss: 0.3901552859112767\n",
      "\ttrain loss: 0.36336379382980544\n",
      "\ttrain loss: 0.41272907074898296\n",
      "\ttrain loss: 0.5905671092361057\n",
      "\ttrain loss: 0.5532374397477162\n",
      "\ttrain loss: 0.3579554054933216\n",
      "\ttrain loss: 0.4825560323318402\n",
      "\ttrain loss: 0.26667457555531254\n",
      "\ttrain loss: 0.4655416300313393\n",
      "\ttrain loss: 0.3690242420193495\n",
      "\ttrain loss: 0.3071646774602095\n",
      "\ttrain loss: 0.32618692762338586\n",
      "\ttrain loss: 0.42122350718216994\n",
      "\ttrain loss: 0.4901240959134623\n",
      "\ttrain loss: 0.4435521036476964\n",
      "\ttrain loss: 0.4362330972360253\n",
      "\ttrain loss: 0.26474481208114287\n",
      "\ttrain loss: 0.5057004965723706\n",
      "\ttrain loss: 0.6162230733596762\n",
      "\ttrain loss: 0.44806727206318575\n",
      "\ttrain loss: 0.6415413444410677\n",
      "\ttrain loss: 0.713359200391682\n",
      "\ttrain loss: 0.5321542226628851\n",
      "\ttrain loss: 0.5279094881496097\n",
      "\ttrain loss: 0.408531633564355\n",
      "\ttrain loss: 0.2539615003248295\n",
      "\ttrain loss: 0.4184891920314436\n",
      "\ttrain loss: 0.6539505238636321\n",
      "\ttrain loss: 0.4231112527053426\n",
      "\ttrain loss: 0.49530332686428535\n",
      "\ttrain loss: 0.5105603306860839\n",
      "\ttrain loss: 0.33892372167338136\n",
      "\ttrain loss: 0.3829625457214173\n",
      "\ttrain loss: 0.37386924963893275\n",
      "\ttrain loss: 0.5201152052542157\n",
      "\ttrain loss: 0.3388585135723906\n",
      "\ttrain loss: 0.39003873850648996\n",
      "\ttrain loss: 0.18039009130179634\n",
      "\ttrain loss: 0.3523566593161661\n",
      "\ttrain loss: 0.5129724989531727\n",
      "\ttrain loss: 0.34866271553900907\n",
      "\ttrain loss: 0.2663676103513918\n",
      "\ttrain loss: 0.4478647382555771\n",
      "\ttrain loss: 0.6075097614584857\n",
      "\ttrain loss: 0.2852586821809988\n",
      "\ttrain loss: 0.41173921436836836\n",
      "\ttrain loss: 0.4723553792050262\n",
      "\ttrain loss: 0.25099629367250287\n",
      "\ttrain loss: 0.2774019291682543\n",
      "\ttrain loss: 0.3784970773176086\n",
      "\ttrain loss: 0.3236275909528817\n",
      "\ttrain loss: 0.34455395462519756\n",
      "\ttrain loss: 0.2810278076330195\n",
      "\ttrain loss: 0.574665857178039\n",
      "\ttrain loss: 0.6687170994723581\n",
      "\ttrain loss: 0.43762088424590906\n",
      "\ttrain loss: 0.36593096270303443\n",
      "\ttrain loss: 0.4380121646732784\n",
      "\ttrain loss: 0.5899929498290629\n",
      "\ttrain loss: 0.6052024274533997\n",
      "\ttrain loss: 0.4255297381237574\n",
      "\ttrain loss: 0.45266633127094047\n",
      "\ttrain loss: 0.48460398870225907\n",
      "\ttrain loss: 0.3707141345894933\n",
      "\ttrain loss: 0.5612565991730236\n",
      "\ttrain loss: 0.47891911473273463\n",
      "\ttrain loss: 0.48190509538649273\n",
      "\ttrain loss: 0.4238275336692419\n",
      "\ttrain loss: 0.6066753013291709\n",
      "\ttrain loss: 0.3352568781663055\n",
      "\ttrain loss: 0.3675597847060844\n",
      "\ttrain loss: 0.16856556813941614\n",
      "\ttrain loss: 0.267287170632994\n",
      "\ttrain loss: 0.3232714248162154\n",
      "\ttrain loss: 0.5331331234463974\n",
      "\ttrain loss: 0.42970533652213455\n",
      "\ttrain loss: 0.41857629093016613\n",
      "\ttrain loss: 0.3139973369098531\n",
      "\ttrain loss: 0.4468412864113911\n",
      "\ttrain loss: 0.4112426423136858\n",
      "\ttrain loss: 0.35129288942872905\n",
      "\ttrain loss: 0.34891252746486073\n",
      "\ttrain loss: 0.481657994695346\n",
      "\ttrain loss: 0.4290666319299198\n",
      "\ttrain loss: 0.3160377143095758\n",
      "\ttrain loss: 0.41995307288897094\n",
      "\ttrain loss: 0.4790228928892216\n",
      "\ttrain loss: 0.4195083118811924\n",
      "\ttrain loss: 0.5116126102904517\n",
      "\ttrain loss: 0.46140306583145935\n",
      "\ttrain loss: 0.5305472095901786\n",
      "\ttrain loss: 0.37145027802033326\n",
      "\ttrain loss: 0.32883521993429765\n",
      "\ttrain loss: 0.27296715404994776\n",
      "\ttrain loss: 0.36149099551887\n",
      "\ttrain loss: 0.4320789207629556\n",
      "\ttrain loss: 0.37586974658928823\n",
      "\ttrain loss: 0.4539611701209394\n",
      "\ttrain loss: 0.4847559041164007\n",
      "\ttrain loss: 0.4085397149046704\n",
      "\ttrain loss: 0.32281956780276133\n",
      "\ttrain loss: 0.5263007025976054\n",
      "\ttrain loss: 0.32046525617910915\n",
      "\ttrain loss: 0.3190842446372606\n",
      "\ttrain loss: 0.35837478246243304\n",
      "\ttrain loss: 0.3799929070740193\n",
      "\ttrain loss: 0.41933082034292246\n",
      "\ttrain loss: 0.29949091593178356\n",
      "\ttrain loss: 0.4172928296617308\n",
      "\ttrain loss: 0.6362975897671987\n",
      "\ttrain loss: 0.4144031650869341\n",
      "\ttrain loss: 0.3635007055369019\n",
      "\ttrain loss: 0.35950025045488776\n",
      "\ttrain loss: 0.3484425215001169\n",
      "\ttrain loss: 0.273823723377402\n",
      "\ttrain loss: 0.4539593668451541\n",
      "\ttrain loss: 0.19376691017065534\n",
      "\ttrain loss: 0.21175682844515376\n",
      "\ttrain loss: 0.5095945829329244\n",
      "\ttrain loss: 0.4214643611567311\n",
      "\ttrain loss: 0.3192935945768872\n",
      "\ttrain loss: 0.3763674160820961\n",
      "\ttrain loss: 0.5195706916012722\n",
      "\ttrain loss: 0.35849952743262054\n",
      "\ttrain loss: 0.4734358556249304\n",
      "\ttrain loss: 0.4228366360220401\n",
      "\ttrain loss: 0.40209423280898715\n",
      "\ttrain loss: 0.317540609433274\n",
      "\ttrain loss: 0.3309930685000651\n",
      "\ttrain loss: 0.3444686265939562\n",
      "\ttrain loss: 0.44331334037740633\n",
      "\ttrain loss: 0.35757237779738416\n",
      "\ttrain loss: 0.5372774360530665\n",
      "\ttrain loss: 0.5176834843591394\n",
      "\ttrain loss: 0.3606225529201512\n",
      "\ttrain loss: 0.4158738287613688\n",
      "\ttrain loss: 0.3247922039471732\n",
      "\ttrain loss: 0.27990407497048675\n",
      "\ttrain loss: 0.4722748831269596\n",
      "\ttrain loss: 0.27295388333372206\n",
      "\ttrain loss: 0.5005860930556292\n",
      "\ttrain loss: 0.37355793822911937\n",
      "\ttrain loss: 0.3457558010382118\n",
      "\ttrain loss: 0.25610565563616033\n",
      "\ttrain loss: 0.31035855141351487\n",
      "\ttrain loss: 0.5225696730425363\n",
      "\ttrain loss: 0.24921854300456825\n",
      "\ttrain loss: 0.5199486896771204\n",
      "\ttrain loss: 0.4708146092087638\n",
      "\ttrain loss: 0.30297805164124\n",
      "\ttrain loss: 0.5688630273189791\n",
      "\ttrain loss: 0.3563589781797545\n",
      "\ttrain loss: 0.5033799966215544\n",
      "\ttrain loss: 0.46620879395248094\n",
      "\ttrain loss: 0.5315971129055354\n",
      "\ttrain loss: 0.36506166730209627\n",
      "\ttrain loss: 0.49875998715931574\n",
      "\ttrain loss: 0.685536629751123\n",
      "\ttrain loss: 0.25427436269027176\n",
      "\ttrain loss: 0.35046940636690693\n",
      "\ttrain loss: 0.48276869252778143\n",
      "\ttrain loss: 0.34661078275736523\n",
      "\ttrain loss: 0.49581267623495795\n",
      "\ttrain loss: 0.42995911053948566\n",
      "\ttrain loss: 0.26163645012896286\n",
      "\ttrain loss: 0.44191102682015393\n",
      "\ttrain loss: 0.6108406286197914\n",
      "\ttrain loss: 0.5144068424373625\n",
      "\ttrain loss: 0.5552223397954525\n",
      "\ttrain loss: 0.46873855763091066\n",
      "\ttrain loss: 0.33588742486266043\n",
      "\ttrain loss: 0.522710959123514\n",
      "\ttrain loss: 0.39387923172608386\n",
      "\ttrain loss: 0.5188706873171196\n",
      "\ttrain loss: 0.4161795773672404\n",
      "\ttrain loss: 0.49076521611953344\n",
      "\ttrain loss: 0.28330917480823503\n",
      "\ttrain loss: 0.45786232941529476\n",
      "\ttrain loss: 0.26812988173005403\n",
      "\ttrain loss: 0.46517990660599606\n",
      "\ttrain loss: 0.3716582723291204\n",
      "\ttrain loss: 0.37876503686226326\n",
      "\ttrain loss: 0.39637026586761787\n",
      "\ttrain loss: 0.311285700821802\n",
      "\ttrain loss: 0.3749076116318296\n",
      "\ttrain loss: 0.4357391639947775\n",
      "\ttrain loss: 0.5599968041901038\n",
      "\ttrain loss: 0.35668258305101186\n",
      "\ttrain loss: 0.39052630794556437\n",
      "\ttrain loss: 0.33444349753891567\n",
      "\ttrain loss: 0.48382704772547\n",
      "\ttrain loss: 0.41142593007533934\n",
      "\ttrain loss: 0.39339135646139933\n",
      "\ttrain loss: 0.2940370002747672\n",
      "\ttrain loss: 0.33468474068088927\n",
      "\ttrain loss: 0.41736644421420854\n",
      "\ttrain loss: 0.3705632727120473\n",
      "\ttrain loss: 0.4113183692962644\n",
      "\ttrain loss: 0.3136640560229538\n",
      "\ttrain loss: 0.41439886917369506\n",
      "\ttrain loss: 0.5028675325641492\n",
      "\ttrain loss: 0.5595713047354913\n",
      "\ttrain loss: 0.3524173354472592\n",
      "\ttrain loss: 0.36666713801511536\n",
      "\ttrain loss: 0.47390379769935354\n",
      "\ttrain loss: 0.3633664440257999\n",
      "\ttrain loss: 0.3428032759445848\n",
      "\ttrain loss: 0.4353449951001014\n",
      "\ttrain loss: 0.3545318330968429\n",
      "\ttrain loss: 0.5608817306043596\n",
      "\ttrain loss: 0.30757419298190575\n",
      "\ttrain loss: 0.2866801337977696\n",
      "\ttrain loss: 0.38651375240557834\n",
      "\ttrain loss: 0.5358709685576353\n",
      "\ttrain loss: 0.40693585873424776\n",
      "\ttrain loss: 0.41061611072141896\n",
      "\ttrain loss: 0.3526854621967638\n",
      "\ttrain loss: 0.5049131508156623\n",
      "\ttrain loss: 0.34277617962524976\n",
      "\ttrain loss: 0.3429056619875808\n",
      "\ttrain loss: 0.3263527875546439\n",
      "\ttrain loss: 0.5926722457350987\n",
      "\ttrain loss: 0.30187294871064047\n",
      "\ttrain loss: 0.4057186676011275\n",
      "\ttrain loss: 0.6129552005049277\n",
      "\ttrain loss: 0.4689947765506164\n",
      "\ttrain loss: 0.40865904695339517\n",
      "\ttrain loss: 0.3001478662183453\n",
      "\ttrain loss: 0.22447176618321665\n",
      "\ttrain loss: 0.5130908033890319\n",
      "\ttrain loss: 0.42960576543103246\n",
      "\ttrain loss: 0.3475205611872003\n",
      "\ttrain loss: 0.46819885916493054\n",
      "\ttrain loss: 0.472430785514187\n",
      "\ttrain loss: 0.53963953382627\n",
      "\ttrain loss: 0.39658193539972436\n",
      "\ttrain loss: 0.3546811086035415\n",
      "\ttrain loss: 0.33907439069939993\n",
      "\ttrain loss: 0.3033111093503561\n",
      "\ttrain loss: 0.5501417057444242\n",
      "\ttrain loss: 0.29271676207138425\n",
      "\ttrain loss: 0.4264756289056922\n",
      "\ttrain loss: 0.41342994043104797\n",
      "\ttrain loss: 0.35447395950986654\n",
      "\ttrain loss: 0.46361504247967095\n",
      "\ttrain loss: 0.44026476666719144\n",
      "\ttrain loss: 0.41584613750193744\n",
      "\ttrain loss: 0.29876713921431997\n",
      "\ttrain loss: 0.45736137447098607\n",
      "\ttrain loss: 0.3077848360477878\n",
      "\ttrain loss: 0.3405742947540904\n",
      "\ttrain loss: 0.5064536068221951\n",
      "\ttrain loss: 0.44501401777774324\n",
      "\ttrain loss: 0.4615922652751079\n",
      "\ttrain loss: 0.31993969553990387\n",
      "\ttrain loss: 0.43962769598776485\n",
      "\ttrain loss: 0.29501226959184235\n",
      "\ttrain loss: 0.3173053323369619\n",
      "\ttrain loss: 0.3590314280877814\n",
      "\ttrain loss: 0.37886229053158116\n",
      "\ttrain loss: 0.48558389554563197\n",
      "\ttrain loss: 0.5605503086973858\n",
      "\ttrain loss: 0.38036276725670504\n",
      "\ttrain loss: 0.30110990797826687\n",
      "\ttrain loss: 0.3269823812429181\n",
      "\ttrain loss: 0.32117132669169063\n",
      "\ttrain loss: 0.5625603130476327\n",
      "\ttrain loss: 0.30795279158830957\n",
      "\ttrain loss: 0.3413454890495319\n",
      "\ttrain loss: 0.2149985722102924\n",
      "\ttrain loss: 0.28264817417268595\n",
      "\ttrain loss: 0.322907309794475\n",
      "\ttrain loss: 0.4318757985182411\n",
      "\ttrain loss: 0.4848617783374876\n",
      "\ttrain loss: 0.309023501126449\n",
      "\ttrain loss: 0.35601083881097106\n",
      "\ttrain loss: 0.4414923545181607\n",
      "\ttrain loss: 0.35594084413857663\n",
      "\ttrain loss: 0.4772569011071446\n",
      "\ttrain loss: 0.5218469939946849\n",
      "\ttrain loss: 0.5061402874636848\n",
      "\ttrain loss: 0.5411175497748576\n",
      "\ttrain loss: 0.5046508096907787\n",
      "\ttrain loss: 0.5890082045007448\n",
      "\ttrain loss: 0.4745318340778598\n",
      "\ttrain loss: 0.23410752119083253\n",
      "\ttrain loss: 0.28107553250495043\n",
      "\ttrain loss: 0.38094353537446124\n",
      "\ttrain loss: 0.43377917126501575\n",
      "\ttrain loss: 0.3082227024175105\n",
      "\ttrain loss: 0.33817741748790076\n",
      "\ttrain loss: 0.4861966451044911\n",
      "\ttrain loss: 0.5286782689077553\n",
      "\ttrain loss: 0.5318706132419615\n",
      "\ttrain loss: 0.4976204456127062\n",
      "\ttrain loss: 0.6144617717937217\n",
      "\ttrain loss: 0.3476597612235235\n",
      "\ttrain loss: 0.25838747013985547\n",
      "\ttrain loss: 0.41752303163318494\n",
      "\ttrain loss: 0.28391175198023677\n",
      "\ttrain loss: 0.45623842623268085\n",
      "\ttrain loss: 0.40388932292947366\n",
      "\ttrain loss: 0.34839243512104123\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(10/15) is saved!\n",
      "\ttrain loss: 0.41856910196712166\n",
      "\ttrain loss: 0.4592474134745033\n",
      "\ttrain loss: 0.3817306849467075\n",
      "\ttrain loss: 0.32697683153705326\n",
      "\ttrain loss: 0.3813486407274709\n",
      "\ttrain loss: 0.25404497378236257\n",
      "\ttrain loss: 0.33370807956890997\n",
      "\ttrain loss: 0.3795210745988843\n",
      "\ttrain loss: 0.4289245850692566\n",
      "\ttrain loss: 0.3430150668690992\n",
      "\ttrain loss: 0.35952682211311054\n",
      "\ttrain loss: 0.4670951375385858\n",
      "\ttrain loss: 0.38746583827000936\n",
      "\ttrain loss: 0.29071157308872\n",
      "\ttrain loss: 0.41349415914870596\n",
      "\ttrain loss: 0.4567009470345826\n",
      "\ttrain loss: 0.2601283539863055\n",
      "\ttrain loss: 0.5212217110808353\n",
      "\ttrain loss: 0.24715771376286833\n",
      "\ttrain loss: 0.3756206758841003\n",
      "\ttrain loss: 0.508904577147782\n",
      "\ttrain loss: 0.4806722148869539\n",
      "\ttrain loss: 0.30263036452664693\n",
      "\ttrain loss: 0.31533462751575525\n",
      "\ttrain loss: 0.31460129946528786\n",
      "\ttrain loss: 0.3800186777272435\n",
      "\ttrain loss: 0.3506719025118238\n",
      "\ttrain loss: 0.2878134696660013\n",
      "\ttrain loss: 0.4086091813456997\n",
      "\ttrain loss: 0.4612420031446673\n",
      "\ttrain loss: 0.2155755629014651\n",
      "\ttrain loss: 0.3288200096563435\n",
      "\ttrain loss: 0.3621415615070668\n",
      "\ttrain loss: 0.3632186477125596\n",
      "\ttrain loss: 0.49305398712425424\n",
      "\ttrain loss: 0.4140526098214143\n",
      "\ttrain loss: 0.31222119644657814\n",
      "\ttrain loss: 0.42552098214324074\n",
      "\ttrain loss: 0.4432454379663725\n",
      "\ttrain loss: 0.2601559506082245\n",
      "\ttrain loss: 0.3443896123985062\n",
      "\ttrain loss: 0.26759170732631377\n",
      "\ttrain loss: 0.43391943319043724\n",
      "\ttrain loss: 0.3532242971220445\n",
      "\ttrain loss: 0.4626746861630938\n",
      "\ttrain loss: 0.21291424123383412\n",
      "\ttrain loss: 0.3498574189312873\n",
      "\ttrain loss: 0.3290265379413108\n",
      "\ttrain loss: 0.528341539657863\n",
      "\ttrain loss: 0.42371609780272895\n",
      "\ttrain loss: 0.4658749912896945\n",
      "\ttrain loss: 0.415383185578329\n",
      "\ttrain loss: 0.57354545558148\n",
      "\ttrain loss: 0.3360387127327772\n",
      "\ttrain loss: 0.3412398073229143\n",
      "\ttrain loss: 0.29311906240718183\n",
      "\ttrain loss: 0.5079441488414547\n",
      "\ttrain loss: 0.36737290959381275\n",
      "\ttrain loss: 0.3368688846039196\n",
      "\ttrain loss: 0.4959104263257884\n",
      "\ttrain loss: 0.36239138487441963\n",
      "\ttrain loss: 0.42830040861253255\n",
      "\ttrain loss: 0.631337242369877\n",
      "\ttrain loss: 0.2360025723734807\n",
      "\ttrain loss: 0.39303537586762305\n",
      "\ttrain loss: 0.6046194666735837\n",
      "\ttrain loss: 0.5766860371779614\n",
      "\ttrain loss: 0.49133298116110136\n",
      "\ttrain loss: 0.46414477421476863\n",
      "\ttrain loss: 0.3761883877771588\n",
      "\ttrain loss: 0.24720572475980512\n",
      "\ttrain loss: 0.4041635831060796\n",
      "\ttrain loss: 0.7205881900598765\n",
      "\ttrain loss: 0.4483796773900086\n",
      "\ttrain loss: 0.38636243547624516\n",
      "\ttrain loss: 0.4311793172696763\n",
      "\ttrain loss: 0.47484331994828066\n",
      "\ttrain loss: 0.33701755701108604\n",
      "\ttrain loss: 0.38212947893779375\n",
      "\ttrain loss: 0.4424522474753094\n",
      "\ttrain loss: 0.4697004368180215\n",
      "\ttrain loss: 0.5098006055357077\n",
      "\ttrain loss: 0.31195412751942736\n",
      "\ttrain loss: 0.3107429865777746\n",
      "\ttrain loss: 0.3759760100316289\n",
      "\ttrain loss: 0.41206073463663584\n",
      "\ttrain loss: 0.5268816591973583\n",
      "\ttrain loss: 0.3467715603881383\n",
      "\ttrain loss: 0.29170838535534727\n",
      "\ttrain loss: 0.29064152688973666\n",
      "\ttrain loss: 0.48585432546231677\n",
      "\ttrain loss: 0.4676216209187478\n",
      "\ttrain loss: 0.4219584480060668\n",
      "\ttrain loss: 0.367065353266106\n",
      "\ttrain loss: 0.6358543087457282\n",
      "\ttrain loss: 0.6458242641064822\n",
      "\ttrain loss: 0.30841537026264293\n",
      "\ttrain loss: 0.17624835271292433\n",
      "\ttrain loss: 0.33728614465801543\n",
      "\ttrain loss: 0.45900148831801557\n",
      "\ttrain loss: 0.3559473118274037\n",
      "\ttrain loss: 0.5479828661901525\n",
      "\ttrain loss: 0.31550473432926607\n",
      "\ttrain loss: 0.338070565486392\n",
      "\ttrain loss: 0.467154835690075\n",
      "\ttrain loss: 0.5304426371977863\n",
      "\ttrain loss: 0.35512823996925735\n",
      "\ttrain loss: 0.46843209121733753\n",
      "\ttrain loss: 0.3416742663639375\n",
      "\ttrain loss: 0.26664167900787894\n",
      "\ttrain loss: 0.40603179931958855\n",
      "\ttrain loss: 0.5168329617697004\n",
      "\ttrain loss: 0.6364656292161002\n",
      "\ttrain loss: 0.4228485325055666\n",
      "\ttrain loss: 0.4195100856785048\n",
      "\ttrain loss: 0.36482254289686256\n",
      "\ttrain loss: 0.31831301507695503\n",
      "\ttrain loss: 0.34545060987227383\n",
      "\ttrain loss: 0.31591171939794543\n",
      "\ttrain loss: 0.3765238044310971\n",
      "\ttrain loss: 0.39364286090518097\n",
      "\ttrain loss: 0.4932675896993697\n",
      "\ttrain loss: 0.5279801036226557\n",
      "\ttrain loss: 0.4530538978923849\n",
      "\ttrain loss: 0.2661052824597421\n",
      "\ttrain loss: 0.3218583447291123\n",
      "\ttrain loss: 0.5566822523202246\n",
      "\ttrain loss: 0.38547153926844624\n",
      "\ttrain loss: 0.38684864103741845\n",
      "\ttrain loss: 0.42075083565789706\n",
      "\ttrain loss: 0.3811286431366039\n",
      "\ttrain loss: 0.3630179055657605\n",
      "\ttrain loss: 0.25673982073029084\n",
      "\ttrain loss: 0.41247374870298786\n",
      "\ttrain loss: 0.29430837469946486\n",
      "\ttrain loss: 0.5684088781851407\n",
      "\ttrain loss: 0.35988051698143625\n",
      "\ttrain loss: 0.4675628880918147\n",
      "\ttrain loss: 0.3839650736528585\n",
      "\ttrain loss: 0.4563240866562298\n",
      "\ttrain loss: 0.481175531240143\n",
      "\ttrain loss: 0.24360497902005823\n",
      "\ttrain loss: 0.5410487479452796\n",
      "\ttrain loss: 0.5913646886798707\n",
      "\ttrain loss: 0.3335839044282043\n",
      "\ttrain loss: 0.401603083871316\n",
      "\ttrain loss: 0.5119896839076311\n",
      "\ttrain loss: 0.39546216924832345\n",
      "\ttrain loss: 0.5358240452778269\n",
      "\ttrain loss: 0.37260928752375216\n",
      "\ttrain loss: 0.495866780089711\n",
      "\ttrain loss: 0.3509442548526538\n",
      "\ttrain loss: 0.48271502974461955\n",
      "\ttrain loss: 0.2617662739190005\n",
      "\ttrain loss: 0.3501580381494312\n",
      "\ttrain loss: 0.4174925375869851\n",
      "\ttrain loss: 0.7849605321442512\n",
      "\ttrain loss: 0.36607187283107767\n",
      "\ttrain loss: 0.42588455476850984\n",
      "\ttrain loss: 0.3667861834474586\n",
      "\ttrain loss: 0.6908376393969617\n",
      "\ttrain loss: 0.3841177341775158\n",
      "\ttrain loss: 0.3993983831012262\n",
      "\ttrain loss: 0.4373145450659536\n",
      "\ttrain loss: 0.5517699251433181\n",
      "\ttrain loss: 0.6309418021255232\n",
      "\ttrain loss: 0.5335869361791963\n",
      "\ttrain loss: 0.390621990994602\n",
      "\ttrain loss: 0.30990769990119915\n",
      "\ttrain loss: 0.47754745523885356\n",
      "\ttrain loss: 0.4043876192431717\n",
      "\ttrain loss: 0.2971257088311483\n",
      "\ttrain loss: 0.3791648061287029\n",
      "\ttrain loss: 0.45148335729290173\n",
      "\ttrain loss: 0.30649398085113366\n",
      "\ttrain loss: 0.3708250465226502\n",
      "\ttrain loss: 0.3265908032438513\n",
      "\ttrain loss: 0.35384891037656563\n",
      "\ttrain loss: 0.3194858892832342\n",
      "\ttrain loss: 0.42889446974255696\n",
      "\ttrain loss: 0.44034605687538875\n",
      "\ttrain loss: 0.3266709494815032\n",
      "\ttrain loss: 0.4166364103537712\n",
      "\ttrain loss: 0.3555063318004835\n",
      "\ttrain loss: 0.4381268800618242\n",
      "\ttrain loss: 0.3201995277051681\n",
      "\ttrain loss: 0.39801237348236307\n",
      "\ttrain loss: 0.556470070254752\n",
      "\ttrain loss: 0.40450521588776805\n",
      "\ttrain loss: 0.4010302543244324\n",
      "\ttrain loss: 0.5698677359290677\n",
      "\ttrain loss: 0.2516063753373979\n",
      "\ttrain loss: 0.4698137615114538\n",
      "\ttrain loss: 0.44055176152639186\n",
      "\ttrain loss: 0.48126513992286657\n",
      "\ttrain loss: 0.264905336095121\n",
      "\ttrain loss: 0.40387929092017955\n",
      "\ttrain loss: 0.6875055105900332\n",
      "\ttrain loss: 0.3068184178444946\n",
      "\ttrain loss: 0.5254132542971404\n",
      "\ttrain loss: 0.2077565035803222\n",
      "\ttrain loss: 0.3322501216790244\n",
      "\ttrain loss: 0.5734791272693681\n",
      "\ttrain loss: 0.36162988264781437\n",
      "\ttrain loss: 0.32933605347117456\n",
      "\ttrain loss: 0.3756098064737654\n",
      "\ttrain loss: 0.36275752608550604\n",
      "\ttrain loss: 0.6837358209847573\n",
      "\ttrain loss: 0.41260302406196725\n",
      "\ttrain loss: 0.36087273039542556\n",
      "\ttrain loss: 0.304502784115401\n",
      "\ttrain loss: 0.3077553331703081\n",
      "\ttrain loss: 0.37089967072719665\n",
      "\ttrain loss: 0.4809347454337412\n",
      "\ttrain loss: 0.4167849120128281\n",
      "\ttrain loss: 0.3562844625831332\n",
      "\ttrain loss: 0.4636435108752496\n",
      "\ttrain loss: 0.2726798653677568\n",
      "\ttrain loss: 0.2887909438769516\n",
      "\ttrain loss: 0.3034312482716028\n",
      "\ttrain loss: 0.35726246468229206\n",
      "\ttrain loss: 0.4467531438391748\n",
      "\ttrain loss: 0.47938025417833485\n",
      "\ttrain loss: 0.4512214802628777\n",
      "\ttrain loss: 0.6057022323614333\n",
      "\ttrain loss: 0.43952978708554424\n",
      "\ttrain loss: 0.19160499778770884\n",
      "\ttrain loss: 0.40109326586578503\n",
      "\ttrain loss: 0.37328634875896194\n",
      "\ttrain loss: 0.5514058261760152\n",
      "\ttrain loss: 0.5925790873321994\n",
      "\ttrain loss: 0.4047187266322165\n",
      "\ttrain loss: 0.49577143529723233\n",
      "\ttrain loss: 0.4876763675150941\n",
      "\ttrain loss: 0.42078935622190466\n",
      "\ttrain loss: 0.33837549264811606\n",
      "\ttrain loss: 0.37879135675064357\n",
      "\ttrain loss: 0.5331649390131944\n",
      "\ttrain loss: 0.36295319808666066\n",
      "\ttrain loss: 0.2933490952492961\n",
      "\ttrain loss: 0.4366177960752805\n",
      "\ttrain loss: 0.29808479437843766\n",
      "\ttrain loss: 0.4451897028331482\n",
      "\ttrain loss: 0.4237731582549321\n",
      "\ttrain loss: 0.29790475846141296\n",
      "\ttrain loss: 0.4651045571405297\n",
      "\ttrain loss: 0.36498004124384364\n",
      "\ttrain loss: 0.44396213783869654\n",
      "\ttrain loss: 0.39135701866121364\n",
      "\ttrain loss: 0.387580981686321\n",
      "\ttrain loss: 0.40667646290586357\n",
      "\ttrain loss: 0.33835024005680314\n",
      "\ttrain loss: 0.6993929649142001\n",
      "\ttrain loss: 0.7753616255607989\n",
      "\ttrain loss: 0.4504780706248089\n",
      "\ttrain loss: 0.20511154457594172\n",
      "\ttrain loss: 0.4921471383376659\n",
      "\ttrain loss: 0.6298905208441219\n",
      "\ttrain loss: 0.24516782704780266\n",
      "\ttrain loss: 0.3206459326754061\n",
      "\ttrain loss: 0.4168114872426286\n",
      "\ttrain loss: 0.4364866318628682\n",
      "\ttrain loss: 0.33355834205253243\n",
      "\ttrain loss: 0.3145236555384293\n",
      "\ttrain loss: 0.4388136789202665\n",
      "\ttrain loss: 0.339753332666473\n",
      "\ttrain loss: 0.4958441711354759\n",
      "\ttrain loss: 0.2900093132209492\n",
      "\ttrain loss: 0.3511019735718799\n",
      "\ttrain loss: 0.40192148780586057\n",
      "\ttrain loss: 0.47404640139704296\n",
      "\ttrain loss: 0.4432861539708748\n",
      "\ttrain loss: 0.519965597508502\n",
      "\ttrain loss: 0.2644495128207755\n",
      "\ttrain loss: 0.2419992987577035\n",
      "\ttrain loss: 0.3536120165787625\n",
      "\ttrain loss: 0.4475431391620346\n",
      "\ttrain loss: 0.39250240933779046\n",
      "\ttrain loss: 0.4907180770866188\n",
      "\ttrain loss: 0.34355869291382524\n",
      "\ttrain loss: 0.6426308077264484\n",
      "\ttrain loss: 0.4424782915327864\n",
      "\ttrain loss: 0.3492958691710516\n",
      "\ttrain loss: 0.4949627245684458\n",
      "\ttrain loss: 0.2534124015167513\n",
      "\ttrain loss: 0.2781333527836726\n",
      "\ttrain loss: 0.4344974801706038\n",
      "\ttrain loss: 0.5171849740956542\n",
      "\ttrain loss: 0.3601518609593968\n",
      "\ttrain loss: 0.40100666352241654\n",
      "\ttrain loss: 0.5303172468490661\n",
      "\ttrain loss: 0.26885717824872685\n",
      "\ttrain loss: 0.35502075432466873\n",
      "\ttrain loss: 0.29785106450491416\n",
      "\ttrain loss: 0.5102660339589482\n",
      "\ttrain loss: 0.3492892853121563\n",
      "\ttrain loss: 0.4672589232913473\n",
      "\ttrain loss: 0.5036264296615138\n",
      "\ttrain loss: 0.2916389787035458\n",
      "\ttrain loss: 0.5284580879750258\n",
      "\ttrain loss: 0.5183144587272437\n",
      "\ttrain loss: 0.42993157200590243\n",
      "\ttrain loss: 0.34942143719107366\n",
      "\ttrain loss: 0.5444929033647864\n",
      "\ttrain loss: 0.31861289834935236\n",
      "\ttrain loss: 0.36427252825358236\n",
      "\ttrain loss: 0.35823328174910213\n",
      "\ttrain loss: 0.43037978241362235\n",
      "\ttrain loss: 0.29194729037153944\n",
      "\ttrain loss: 0.45644900114495657\n",
      "\ttrain loss: 0.3185447361765027\n",
      "\ttrain loss: 0.3701459534846086\n",
      "\ttrain loss: 0.4120717939325321\n",
      "\ttrain loss: 0.28595283449987197\n",
      "\ttrain loss: 0.23523701781960585\n",
      "\ttrain loss: 0.29867339047226915\n",
      "\ttrain loss: 0.37212044264763033\n",
      "\ttrain loss: 0.2550172318331413\n",
      "\ttrain loss: 0.41323245747727955\n",
      "\ttrain loss: 0.3595671773349871\n",
      "\ttrain loss: 0.48700975873303737\n",
      "\ttrain loss: 0.37624757546411425\n",
      "\ttrain loss: 0.2590263274394209\n",
      "\ttrain loss: 0.3793801790135341\n",
      "\ttrain loss: 0.4970343304485191\n",
      "\ttrain loss: 0.5898841064923751\n",
      "\ttrain loss: 0.4088716949774694\n",
      "\ttrain loss: 0.3545434462072228\n",
      "\ttrain loss: 0.40887049461074354\n",
      "\ttrain loss: 0.31208601128413366\n",
      "\ttrain loss: 0.6803000280591572\n",
      "\ttrain loss: 0.4007174011142044\n",
      "\ttrain loss: 0.42362284600633004\n",
      "\ttrain loss: 0.43062833226756386\n",
      "\ttrain loss: 0.3788838528248464\n",
      "\ttrain loss: 0.4033622101828559\n",
      "\ttrain loss: 0.4227268653476557\n",
      "\ttrain loss: 0.4435872105667859\n",
      "\ttrain loss: 0.36505685676856947\n",
      "\ttrain loss: 0.3410739682322953\n",
      "\ttrain loss: 0.47846190622124884\n",
      "\ttrain loss: 0.5925526799026742\n",
      "\ttrain loss: 0.38098819095919767\n",
      "\ttrain loss: 0.43247417659285936\n",
      "\ttrain loss: 0.46764478624029604\n",
      "\ttrain loss: 0.46768661494431973\n",
      "\ttrain loss: 0.4495204744913026\n",
      "\ttrain loss: 0.43582200749870426\n",
      "\ttrain loss: 0.43142262348395755\n",
      "\ttrain loss: 0.3428444649608843\n",
      "\ttrain loss: 0.3856221057599095\n",
      "\ttrain loss: 0.31056618603000463\n",
      "\ttrain loss: 0.268013115090885\n",
      "\ttrain loss: 0.37605731504464573\n",
      "\ttrain loss: 0.3221305791182217\n",
      "\ttrain loss: 0.4481896772460165\n",
      "\ttrain loss: 0.42134890863570107\n",
      "\ttrain loss: 0.42502318619226703\n",
      "\ttrain loss: 0.42955286532047743\n",
      "\ttrain loss: 0.4189866331784353\n",
      "\ttrain loss: 0.42583767152733154\n",
      "\ttrain loss: 0.5112321931147783\n",
      "\ttrain loss: 0.4373796672314846\n",
      "\ttrain loss: 0.403703388533861\n",
      "\ttrain loss: 0.306401732377544\n",
      "\ttrain loss: 0.44857802529075586\n",
      "\ttrain loss: 0.32392213483829063\n",
      "\ttrain loss: 0.3753865358923267\n",
      "\ttrain loss: 0.47111577824579376\n",
      "\ttrain loss: 0.540457291784872\n",
      "\ttrain loss: 0.6185629623952558\n",
      "\ttrain loss: 0.31418056960948704\n",
      "\ttrain loss: 0.32232485924267096\n",
      "\ttrain loss: 0.4506919314492009\n",
      "\ttrain loss: 0.60376350836426\n",
      "\ttrain loss: 0.4823325141936817\n",
      "\ttrain loss: 0.43489675776825715\n",
      "\ttrain loss: 0.37179848721364706\n",
      "\ttrain loss: 0.34584479321557726\n",
      "\ttrain loss: 0.40051260078550444\n",
      "\ttrain loss: 0.27867072761402223\n",
      "\ttrain loss: 0.36763877556139035\n",
      "\ttrain loss: 0.4113485231686526\n",
      "\ttrain loss: 0.46590589586300957\n",
      "\ttrain loss: 0.29853793805924883\n",
      "\ttrain loss: 0.46441619792776045\n",
      "\ttrain loss: 0.29703898417092334\n",
      "\ttrain loss: 0.42251832656068394\n",
      "\ttrain loss: 0.39615019836493787\n",
      "\ttrain loss: 0.3050668721632979\n",
      "\ttrain loss: 0.30324284013021796\n",
      "\ttrain loss: 0.34803810522349726\n",
      "\ttrain loss: 0.667784380493375\n",
      "\ttrain loss: 0.4374396377145817\n",
      "\ttrain loss: 0.43142893333738447\n",
      "\ttrain loss: 0.34231028246283757\n",
      "\ttrain loss: 0.3808044519156144\n",
      "\ttrain loss: 0.5854366451803974\n",
      "\ttrain loss: 0.4127710743564389\n",
      "\ttrain loss: 0.41841773312539887\n",
      "\ttrain loss: 0.4647344779934593\n",
      "\ttrain loss: 0.4280335611525893\n",
      "\ttrain loss: 0.4459288178838117\n",
      "\ttrain loss: 0.29641203985261905\n",
      "\ttrain loss: 0.37731619255978044\n",
      "\ttrain loss: 0.5067554323700106\n",
      "\ttrain loss: 0.5604305843532842\n",
      "\ttrain loss: 0.4959963684916302\n",
      "\ttrain loss: 0.453660464335764\n",
      "\ttrain loss: 0.36688126503156643\n",
      "\ttrain loss: 0.24622839734565422\n",
      "\ttrain loss: 0.38631359039414154\n",
      "\ttrain loss: 0.48246466028185975\n",
      "\ttrain loss: 0.2857041408291626\n",
      "\ttrain loss: 0.48449347934002207\n",
      "\ttrain loss: 0.4643170123900805\n",
      "\ttrain loss: 0.421580486754128\n",
      "\ttrain loss: 0.5699680725600579\n",
      "\ttrain loss: 0.5607341127482472\n",
      "\ttrain loss: 0.4284689834132737\n",
      "\ttrain loss: 0.36996847114808684\n",
      "\ttrain loss: 0.546860222017899\n",
      "\ttrain loss: 0.5132577308149401\n",
      "\ttrain loss: 0.30757256557460433\n",
      "\ttrain loss: 0.715645998094243\n",
      "\ttrain loss: 0.788797727136302\n",
      "\ttrain loss: 0.537640043396203\n",
      "\ttrain loss: 0.18875697702558256\n",
      "\ttrain loss: 0.39546677630641985\n",
      "\ttrain loss: 0.32132716803438743\n",
      "\ttrain loss: 0.3953827198615014\n",
      "\ttrain loss: 0.3786367729496595\n",
      "\ttrain loss: 0.639512127330998\n",
      "\ttrain loss: 0.32795986293932905\n",
      "\ttrain loss: 0.4594192250290676\n",
      "\ttrain loss: 0.34467350557155285\n",
      "\ttrain loss: 0.6400552243490847\n",
      "\ttrain loss: 0.5020390753890863\n",
      "\ttrain loss: 0.3043133879642288\n",
      "\ttrain loss: 0.458904725112888\n",
      "\ttrain loss: 0.48359244061444473\n",
      "\ttrain loss: 0.1716910617718183\n",
      "\ttrain loss: 0.32436214853531076\n",
      "\ttrain loss: 0.23856968582363883\n",
      "\ttrain loss: 0.6505654635780138\n",
      "\ttrain loss: 0.37469282919062163\n",
      "\ttrain loss: 0.3238775345204319\n",
      "\ttrain loss: 0.20763090910546264\n",
      "\ttrain loss: 0.3669633355456319\n",
      "\ttrain loss: 0.4298301955021136\n",
      "\ttrain loss: 0.34368841654545157\n",
      "\ttrain loss: 0.3295812932257721\n",
      "\ttrain loss: 0.2629395676223553\n",
      "\ttrain loss: 0.5462123249198022\n",
      "\ttrain loss: 0.47293444966234055\n",
      "\ttrain loss: 0.3577593634726852\n",
      "\ttrain loss: 0.33910306944835455\n",
      "\ttrain loss: 0.4424676631625093\n",
      "\ttrain loss: 0.4939968860453669\n",
      "\ttrain loss: 0.45888551080161544\n",
      "\ttrain loss: 0.4527923080924233\n",
      "\ttrain loss: 0.4994372690473018\n",
      "\ttrain loss: 0.36922989641834386\n",
      "\ttrain loss: 0.2687884071101334\n",
      "\ttrain loss: 0.27138474104782395\n",
      "\ttrain loss: 0.4547690435886854\n",
      "\ttrain loss: 0.5793888207389088\n",
      "\ttrain loss: 0.2967729102934002\n",
      "\ttrain loss: 0.46332393443257797\n",
      "\ttrain loss: 0.2598399632466002\n",
      "\ttrain loss: 0.6678163423181832\n",
      "\ttrain loss: 0.4527893184904348\n",
      "\ttrain loss: 0.3001486546753134\n",
      "\ttrain loss: 0.4920246946291532\n",
      "\ttrain loss: 0.41861290728338785\n",
      "\ttrain loss: 0.3792021881217258\n",
      "\ttrain loss: 0.3885389618651506\n",
      "\ttrain loss: 0.33107990753805183\n",
      "\ttrain loss: 0.4939269877641822\n",
      "\ttrain loss: 0.7297880881092347\n",
      "\ttrain loss: 0.26032582804446447\n",
      "\ttrain loss: 0.3857158468058456\n",
      "\ttrain loss: 0.3647142176469338\n",
      "\ttrain loss: 0.4199228745879303\n",
      "\ttrain loss: 0.32320669711196004\n",
      "\ttrain loss: 0.408585312026465\n",
      "\ttrain loss: 0.39615832115853344\n",
      "\ttrain loss: 0.3560062776902909\n",
      "\ttrain loss: 0.48396769035442155\n",
      "\ttrain loss: 0.3247223205172749\n",
      "\ttrain loss: 0.42404444043783573\n",
      "\ttrain loss: 0.3469195259335834\n",
      "\ttrain loss: 0.47824069963522303\n",
      "\ttrain loss: 0.4335140115586137\n",
      "\ttrain loss: 0.5390728631476275\n",
      "\ttrain loss: 0.35542783143806345\n",
      "\ttrain loss: 0.38286005987514393\n",
      "\ttrain loss: 0.4635160529723032\n",
      "\ttrain loss: 0.5128857336393794\n",
      "\ttrain loss: 0.30748007529802424\n",
      "\ttrain loss: 0.5278537344573164\n",
      "\ttrain loss: 0.43119868967124053\n",
      "\ttrain loss: 0.38822528616801794\n",
      "\ttrain loss: 0.5366138894901448\n",
      "\ttrain loss: 0.337074790426075\n",
      "\ttrain loss: 0.299518538420401\n",
      "\ttrain loss: 0.4233714532429563\n",
      "\ttrain loss: 0.3188780705233028\n",
      "\ttrain loss: 0.3541364009634258\n",
      "\ttrain loss: 0.48869052648906053\n",
      "\ttrain loss: 0.38556516531144425\n",
      "\ttrain loss: 0.38921121439607387\n",
      "\ttrain loss: 0.3324762293997294\n",
      "\ttrain loss: 0.38935707511225476\n",
      "\ttrain loss: 0.4043909713583902\n",
      "\ttrain loss: 0.30308304897809096\n",
      "\ttrain loss: 0.3974590699630481\n",
      "\ttrain loss: 0.5504974014235394\n",
      "\ttrain loss: 0.31877416035849715\n",
      "\ttrain loss: 0.4093568561141184\n",
      "\ttrain loss: 0.32221505362108077\n",
      "\ttrain loss: 0.3029631545552034\n",
      "\ttrain loss: 0.540624144366805\n",
      "\ttrain loss: 0.47044941975171706\n",
      "\ttrain loss: 0.4242047766173783\n",
      "\ttrain loss: 0.5041074351760095\n",
      "\ttrain loss: 0.26461474109203686\n",
      "\ttrain loss: 0.4045332137633074\n",
      "\ttrain loss: 0.3481904370607417\n",
      "\ttrain loss: 0.5194318874104245\n",
      "\ttrain loss: 0.2915491977246172\n",
      "\ttrain loss: 0.4015876072595677\n",
      "\ttrain loss: 0.46262330509737337\n",
      "\ttrain loss: 0.4324842679434812\n",
      "\ttrain loss: 0.40335232211473526\n",
      "\ttrain loss: 0.5300369926163246\n",
      "\ttrain loss: 0.2951602768045998\n",
      "\ttrain loss: 0.469126545913254\n",
      "\ttrain loss: 0.4030666775429963\n",
      "\ttrain loss: 0.4521674452769151\n",
      "\ttrain loss: 0.24722979700153705\n",
      "\ttrain loss: 0.23247180245735427\n",
      "\ttrain loss: 0.28138463309388934\n",
      "\ttrain loss: 0.5019401223392075\n",
      "\ttrain loss: 0.3689436749979742\n",
      "\ttrain loss: 0.3858137445271743\n",
      "\ttrain loss: 0.32328409741779707\n",
      "\ttrain loss: 0.5862193883927288\n",
      "\ttrain loss: 0.24910093601986427\n",
      "\ttrain loss: 0.5840234243775491\n",
      "\ttrain loss: 0.44697490243354815\n",
      "\ttrain loss: 0.5217136994620317\n",
      "\ttrain loss: 0.3928522201014661\n",
      "\ttrain loss: 0.5708882911643551\n",
      "\ttrain loss: 0.29146387369507415\n",
      "\ttrain loss: 0.48175629452765595\n",
      "\ttrain loss: 0.33057045924319217\n",
      "\ttrain loss: 0.34776978698507527\n",
      "\ttrain loss: 0.34622299510196786\n",
      "\ttrain loss: 0.37609859303169024\n",
      "\ttrain loss: 0.31854567355870733\n",
      "\ttrain loss: 0.4463401586239112\n",
      "\ttrain loss: 0.35592894402009856\n",
      "\ttrain loss: 0.3523285357087075\n",
      "\ttrain loss: 0.4430492082898262\n",
      "\ttrain loss: 0.26496694569843005\n",
      "\ttrain loss: 0.34909334418054855\n",
      "\ttrain loss: 0.4210074048715236\n",
      "\ttrain loss: 0.46617279400759803\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(11/15) is saved!\n",
      "\ttrain loss: 0.35372272916849085\n",
      "\ttrain loss: 0.5182775518324119\n",
      "\ttrain loss: 0.4081733573523623\n",
      "\ttrain loss: 0.5119405190377346\n",
      "\ttrain loss: 0.35910733495055114\n",
      "\ttrain loss: 0.3141963309726042\n",
      "\ttrain loss: 0.2569760785191282\n",
      "\ttrain loss: 0.5140778685820344\n",
      "\ttrain loss: 0.45224438015995577\n",
      "\ttrain loss: 0.2822954167972256\n",
      "\ttrain loss: 0.3754502663452304\n",
      "\ttrain loss: 0.4118124138076857\n",
      "\ttrain loss: 0.4986938707296489\n",
      "\ttrain loss: 0.5824676567034619\n",
      "\ttrain loss: 0.28145848889539277\n",
      "\ttrain loss: 0.40196554035660803\n",
      "\ttrain loss: 0.43435931173486086\n",
      "\ttrain loss: 0.3602073085346555\n",
      "\ttrain loss: 0.3607178345403088\n",
      "\ttrain loss: 0.41863375401636516\n",
      "\ttrain loss: 0.7776664275472339\n",
      "\ttrain loss: 0.28662331097384786\n",
      "\ttrain loss: 0.2642745864234323\n",
      "\ttrain loss: 0.18035583056997137\n",
      "\ttrain loss: 0.47845525183520504\n",
      "\ttrain loss: 0.4449923336609466\n",
      "\ttrain loss: 0.49347099482672674\n",
      "\ttrain loss: 0.4807716055554935\n",
      "\ttrain loss: 0.4720692956313152\n",
      "\ttrain loss: 0.4972802002125194\n",
      "\ttrain loss: 0.3401910216832075\n",
      "\ttrain loss: 0.3231354565663882\n",
      "\ttrain loss: 0.5923990390836487\n",
      "\ttrain loss: 0.337030491339243\n",
      "\ttrain loss: 0.43284601268327216\n",
      "\ttrain loss: 0.3615205525093399\n",
      "\ttrain loss: 0.55020114473155\n",
      "\ttrain loss: 0.4244028191133169\n",
      "\ttrain loss: 0.3912850727399204\n",
      "\ttrain loss: 0.38874143915518683\n",
      "\ttrain loss: 0.3585431641460877\n",
      "\ttrain loss: 0.28064852106726024\n",
      "\ttrain loss: 0.2763339913649073\n",
      "\ttrain loss: 0.528867238591971\n",
      "\ttrain loss: 0.39023215732794314\n",
      "\ttrain loss: 0.41239397153838475\n",
      "\ttrain loss: 0.29379877822052936\n",
      "\ttrain loss: 0.44787300850696476\n",
      "\ttrain loss: 0.31252679046729526\n",
      "\ttrain loss: 0.4181078518602319\n",
      "\ttrain loss: 0.5005227202853342\n",
      "\ttrain loss: 0.37964204642847643\n",
      "\ttrain loss: 0.44888238798785074\n",
      "\ttrain loss: 0.43731580284055116\n",
      "\ttrain loss: 0.46635191205703896\n",
      "\ttrain loss: 0.28401708465957953\n",
      "\ttrain loss: 0.37735588419781996\n",
      "\ttrain loss: 0.3560836160084383\n",
      "\ttrain loss: 0.47319347283571533\n",
      "\ttrain loss: 0.34568166757046515\n",
      "\ttrain loss: 0.42722970345033107\n",
      "\ttrain loss: 0.5279115474975417\n",
      "\ttrain loss: 0.3398387319474119\n",
      "\ttrain loss: 0.31185656377604476\n",
      "\ttrain loss: 0.3836579569062384\n",
      "\ttrain loss: 0.3320258310476772\n",
      "\ttrain loss: 0.4591139924694646\n",
      "\ttrain loss: 0.30445807134648495\n",
      "\ttrain loss: 0.4644328108671103\n",
      "\ttrain loss: 0.2799787390051571\n",
      "\ttrain loss: 0.3342533321410083\n",
      "\ttrain loss: 0.4018600914745587\n",
      "\ttrain loss: 0.4767216282256734\n",
      "\ttrain loss: 0.3921347615221197\n",
      "\ttrain loss: 0.38275036477387187\n",
      "\ttrain loss: 0.3337851176680643\n",
      "\ttrain loss: 0.36189389416085216\n",
      "\ttrain loss: 0.3233278717792978\n",
      "\ttrain loss: 0.32191330315147737\n",
      "\ttrain loss: 0.48263622363337455\n",
      "\ttrain loss: 0.3487467953778094\n",
      "\ttrain loss: 0.380703670402144\n",
      "\ttrain loss: 0.4401055632757017\n",
      "\ttrain loss: 0.40334731791815603\n",
      "\ttrain loss: 0.3893135671819837\n",
      "\ttrain loss: 0.45907662591972975\n",
      "\ttrain loss: 0.22065739037140117\n",
      "\ttrain loss: 0.3962502961812895\n",
      "\ttrain loss: 0.32114939445370616\n",
      "\ttrain loss: 0.5512849430329657\n",
      "\ttrain loss: 0.5032156389843769\n",
      "\ttrain loss: 0.3957268970329643\n",
      "\ttrain loss: 0.24409021895296906\n",
      "\ttrain loss: 0.4875535218477457\n",
      "\ttrain loss: 0.3929136697180663\n",
      "\ttrain loss: 0.4589210563930624\n",
      "\ttrain loss: 0.4358954960067557\n",
      "\ttrain loss: 0.3265256599160114\n",
      "\ttrain loss: 0.33923919889375825\n",
      "\ttrain loss: 0.49667723702867\n",
      "\ttrain loss: 0.3985799196755412\n",
      "\ttrain loss: 0.4855423888873702\n",
      "\ttrain loss: 0.3904168574484078\n",
      "\ttrain loss: 0.6453125854044148\n",
      "\ttrain loss: 0.3720885485955246\n",
      "\ttrain loss: 0.409421772772283\n",
      "\ttrain loss: 0.3779774979151618\n",
      "\ttrain loss: 0.3197190583036744\n",
      "\ttrain loss: 0.37864203777799066\n",
      "\ttrain loss: 0.5964127623718423\n",
      "\ttrain loss: 0.44136904646320696\n",
      "\ttrain loss: 0.41266834141655084\n",
      "\ttrain loss: 0.5820267788537116\n",
      "\ttrain loss: 0.46281548475025414\n",
      "\ttrain loss: 0.46659093422571585\n",
      "\ttrain loss: 0.41100983629282667\n",
      "\ttrain loss: 0.41673561681713034\n",
      "\ttrain loss: 0.2265509043955217\n",
      "\ttrain loss: 0.5019394990976158\n",
      "\ttrain loss: 0.39609889744352983\n",
      "\ttrain loss: 0.30693222492919753\n",
      "\ttrain loss: 0.25047202942855146\n",
      "\ttrain loss: 0.2844763055862656\n",
      "\ttrain loss: 0.4113049950755231\n",
      "\ttrain loss: 0.4305556138507985\n",
      "\ttrain loss: 0.39687796344160725\n",
      "\ttrain loss: 0.3683933201131795\n",
      "\ttrain loss: 0.7104873356819663\n",
      "\ttrain loss: 0.5998281803462833\n",
      "\ttrain loss: 0.4158610355472381\n",
      "\ttrain loss: 0.5025575309677188\n",
      "\ttrain loss: 0.5438386797055105\n",
      "\ttrain loss: 0.4692443456313092\n",
      "\ttrain loss: 0.34843369875648245\n",
      "\ttrain loss: 0.5816947812492725\n",
      "\ttrain loss: 0.33556362179578186\n",
      "\ttrain loss: 0.4775660816331932\n",
      "\ttrain loss: 0.5779219951241091\n",
      "\ttrain loss: 0.3314879955577187\n",
      "\ttrain loss: 0.6419575043540386\n",
      "\ttrain loss: 0.3447486094788983\n",
      "\ttrain loss: 0.46556088028188886\n",
      "\ttrain loss: 0.4119422248023923\n",
      "\ttrain loss: 0.32708104964175233\n",
      "\ttrain loss: 0.3000785915384948\n",
      "\ttrain loss: 0.35278251557614393\n",
      "\ttrain loss: 0.37079944641207296\n",
      "\ttrain loss: 0.38802119542971847\n",
      "\ttrain loss: 0.41344837861424877\n",
      "\ttrain loss: 0.2809926799924293\n",
      "\ttrain loss: 0.5671025069100346\n",
      "\ttrain loss: 0.351377864791928\n",
      "\ttrain loss: 0.5190777392012846\n",
      "\ttrain loss: 0.37969125663521086\n",
      "\ttrain loss: 0.4025734596005659\n",
      "\ttrain loss: 0.32026702365524984\n",
      "\ttrain loss: 0.44345800398558877\n",
      "\ttrain loss: 0.3242986402286189\n",
      "\ttrain loss: 0.32789473214205994\n",
      "\ttrain loss: 0.4747986165382347\n",
      "\ttrain loss: 0.3375666329724004\n",
      "\ttrain loss: 0.4810606832918147\n",
      "\ttrain loss: 0.4087744513413526\n",
      "\ttrain loss: 0.37210678902460903\n",
      "\ttrain loss: 0.5029205310151235\n",
      "\ttrain loss: 0.3799913205513895\n",
      "\ttrain loss: 0.4385557801684791\n",
      "\ttrain loss: 0.43840044559598706\n",
      "\ttrain loss: 0.4787152332709631\n",
      "\ttrain loss: 0.4363605694034024\n",
      "\ttrain loss: 0.32085911132145134\n",
      "\ttrain loss: 0.3622358773209038\n",
      "\ttrain loss: 0.30093977454862275\n",
      "\ttrain loss: 0.40521573879459605\n",
      "\ttrain loss: 0.35627057020939856\n",
      "\ttrain loss: 0.4401565970178289\n",
      "\ttrain loss: 0.5520966673696095\n",
      "\ttrain loss: 0.32956851843836515\n",
      "\ttrain loss: 0.41863282932257984\n",
      "\ttrain loss: 0.3507811821145239\n",
      "\ttrain loss: 0.46766068648705283\n",
      "\ttrain loss: 0.46969335954351055\n",
      "\ttrain loss: 0.4721067188000707\n",
      "\ttrain loss: 0.342740931066589\n",
      "\ttrain loss: 0.31155364787591533\n",
      "\ttrain loss: 0.48405740474334424\n",
      "\ttrain loss: 0.46100437884982565\n",
      "\ttrain loss: 0.37612217948667553\n",
      "\ttrain loss: 0.3392888623383319\n",
      "\ttrain loss: 0.46180745512993504\n",
      "\ttrain loss: 0.3585164681614012\n",
      "\ttrain loss: 0.36140885902546915\n",
      "\ttrain loss: 0.4542331695073631\n",
      "\ttrain loss: 0.42770518033685856\n",
      "\ttrain loss: 0.44252337598700486\n",
      "\ttrain loss: 0.24750592023060547\n",
      "\ttrain loss: 0.3840500648548044\n",
      "\ttrain loss: 0.4940224137798849\n",
      "\ttrain loss: 0.4788025749465941\n",
      "\ttrain loss: 0.3262920043555225\n",
      "\ttrain loss: 0.3856194502972608\n",
      "\ttrain loss: 0.40952974233943373\n",
      "\ttrain loss: 0.32054764412553455\n",
      "\ttrain loss: 0.4181691806340254\n",
      "\ttrain loss: 0.6429629348668415\n",
      "\ttrain loss: 0.4925269520216327\n",
      "\ttrain loss: 0.3525210065051282\n",
      "\ttrain loss: 0.3487304573258069\n",
      "\ttrain loss: 0.333575953490035\n",
      "\ttrain loss: 0.49491781599148305\n",
      "\ttrain loss: 0.39837904662991747\n",
      "\ttrain loss: 0.41733084317581426\n",
      "\ttrain loss: 0.4052965019941241\n",
      "\ttrain loss: 0.3940520866700856\n",
      "\ttrain loss: 0.36272188313296666\n",
      "\ttrain loss: 0.3735185854534092\n",
      "\ttrain loss: 0.3787505174294399\n",
      "\ttrain loss: 0.5791918437892133\n",
      "\ttrain loss: 0.44092148734503334\n",
      "\ttrain loss: 0.45198674967405905\n",
      "\ttrain loss: 0.4203392659418944\n",
      "\ttrain loss: 0.3755005475669619\n",
      "\ttrain loss: 0.3467410785746359\n",
      "\ttrain loss: 0.3694510847808914\n",
      "\ttrain loss: 0.5982382459499289\n",
      "\ttrain loss: 0.557046323703505\n",
      "\ttrain loss: 0.32388019281856023\n",
      "\ttrain loss: 0.3794059504969306\n",
      "\ttrain loss: 0.25327643650334825\n",
      "\ttrain loss: 0.3334679597585999\n",
      "\ttrain loss: 0.25881984286123105\n",
      "\ttrain loss: 0.4171670445818035\n",
      "\ttrain loss: 0.5005173265129321\n",
      "\ttrain loss: 0.3620925007554759\n",
      "\ttrain loss: 0.435402030148363\n",
      "\ttrain loss: 0.3598110024756204\n",
      "\ttrain loss: 0.44073226732542237\n",
      "\ttrain loss: 0.42830917332817253\n",
      "\ttrain loss: 0.5609125387441697\n",
      "\ttrain loss: 0.2830793346195292\n",
      "\ttrain loss: 0.39555540561469804\n",
      "\ttrain loss: 0.32006400351649583\n",
      "\ttrain loss: 0.5286124912087955\n",
      "\ttrain loss: 0.36606151577563323\n",
      "\ttrain loss: 0.3349458570696222\n",
      "\ttrain loss: 0.41120901716508007\n",
      "\ttrain loss: 0.39722297975363696\n",
      "\ttrain loss: 0.32112451142219695\n",
      "\ttrain loss: 0.2627527684583834\n",
      "\ttrain loss: 0.4246974401788095\n",
      "\ttrain loss: 0.6278871900823204\n",
      "\ttrain loss: 0.35342225748988304\n",
      "\ttrain loss: 0.29103495894126313\n",
      "\ttrain loss: 0.3903138641191962\n",
      "\ttrain loss: 0.3338796918900525\n",
      "\ttrain loss: 0.4212150054348124\n",
      "\ttrain loss: 0.45893532857726965\n",
      "\ttrain loss: 0.42480767717607104\n",
      "\ttrain loss: 0.35209919833296344\n",
      "\ttrain loss: 0.39363530939509067\n",
      "\ttrain loss: 0.3342630972968716\n",
      "\ttrain loss: 0.33473447293103953\n",
      "\ttrain loss: 0.32714233284995453\n",
      "\ttrain loss: 0.4288607027454659\n",
      "\ttrain loss: 0.4898696395339972\n",
      "\ttrain loss: 0.5134670169949132\n",
      "\ttrain loss: 0.3407836838497015\n",
      "\ttrain loss: 0.5068074017109441\n",
      "\ttrain loss: 0.3454417642597603\n",
      "\ttrain loss: 0.5873577650788622\n",
      "\ttrain loss: 0.36329154763807603\n",
      "\ttrain loss: 0.47658316488391833\n",
      "\ttrain loss: 0.3478779268745228\n",
      "\ttrain loss: 0.32017544568401934\n",
      "\ttrain loss: 0.3712618970940864\n",
      "\ttrain loss: 0.4297706052778114\n",
      "\ttrain loss: 0.32573698197736756\n",
      "\ttrain loss: 0.3179921892877996\n",
      "\ttrain loss: 0.5950197957181891\n",
      "\ttrain loss: 0.39646614425170923\n",
      "\ttrain loss: 0.33753080550178327\n",
      "\ttrain loss: 0.3422817544935749\n",
      "\ttrain loss: 0.2435868406868555\n",
      "\ttrain loss: 0.35009665721451466\n",
      "\ttrain loss: 0.34710372813683776\n",
      "\ttrain loss: 0.35809892863625603\n",
      "\ttrain loss: 0.3075311343333004\n",
      "\ttrain loss: 0.4599833450928714\n",
      "\ttrain loss: 0.42188149997643676\n",
      "\ttrain loss: 0.445765643249662\n",
      "\ttrain loss: 0.46804669854482517\n",
      "\ttrain loss: 0.44486667310808603\n",
      "\ttrain loss: 0.4562692864108441\n",
      "\ttrain loss: 0.4149388813305052\n",
      "\ttrain loss: 0.42796273114659944\n",
      "\ttrain loss: 0.5604802888097877\n",
      "\ttrain loss: 0.3784195488974814\n",
      "\ttrain loss: 0.3266683257194338\n",
      "\ttrain loss: 0.4415842803138966\n",
      "\ttrain loss: 0.568790438152889\n",
      "\ttrain loss: 0.4292379876257367\n",
      "\ttrain loss: 0.4312667552870995\n",
      "\ttrain loss: 0.30451302283293663\n",
      "\ttrain loss: 0.49888995499533817\n",
      "\ttrain loss: 0.23793169194265934\n",
      "\ttrain loss: 0.5510106806659876\n",
      "\ttrain loss: 0.3373902665150056\n",
      "\ttrain loss: 0.34368037025939474\n",
      "\ttrain loss: 0.4438386696007063\n",
      "\ttrain loss: 0.2874204489681592\n",
      "\ttrain loss: 0.36758971495129933\n",
      "\ttrain loss: 0.38598784090106464\n",
      "\ttrain loss: 0.22063377318110686\n",
      "\ttrain loss: 0.2631097426108368\n",
      "\ttrain loss: 0.3601148657047531\n",
      "\ttrain loss: 0.48329905300413356\n",
      "\ttrain loss: 0.6857537340044484\n",
      "\ttrain loss: 0.1766129195476842\n",
      "\ttrain loss: 0.46324465568710627\n",
      "\ttrain loss: 0.3273869197535083\n",
      "\ttrain loss: 0.44856389677367176\n",
      "\ttrain loss: 0.4003753010972928\n",
      "\ttrain loss: 0.3390264312999126\n",
      "\ttrain loss: 0.37506923132219083\n",
      "\ttrain loss: 0.271564899788119\n",
      "\ttrain loss: 0.6171619530061514\n",
      "\ttrain loss: 0.4948464628190029\n",
      "\ttrain loss: 0.3336436803391811\n",
      "\ttrain loss: 0.31857143292471296\n",
      "\ttrain loss: 0.4068530490311337\n",
      "\ttrain loss: 0.6916079458191106\n",
      "\ttrain loss: 0.27739850410542827\n",
      "\ttrain loss: 0.20810323538629766\n",
      "\ttrain loss: 0.41868528582717757\n",
      "\ttrain loss: 0.523914178277085\n",
      "\ttrain loss: 0.38887910434526557\n",
      "\ttrain loss: 0.3387123365788004\n",
      "\ttrain loss: 0.3361187121034223\n",
      "\ttrain loss: 0.49821894577443593\n",
      "\ttrain loss: 0.33165245847369607\n",
      "\ttrain loss: 0.40536119030396894\n",
      "\ttrain loss: 0.31654675429695533\n",
      "\ttrain loss: 0.4347870719075658\n",
      "\ttrain loss: 0.38245218262449077\n",
      "\ttrain loss: 0.42886339473534624\n",
      "\ttrain loss: 0.3642741375709496\n",
      "\ttrain loss: 0.5053192801326388\n",
      "\ttrain loss: 0.7170684557324871\n",
      "\ttrain loss: 0.31935326207657294\n",
      "\ttrain loss: 0.4514102441266693\n",
      "\ttrain loss: 0.3603239778579075\n",
      "\ttrain loss: 0.43781520257351947\n",
      "\ttrain loss: 0.24236189512539316\n",
      "\ttrain loss: 0.4743044589601117\n",
      "\ttrain loss: 0.44246844640308997\n",
      "\ttrain loss: 0.3465698577945332\n",
      "\ttrain loss: 0.3593331094670732\n",
      "\ttrain loss: 0.317972719894171\n",
      "\ttrain loss: 0.3577768351218118\n",
      "\ttrain loss: 0.4079882614720204\n",
      "\ttrain loss: 0.6617965182922281\n",
      "\ttrain loss: 0.46105524806020964\n",
      "\ttrain loss: 0.4689848645118473\n",
      "\ttrain loss: 0.46786310448385754\n",
      "\ttrain loss: 0.5531248582725508\n",
      "\ttrain loss: 0.4578675126053845\n",
      "\ttrain loss: 0.4997862707289107\n",
      "\ttrain loss: 0.3683398838411474\n",
      "\ttrain loss: 0.41915839458532245\n",
      "\ttrain loss: 0.4682645251582105\n",
      "\ttrain loss: 0.25993517346165124\n",
      "\ttrain loss: 0.39899065129312994\n",
      "\ttrain loss: 0.358160009276053\n",
      "\ttrain loss: 0.45347585176578287\n",
      "\ttrain loss: 0.40002516661341436\n",
      "\ttrain loss: 0.5518113607683643\n",
      "\ttrain loss: 0.4474930306973859\n",
      "\ttrain loss: 0.3005664931291598\n",
      "\ttrain loss: 0.33075526375753417\n",
      "\ttrain loss: 0.3168115453892657\n",
      "\ttrain loss: 0.3783002284797314\n",
      "\ttrain loss: 0.37946713292780204\n",
      "\ttrain loss: 0.3099072063865091\n",
      "\ttrain loss: 0.39430338586363\n",
      "\ttrain loss: 0.32945599747056287\n",
      "\ttrain loss: 0.38118355831689754\n",
      "\ttrain loss: 0.2944211816097017\n",
      "\ttrain loss: 0.4874785523925239\n",
      "\ttrain loss: 0.2895895692599502\n",
      "\ttrain loss: 0.34163212491280903\n",
      "\ttrain loss: 0.4801505590376269\n",
      "\ttrain loss: 0.41156099153373454\n",
      "\ttrain loss: 0.3766427988175446\n",
      "\ttrain loss: 0.812113898211581\n",
      "\ttrain loss: 0.44031154934267974\n",
      "\ttrain loss: 0.5007163391308442\n",
      "\ttrain loss: 0.2829725679461825\n",
      "\ttrain loss: 0.35281428227906975\n",
      "\ttrain loss: 0.39551029986898045\n",
      "\ttrain loss: 0.5454372510639375\n",
      "\ttrain loss: 0.6337950092626643\n",
      "\ttrain loss: 0.34278307734559565\n",
      "\ttrain loss: 0.28789087990286205\n",
      "\ttrain loss: 0.44472423719839144\n",
      "\ttrain loss: 0.2557054027857286\n",
      "\ttrain loss: 0.3548124233108773\n",
      "\ttrain loss: 0.32146584689493907\n",
      "\ttrain loss: 0.29501639343402686\n",
      "\ttrain loss: 0.3931320254622869\n",
      "\ttrain loss: 0.37935663259114294\n",
      "\ttrain loss: 0.45421096226611035\n",
      "\ttrain loss: 0.3451731372284622\n",
      "\ttrain loss: 0.27004468524048564\n",
      "\ttrain loss: 0.3619075266434998\n",
      "\ttrain loss: 0.2589175152741744\n",
      "\ttrain loss: 0.41557370586982567\n",
      "\ttrain loss: 0.3993794300126852\n",
      "\ttrain loss: 0.39768304316416603\n",
      "\ttrain loss: 0.36883844949247346\n",
      "\ttrain loss: 0.32822595372853647\n",
      "\ttrain loss: 0.3843726569835707\n",
      "\ttrain loss: 0.4030020703502852\n",
      "\ttrain loss: 0.30717093911023324\n",
      "\ttrain loss: 0.3237614255413962\n",
      "\ttrain loss: 0.2545546947482565\n",
      "\ttrain loss: 0.3673059555898499\n",
      "\ttrain loss: 0.37332325959432777\n",
      "\ttrain loss: 0.470702513082654\n",
      "\ttrain loss: 0.3940601220526867\n",
      "\ttrain loss: 0.3701045788188416\n",
      "\ttrain loss: 0.5084545970723007\n",
      "\ttrain loss: 0.2541420121304734\n",
      "\ttrain loss: 0.5388227119422695\n",
      "\ttrain loss: 0.5339686553107983\n",
      "\ttrain loss: 0.2952699706533677\n",
      "\ttrain loss: 0.47693115834106087\n",
      "\ttrain loss: 0.531285300823318\n",
      "\ttrain loss: 0.40815132452465874\n",
      "\ttrain loss: 0.44627010520311317\n",
      "\ttrain loss: 0.382535645267131\n",
      "\ttrain loss: 0.46555811934811503\n",
      "\ttrain loss: 0.4591366854867643\n",
      "\ttrain loss: 0.38887405717752443\n",
      "\ttrain loss: 0.3547314174634759\n",
      "\ttrain loss: 0.3650083645883805\n",
      "\ttrain loss: 0.37408978093918377\n",
      "\ttrain loss: 0.4458420942281595\n",
      "\ttrain loss: 0.41585248204319925\n",
      "\ttrain loss: 0.32965708515121883\n",
      "\ttrain loss: 0.3652693306130874\n",
      "\ttrain loss: 0.40139235338055124\n",
      "\ttrain loss: 0.3675582171995333\n",
      "\ttrain loss: 0.30826668954046643\n",
      "\ttrain loss: 0.44671939282677947\n",
      "\ttrain loss: 0.532903466074752\n",
      "\ttrain loss: 0.3891702912344025\n",
      "\ttrain loss: 0.3639505370381468\n",
      "\ttrain loss: 0.27389873278173915\n",
      "\ttrain loss: 0.3179716261248717\n",
      "\ttrain loss: 0.5281244564201164\n",
      "\ttrain loss: 0.45530822614827\n",
      "\ttrain loss: 0.5453070158709272\n",
      "\ttrain loss: 0.4939113014930431\n",
      "\ttrain loss: 0.4804948111027466\n",
      "\ttrain loss: 0.3655414117758744\n",
      "\ttrain loss: 0.4102041090800487\n",
      "\ttrain loss: 0.3537101010518725\n",
      "\ttrain loss: 0.1958601374680624\n",
      "\ttrain loss: 0.33951027097110653\n",
      "\ttrain loss: 0.4460598324349888\n",
      "\ttrain loss: 0.680158016632921\n",
      "\ttrain loss: 0.5714020546776639\n",
      "\ttrain loss: 0.5455233721871149\n",
      "\ttrain loss: 0.3675970755200728\n",
      "\ttrain loss: 0.37542233968918326\n",
      "\ttrain loss: 0.4556247711797706\n",
      "\ttrain loss: 0.3492292736898397\n",
      "\ttrain loss: 0.4301014079622897\n",
      "\ttrain loss: 0.35198286264738055\n",
      "\ttrain loss: 0.3554661710963466\n",
      "\ttrain loss: 0.7006729437522297\n",
      "\ttrain loss: 0.5660525523569012\n",
      "\ttrain loss: 0.43028546316358685\n",
      "\ttrain loss: 0.5871057564524709\n",
      "\ttrain loss: 0.4253067822346034\n",
      "\ttrain loss: 0.3129991344510055\n",
      "\ttrain loss: 0.3782469507933878\n",
      "\ttrain loss: 0.366690703871645\n",
      "\ttrain loss: 0.40015012501625696\n",
      "\ttrain loss: 0.23541046225352905\n",
      "\ttrain loss: 0.3844329449751155\n",
      "\ttrain loss: 0.4644143017723125\n",
      "\ttrain loss: 0.3858401959715534\n",
      "\ttrain loss: 0.589174873912787\n",
      "\ttrain loss: 0.3505808857272365\n",
      "\ttrain loss: 0.5156852409134698\n",
      "\ttrain loss: 0.3979031021906847\n",
      "\ttrain loss: 0.3563348520790895\n",
      "\ttrain loss: 0.31552702773529767\n",
      "\ttrain loss: 0.8862005782620123\n",
      "\ttrain loss: 0.47463422181706066\n",
      "\ttrain loss: 0.4369209575193818\n",
      "\ttrain loss: 0.40315184711047913\n",
      "\ttrain loss: 0.4672492987456428\n",
      "\ttrain loss: 0.46444271483171823\n",
      "\ttrain loss: 0.29137444137112656\n",
      "\ttrain loss: 0.382772192596906\n",
      "\ttrain loss: 0.4564865807282338\n",
      "\ttrain loss: 0.4615320729140785\n",
      "\ttrain loss: 0.28532993775538934\n",
      "\ttrain loss: 0.535052437411057\n",
      "\ttrain loss: 0.5625280378082025\n",
      "\ttrain loss: 0.4541127246825148\n",
      "\ttrain loss: 0.32126161718824175\n",
      "\ttrain loss: 0.49672913831885734\n",
      "\ttrain loss: 0.5137435490563771\n",
      "\ttrain loss: 0.4181613366324942\n",
      "\ttrain loss: 0.39785632175162783\n",
      "\ttrain loss: 0.5883535028545667\n",
      "\ttrain loss: 0.3962240968971804\n",
      "\ttrain loss: 0.38242353830563974\n",
      "\ttrain loss: 0.3890187950918472\n",
      "\ttrain loss: 0.3285220164371659\n",
      "\ttrain loss: 0.4384250678500529\n",
      "\ttrain loss: 0.4253376762031489\n",
      "\ttrain loss: 0.4733317736858915\n",
      "\ttrain loss: 0.5181918245546246\n",
      "\ttrain loss: 0.45776968003573093\n",
      "\ttrain loss: 0.48592261240954754\n",
      "\ttrain loss: 0.37007526969581817\n",
      "\ttrain loss: 0.43283611556352886\n",
      "\ttrain loss: 0.25013859363845004\n",
      "\ttrain loss: 0.4776311577908703\n",
      "\ttrain loss: 0.3391034531546324\n",
      "\ttrain loss: 0.28065273268629565\n",
      "\ttrain loss: 0.38927960341844503\n",
      "\ttrain loss: 0.38523646952888435\n",
      "\ttrain loss: 0.3526414541920099\n",
      "\ttrain loss: 0.45642375179634265\n",
      "\ttrain loss: 0.3340131156589393\n",
      "\ttrain loss: 0.38584123407329546\n",
      "\ttrain loss: 0.4932642699384383\n",
      "\ttrain loss: 0.2764134163673566\n",
      "\ttrain loss: 0.5447533049587164\n",
      "\ttrain loss: 0.23941605764176896\n",
      "\ttrain loss: 0.33470871379220807\n",
      "\ttrain loss: 0.44797494834214174\n",
      "\ttrain loss: 0.5809424855719645\n",
      "\ttrain loss: 0.4124136291159343\n",
      "\ttrain loss: 0.4142030000766307\n",
      "\ttrain loss: 0.30212804242899194\n",
      "\ttrain loss: 0.3632477504817269\n",
      "\ttrain loss: 0.6173182408323858\n",
      "\ttrain loss: 0.292052389738739\n",
      "\ttrain loss: 0.3522818291247103\n",
      "\ttrain loss: 0.376659448847876\n",
      "\ttrain loss: 0.46494941444320387\n",
      "\ttrain loss: 0.7567807418982201\n",
      "\ttrain loss: 0.5916585283518006\n",
      "\ttrain loss: 0.2610725890262249\n",
      "\ttrain loss: 0.4633061354392298\n",
      "\ttrain loss: 0.5154095861522033\n",
      "\ttrain loss: 0.3551641154248198\n",
      "\ttrain loss: 0.4014173961073387\n",
      "\ttrain loss: 0.4323084418511912\n",
      "\ttrain loss: 0.3076870096568424\n",
      "\ttrain loss: 0.4382836125551963\n",
      "\ttrain loss: 0.2813014152244641\n",
      "\ttrain loss: 0.3379751807590674\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(12/15) is saved!\n",
      "\ttrain loss: 0.33951198291800466\n",
      "\ttrain loss: 0.37491346125415104\n",
      "\ttrain loss: 0.5396297100763331\n",
      "\ttrain loss: 0.4428732063434891\n",
      "\ttrain loss: 0.3942941662866224\n",
      "\ttrain loss: 0.4747439183545489\n",
      "\ttrain loss: 0.29573649693575754\n",
      "\ttrain loss: 0.38228767004668285\n",
      "\ttrain loss: 0.41246578249841664\n",
      "\ttrain loss: 0.4255056455045678\n",
      "\ttrain loss: 0.7813441433939596\n",
      "\ttrain loss: 0.2754538058983284\n",
      "\ttrain loss: 0.39088555323084134\n",
      "\ttrain loss: 0.28805692087762547\n",
      "\ttrain loss: 0.38229322878583044\n",
      "\ttrain loss: 0.5501400052771676\n",
      "\ttrain loss: 0.35755838929572914\n",
      "\ttrain loss: 0.37177978643169773\n",
      "\ttrain loss: 0.476140856717786\n",
      "\ttrain loss: 0.32537894678214313\n",
      "\ttrain loss: 0.22698456105926085\n",
      "\ttrain loss: 0.33558563492963545\n",
      "\ttrain loss: 0.4221548299146803\n",
      "\ttrain loss: 0.48621102680230255\n",
      "\ttrain loss: 0.5330522505536843\n",
      "\ttrain loss: 0.3359335787104607\n",
      "\ttrain loss: 0.4612730914725268\n",
      "\ttrain loss: 0.16572239550124535\n",
      "\ttrain loss: 0.47873160032069406\n",
      "\ttrain loss: 0.45459813446658237\n",
      "\ttrain loss: 0.4827028778381176\n",
      "\ttrain loss: 0.40916651312762015\n",
      "\ttrain loss: 0.293503836377657\n",
      "\ttrain loss: 0.31205134286915337\n",
      "\ttrain loss: 0.3579493270540802\n",
      "\ttrain loss: 0.3199795579413268\n",
      "\ttrain loss: 0.6417467164934695\n",
      "\ttrain loss: 0.3336906153603344\n",
      "\ttrain loss: 0.5944833554100977\n",
      "\ttrain loss: 0.7210173154579078\n",
      "\ttrain loss: 0.4528364543147376\n",
      "\ttrain loss: 0.6262100034074585\n",
      "\ttrain loss: 0.3284987849317157\n",
      "\ttrain loss: 0.45876929390001003\n",
      "\ttrain loss: 0.3818077104985287\n",
      "\ttrain loss: 0.31463273739070485\n",
      "\ttrain loss: 0.8712832920845033\n",
      "\ttrain loss: 0.31356888900561797\n",
      "\ttrain loss: 0.41414194274977223\n",
      "\ttrain loss: 0.346737100422203\n",
      "\ttrain loss: 0.3304654567098086\n",
      "\ttrain loss: 0.4291748680035154\n",
      "\ttrain loss: 0.38091555381453157\n",
      "\ttrain loss: 0.7749206285079571\n",
      "\ttrain loss: 0.37342317994837426\n",
      "\ttrain loss: 0.42336380363518716\n",
      "\ttrain loss: 0.420513723014588\n",
      "\ttrain loss: 0.4507745162626523\n",
      "\ttrain loss: 0.4557652067513849\n",
      "\ttrain loss: 0.3981972928500713\n",
      "\ttrain loss: 0.2597252175786303\n",
      "\ttrain loss: 0.36145187831378633\n",
      "\ttrain loss: 0.44148760717920416\n",
      "\ttrain loss: 0.26542874071725797\n",
      "\ttrain loss: 0.3452910901139073\n",
      "\ttrain loss: 0.2919703590709647\n",
      "\ttrain loss: 0.2934928365643824\n",
      "\ttrain loss: 0.5053995263248185\n",
      "\ttrain loss: 0.32315529578523006\n",
      "\ttrain loss: 0.46037977543585873\n",
      "\ttrain loss: 0.231461490313147\n",
      "\ttrain loss: 0.34715833565769144\n",
      "\ttrain loss: 0.29195316385984804\n",
      "\ttrain loss: 0.5004844840707671\n",
      "\ttrain loss: 0.17674675482380203\n",
      "\ttrain loss: 0.40592424184971\n",
      "\ttrain loss: 0.5095499128554488\n",
      "\ttrain loss: 0.3917079762012997\n",
      "\ttrain loss: 0.2731815695596801\n",
      "\ttrain loss: 0.4999613133659364\n",
      "\ttrain loss: 0.3748646327399261\n",
      "\ttrain loss: 0.589653582639888\n",
      "\ttrain loss: 0.5300092494610913\n",
      "\ttrain loss: 0.2889508105658506\n",
      "\ttrain loss: 0.4558435631015274\n",
      "\ttrain loss: 0.3031100670771877\n",
      "\ttrain loss: 0.2599424034835479\n",
      "\ttrain loss: 0.27761236204029727\n",
      "\ttrain loss: 0.44482699032106066\n",
      "\ttrain loss: 0.3867525164886617\n",
      "\ttrain loss: 0.28486628634375\n",
      "\ttrain loss: 0.5356767687957174\n",
      "\ttrain loss: 0.4614340873100687\n",
      "\ttrain loss: 0.4303339712611105\n",
      "\ttrain loss: 0.617814739497931\n",
      "\ttrain loss: 0.34941104619986274\n",
      "\ttrain loss: 0.21494838829597926\n",
      "\ttrain loss: 0.3230709067829518\n",
      "\ttrain loss: 0.36097586372640617\n",
      "\ttrain loss: 0.5677126542959042\n",
      "\ttrain loss: 0.42305993448221346\n",
      "\ttrain loss: 0.43775804766787535\n",
      "\ttrain loss: 0.2864582975383417\n",
      "\ttrain loss: 0.37883337046413745\n",
      "\ttrain loss: 0.3857320735497651\n",
      "\ttrain loss: 0.3707182236435483\n",
      "\ttrain loss: 0.46041942034027605\n",
      "\ttrain loss: 0.4294235872854734\n",
      "\ttrain loss: 0.5051574698143767\n",
      "\ttrain loss: 0.5351878020760645\n",
      "\ttrain loss: 0.2979683684070523\n",
      "\ttrain loss: 0.763826948286315\n",
      "\ttrain loss: 0.3248323353796623\n",
      "\ttrain loss: 0.5454934815149448\n",
      "\ttrain loss: 0.3572748789011643\n",
      "\ttrain loss: 0.3778786792488232\n",
      "\ttrain loss: 0.33870268589690844\n",
      "\ttrain loss: 0.5429602231696464\n",
      "\ttrain loss: 0.45218546851454455\n",
      "\ttrain loss: 0.305345154247461\n",
      "\ttrain loss: 0.4164891789221582\n",
      "\ttrain loss: 0.315127541867153\n",
      "\ttrain loss: 0.30141782169088815\n",
      "\ttrain loss: 0.3383503410780659\n",
      "\ttrain loss: 0.32794778920115575\n",
      "\ttrain loss: 0.5016583440738375\n",
      "\ttrain loss: 0.29011300614771074\n",
      "\ttrain loss: 0.2773045424659473\n",
      "\ttrain loss: 0.4051463230580213\n",
      "\ttrain loss: 0.41718932883735454\n",
      "\ttrain loss: 0.216560020224372\n",
      "\ttrain loss: 0.36995319575953783\n",
      "\ttrain loss: 0.27485037502045684\n",
      "\ttrain loss: 0.39409474622060775\n",
      "\ttrain loss: 0.40867686495006367\n",
      "\ttrain loss: 0.3391889286649684\n",
      "\ttrain loss: 0.48053298353397667\n",
      "\ttrain loss: 0.423864230208337\n",
      "\ttrain loss: 0.217677394067177\n",
      "\ttrain loss: 0.28337850050250213\n",
      "\ttrain loss: 0.2806620225387557\n",
      "\ttrain loss: 0.4108978152126759\n",
      "\ttrain loss: 0.35585602927670595\n",
      "\ttrain loss: 0.6019718106385192\n",
      "\ttrain loss: 0.3919732388611769\n",
      "\ttrain loss: 0.23955459364546938\n",
      "\ttrain loss: 0.3121726826934484\n",
      "\ttrain loss: 0.4186040442785487\n",
      "\ttrain loss: 0.3118595148546126\n",
      "\ttrain loss: 0.3481203096456671\n",
      "\ttrain loss: 0.5998171339063886\n",
      "\ttrain loss: 0.4374788588845698\n",
      "\ttrain loss: 0.29299452626251454\n",
      "\ttrain loss: 0.5043021469709655\n",
      "\ttrain loss: 0.25162173167645974\n",
      "\ttrain loss: 0.41869938581980504\n",
      "\ttrain loss: 0.4350650795087333\n",
      "\ttrain loss: 0.37454488724513146\n",
      "\ttrain loss: 0.464637329483603\n",
      "\ttrain loss: 0.27951300756568814\n",
      "\ttrain loss: 0.4824640116382842\n",
      "\ttrain loss: 0.6787939818626283\n",
      "\ttrain loss: 0.3591761678749603\n",
      "\ttrain loss: 0.2839145770914835\n",
      "\ttrain loss: 0.3360725650173563\n",
      "\ttrain loss: 0.5211390776579267\n",
      "\ttrain loss: 0.3072428980598416\n",
      "\ttrain loss: 0.38568229716349534\n",
      "\ttrain loss: 0.4191587022445943\n",
      "\ttrain loss: 0.2573742245191733\n",
      "\ttrain loss: 0.3570174479032383\n",
      "\ttrain loss: 0.5125963381398053\n",
      "\ttrain loss: 0.28350955385884613\n",
      "\ttrain loss: 0.36284839028115495\n",
      "\ttrain loss: 0.45087654648976316\n",
      "\ttrain loss: 0.5191444369450189\n",
      "\ttrain loss: 0.38991271765899205\n",
      "\ttrain loss: 0.3103840283700263\n",
      "\ttrain loss: 0.23559869826735277\n",
      "\ttrain loss: 0.3845976481646084\n",
      "\ttrain loss: 0.40501608639377606\n",
      "\ttrain loss: 0.35857158973828107\n",
      "\ttrain loss: 0.48759884918784446\n",
      "\ttrain loss: 0.3582822686535472\n",
      "\ttrain loss: 0.43678161511397323\n",
      "\ttrain loss: 0.3932101942168221\n",
      "\ttrain loss: 0.4170949107724476\n",
      "\ttrain loss: 0.4334539439609315\n",
      "\ttrain loss: 0.3047693634308627\n",
      "\ttrain loss: 0.42837712794175453\n",
      "\ttrain loss: 0.37830305505454753\n",
      "\ttrain loss: 0.3553862485460354\n",
      "\ttrain loss: 0.3084805930627192\n",
      "\ttrain loss: 0.47470385647524604\n",
      "\ttrain loss: 0.46630124451691546\n",
      "\ttrain loss: 0.4860520425748374\n",
      "\ttrain loss: 0.45419585950236036\n",
      "\ttrain loss: 0.40129690381474115\n",
      "\ttrain loss: 0.36213919423666946\n",
      "\ttrain loss: 0.30577154075716234\n",
      "\ttrain loss: 0.47402328851780023\n",
      "\ttrain loss: 0.1945540009927161\n",
      "\ttrain loss: 0.3791307710453621\n",
      "\ttrain loss: 0.3868092052419795\n",
      "\ttrain loss: 0.37965862442493936\n",
      "\ttrain loss: 0.3696516125506684\n",
      "\ttrain loss: 0.29753364973322305\n",
      "\ttrain loss: 0.26486553586301065\n",
      "\ttrain loss: 0.41554248065554134\n",
      "\ttrain loss: 0.230550316218737\n",
      "\ttrain loss: 0.4275794606361376\n",
      "\ttrain loss: 0.2906974877378063\n",
      "\ttrain loss: 0.5315190606614636\n",
      "\ttrain loss: 0.36633897030084783\n",
      "\ttrain loss: 0.655105100717333\n",
      "\ttrain loss: 0.41737122461925114\n",
      "\ttrain loss: 0.47602717693920715\n",
      "\ttrain loss: 0.24986360526152918\n",
      "\ttrain loss: 0.4319244722026885\n",
      "\ttrain loss: 0.356975845883551\n",
      "\ttrain loss: 0.36036017924983665\n",
      "\ttrain loss: 0.5403013489872948\n",
      "\ttrain loss: 0.3004906604267753\n",
      "\ttrain loss: 0.4641039038514816\n",
      "\ttrain loss: 0.42457186970939875\n",
      "\ttrain loss: 0.4424348295102954\n",
      "\ttrain loss: 0.41830784138549904\n",
      "\ttrain loss: 0.3180688329329356\n",
      "\ttrain loss: 0.3445195822288467\n",
      "\ttrain loss: 0.3602896475630889\n",
      "\ttrain loss: 0.31118514826156396\n",
      "\ttrain loss: 0.28614398018626447\n",
      "\ttrain loss: 0.2939147913583867\n",
      "\ttrain loss: 0.45111002273149436\n",
      "\ttrain loss: 0.39752682677732\n",
      "\ttrain loss: 0.29896160707825076\n",
      "\ttrain loss: 0.4039137389183193\n",
      "\ttrain loss: 0.4136649689203098\n",
      "\ttrain loss: 0.41044553580858617\n",
      "\ttrain loss: 0.3408177503755028\n",
      "\ttrain loss: 0.4049902106720683\n",
      "\ttrain loss: 0.23178576317087937\n",
      "\ttrain loss: 0.4904700748204074\n",
      "\ttrain loss: 0.5856334805550537\n",
      "\ttrain loss: 0.5818042971637989\n",
      "\ttrain loss: 0.5348860246978973\n",
      "\ttrain loss: 0.3044231305177304\n",
      "\ttrain loss: 0.4720884245197498\n",
      "\ttrain loss: 0.22388494031679732\n",
      "\ttrain loss: 0.2973514885617216\n",
      "\ttrain loss: 0.21062718316246382\n",
      "\ttrain loss: 0.34816267673015866\n",
      "\ttrain loss: 0.5314910467033069\n",
      "\ttrain loss: 0.49384552569555035\n",
      "\ttrain loss: 0.37677949506734465\n",
      "\ttrain loss: 0.43309935309706016\n",
      "\ttrain loss: 0.4096117491252763\n",
      "\ttrain loss: 0.41183242062394787\n",
      "\ttrain loss: 0.3333834043633651\n",
      "\ttrain loss: 0.1906021094397709\n",
      "\ttrain loss: 0.41459859651870734\n",
      "\ttrain loss: 0.4903462912215703\n",
      "\ttrain loss: 0.315143650954426\n",
      "\ttrain loss: 0.18670158381597418\n",
      "\ttrain loss: 0.3905529914914717\n",
      "\ttrain loss: 0.3191525068866356\n",
      "\ttrain loss: 0.3470257422769672\n",
      "\ttrain loss: 0.376815097071253\n",
      "\ttrain loss: 0.5465928820438987\n",
      "\ttrain loss: 0.3709341281626476\n",
      "\ttrain loss: 0.43595096437414205\n",
      "\ttrain loss: 0.4512575102608225\n",
      "\ttrain loss: 0.46940737424604584\n",
      "\ttrain loss: 0.6982503996570418\n",
      "\ttrain loss: 0.4057390745723009\n",
      "\ttrain loss: 0.5903936977381189\n",
      "\ttrain loss: 0.2815340751404584\n",
      "\ttrain loss: 0.48597246086822166\n",
      "\ttrain loss: 0.4332998126998206\n",
      "\ttrain loss: 0.6447476858266603\n",
      "\ttrain loss: 0.42539179863674687\n",
      "\ttrain loss: 0.3528516281548502\n",
      "\ttrain loss: 0.5688361363838879\n",
      "\ttrain loss: 0.4507059837074717\n",
      "\ttrain loss: 0.23208979169219598\n",
      "\ttrain loss: 0.3958366993945863\n",
      "\ttrain loss: 0.38078052310794563\n",
      "\ttrain loss: 0.3192227697155679\n",
      "\ttrain loss: 0.46765702468512277\n",
      "\ttrain loss: 0.37143185170271553\n",
      "\ttrain loss: 0.656376471015288\n",
      "\ttrain loss: 0.5455325396297956\n",
      "\ttrain loss: 0.5634099878754353\n",
      "\ttrain loss: 0.3874708703362709\n",
      "\ttrain loss: 0.48312717457982235\n",
      "\ttrain loss: 0.28675911426457434\n",
      "\ttrain loss: 0.4126841269870037\n",
      "\ttrain loss: 0.22228098116222494\n",
      "\ttrain loss: 0.22601545016862365\n",
      "\ttrain loss: 0.484841761302378\n",
      "\ttrain loss: 0.44011948401332535\n",
      "\ttrain loss: 0.33863593280477466\n",
      "\ttrain loss: 0.4471355748527611\n",
      "\ttrain loss: 0.43439411312363635\n",
      "\ttrain loss: 0.42668528677881284\n",
      "\ttrain loss: 0.3005953393479286\n",
      "\ttrain loss: 0.4206550950627239\n",
      "\ttrain loss: 0.47157483698930813\n",
      "\ttrain loss: 0.3743678271766294\n",
      "\ttrain loss: 0.384305430249898\n",
      "\ttrain loss: 0.40936891933178005\n",
      "\ttrain loss: 0.33759843279253765\n",
      "\ttrain loss: 0.26043433030502494\n",
      "\ttrain loss: 0.6091341971534046\n",
      "\ttrain loss: 0.7033252051233826\n",
      "\ttrain loss: 0.3881645794573\n",
      "\ttrain loss: 0.39984412053953355\n",
      "\ttrain loss: 0.35284578982279025\n",
      "\ttrain loss: 0.7242302686943656\n",
      "\ttrain loss: 0.39666460945719717\n",
      "\ttrain loss: 0.4423060920226091\n",
      "\ttrain loss: 0.36700248139074687\n",
      "\ttrain loss: 0.30427072018903367\n",
      "\ttrain loss: 0.5285617227250293\n",
      "\ttrain loss: 0.5137698537256324\n",
      "\ttrain loss: 0.342407280136823\n",
      "\ttrain loss: 0.30439361330823667\n",
      "\ttrain loss: 0.4514757607890344\n",
      "\ttrain loss: 0.46681666211423495\n",
      "\ttrain loss: 0.5084471024576309\n",
      "\ttrain loss: 0.6117400995844687\n",
      "\ttrain loss: 0.3270359466622914\n",
      "\ttrain loss: 0.3381801905079348\n",
      "\ttrain loss: 0.4390096860579453\n",
      "\ttrain loss: 0.447920607694299\n",
      "\ttrain loss: 0.5863712712832672\n",
      "\ttrain loss: 0.34988207850542774\n",
      "\ttrain loss: 0.3240849803879393\n",
      "\ttrain loss: 0.31531342384963046\n",
      "\ttrain loss: 0.6587919061853265\n",
      "\ttrain loss: 0.5406244255034394\n",
      "\ttrain loss: 0.3699755135579378\n",
      "\ttrain loss: 0.4202861828215491\n",
      "\ttrain loss: 0.20083424156999358\n",
      "\ttrain loss: 0.510629449466518\n",
      "\ttrain loss: 0.5069430256173705\n",
      "\ttrain loss: 0.40137605241466323\n",
      "\ttrain loss: 0.41314858128962156\n",
      "\ttrain loss: 0.3887280537795098\n",
      "\ttrain loss: 0.3132509204712547\n",
      "\ttrain loss: 0.5350413399577716\n",
      "\ttrain loss: 0.3319060285283151\n",
      "\ttrain loss: 0.6164637160216875\n",
      "\ttrain loss: 0.33509132107258766\n",
      "\ttrain loss: 0.40458961918817804\n",
      "\ttrain loss: 0.38968164675865036\n",
      "\ttrain loss: 0.35384412352833156\n",
      "\ttrain loss: 0.37991961593309925\n",
      "\ttrain loss: 0.31591752766244974\n",
      "\ttrain loss: 0.4018610060936272\n",
      "\ttrain loss: 0.5069862305686622\n",
      "\ttrain loss: 0.5033691982857669\n",
      "\ttrain loss: 0.3475254763142899\n",
      "\ttrain loss: 0.2722990135743938\n",
      "\ttrain loss: 0.47701219546682005\n",
      "\ttrain loss: 0.4498086805860901\n",
      "\ttrain loss: 0.34794750436805016\n",
      "\ttrain loss: 0.4177620992242574\n",
      "\ttrain loss: 0.2898809259674912\n",
      "\ttrain loss: 0.4006882873966638\n",
      "\ttrain loss: 0.5251124949614222\n",
      "\ttrain loss: 0.38102696468576724\n",
      "\ttrain loss: 0.3892611803630254\n",
      "\ttrain loss: 0.3773407385799575\n",
      "\ttrain loss: 0.5795844861527716\n",
      "\ttrain loss: 0.3717854548284698\n",
      "\ttrain loss: 0.3492969985087278\n",
      "\ttrain loss: 0.31384569470914075\n",
      "\ttrain loss: 0.4328600970456862\n",
      "\ttrain loss: 0.3035040174239633\n",
      "\ttrain loss: 0.44903670419073566\n",
      "\ttrain loss: 0.43669017248638814\n",
      "\ttrain loss: 0.6349412243549628\n",
      "\ttrain loss: 0.35493384508664283\n",
      "\ttrain loss: 0.2899084874459382\n",
      "\ttrain loss: 0.4621345232346353\n",
      "\ttrain loss: 0.4062575176457589\n",
      "\ttrain loss: 0.49257229203130515\n",
      "\ttrain loss: 0.5980753751332304\n",
      "\ttrain loss: 0.36234995706521794\n",
      "\ttrain loss: 0.3741669806002379\n",
      "\ttrain loss: 0.6170734225336212\n",
      "\ttrain loss: 0.425638448003951\n",
      "\ttrain loss: 0.4600001989729352\n",
      "\ttrain loss: 0.3603771827334877\n",
      "\ttrain loss: 0.47583424019317383\n",
      "\ttrain loss: 0.2343771043982698\n",
      "\ttrain loss: 0.40411716044617374\n",
      "\ttrain loss: 0.3841549770774194\n",
      "\ttrain loss: 0.4075363273278055\n",
      "\ttrain loss: 0.3239489948772002\n",
      "\ttrain loss: 0.30750743012286663\n",
      "\ttrain loss: 0.338103015168968\n",
      "\ttrain loss: 0.3426348694857513\n",
      "\ttrain loss: 0.36065783944525215\n",
      "\ttrain loss: 0.4631408527060862\n",
      "\ttrain loss: 0.4970396367466542\n",
      "\ttrain loss: 0.32906133404006244\n",
      "\ttrain loss: 0.3954797945106202\n",
      "\ttrain loss: 0.30352706500813775\n",
      "\ttrain loss: 0.44688960204014727\n",
      "\ttrain loss: 0.3270737966658348\n",
      "\ttrain loss: 0.31898628921619665\n",
      "\ttrain loss: 0.2633991395290823\n",
      "\ttrain loss: 0.355256127013551\n",
      "\ttrain loss: 0.5642048962688777\n",
      "\ttrain loss: 0.3449387546816677\n",
      "\ttrain loss: 0.42263224658493004\n",
      "\ttrain loss: 0.30625976133140626\n",
      "\ttrain loss: 0.49918339580235344\n",
      "\ttrain loss: 0.5578796176422962\n",
      "\ttrain loss: 0.46332586684517685\n",
      "\ttrain loss: 0.5252265586744752\n",
      "\ttrain loss: 0.23472753902823373\n",
      "\ttrain loss: 0.3076275418142488\n",
      "\ttrain loss: 0.343878962399201\n",
      "\ttrain loss: 0.4250953345910893\n",
      "\ttrain loss: 0.31829278785033466\n",
      "\ttrain loss: 0.3940117392094607\n",
      "\ttrain loss: 0.5116059389559171\n",
      "\ttrain loss: 0.3267483936623843\n",
      "\ttrain loss: 0.24106328825629803\n",
      "\ttrain loss: 0.3170252493776595\n",
      "\ttrain loss: 0.5363558840536945\n",
      "\ttrain loss: 0.43831199507484164\n",
      "\ttrain loss: 0.4126080450669126\n",
      "\ttrain loss: 0.3777799935293292\n",
      "\ttrain loss: 0.44798987936112206\n",
      "\ttrain loss: 0.40957654837095203\n",
      "\ttrain loss: 0.4719233696880783\n",
      "\ttrain loss: 0.5326600396667103\n",
      "\ttrain loss: 0.37604504489218576\n",
      "\ttrain loss: 0.34338810044308166\n",
      "\ttrain loss: 0.3597153843709566\n",
      "\ttrain loss: 0.3216911090773561\n",
      "\ttrain loss: 0.5366537075089762\n",
      "\ttrain loss: 0.33828071824393746\n",
      "\ttrain loss: 0.45131946351853036\n",
      "\ttrain loss: 0.3821531063259271\n",
      "\ttrain loss: 0.5974803024653954\n",
      "\ttrain loss: 0.3255017617994961\n",
      "\ttrain loss: 0.3635520650683603\n",
      "\ttrain loss: 0.29314776017734323\n",
      "\ttrain loss: 0.4223860529371035\n",
      "\ttrain loss: 0.2995820465221718\n",
      "\ttrain loss: 0.3749377418670304\n",
      "\ttrain loss: 0.6510577623185285\n",
      "\ttrain loss: 0.40184757563298734\n",
      "\ttrain loss: 0.434977206051174\n",
      "\ttrain loss: 0.3521072502245811\n",
      "\ttrain loss: 0.4896412692768848\n",
      "\ttrain loss: 0.5641710210271681\n",
      "\ttrain loss: 0.4685949040801655\n",
      "\ttrain loss: 0.2919714858275078\n",
      "\ttrain loss: 0.5248289823556775\n",
      "\ttrain loss: 0.27522172675864154\n",
      "\ttrain loss: 0.3307250365425367\n",
      "\ttrain loss: 0.32342909427845246\n",
      "\ttrain loss: 0.5532519622195227\n",
      "\ttrain loss: 0.48817766619459024\n",
      "\ttrain loss: 0.6617884901513038\n",
      "\ttrain loss: 0.45007529441875016\n",
      "\ttrain loss: 0.34320042680760066\n",
      "\ttrain loss: 0.5205401180162255\n",
      "\ttrain loss: 0.3410355027500187\n",
      "\ttrain loss: 0.21927849474332645\n",
      "\ttrain loss: 0.526018084393769\n",
      "\ttrain loss: 0.31777157837197356\n",
      "\ttrain loss: 0.4967342082751468\n",
      "\ttrain loss: 0.360483268272179\n",
      "\ttrain loss: 0.5035551128509049\n",
      "\ttrain loss: 0.5691141225379001\n",
      "\ttrain loss: 0.28001540296516675\n",
      "\ttrain loss: 0.48903916169355166\n",
      "\ttrain loss: 0.36265102315454756\n",
      "\ttrain loss: 0.3838466007942355\n",
      "\ttrain loss: 0.3437916040817749\n",
      "\ttrain loss: 0.3516958313155276\n",
      "\ttrain loss: 0.32299476384057557\n",
      "\ttrain loss: 0.19270863486770246\n",
      "\ttrain loss: 0.2873580544843743\n",
      "\ttrain loss: 0.29322455188490515\n",
      "\ttrain loss: 0.4465747495648627\n",
      "\ttrain loss: 0.295380759448602\n",
      "\ttrain loss: 0.37703959930672637\n",
      "\ttrain loss: 0.617444962972985\n",
      "\ttrain loss: 0.5767066849834142\n",
      "\ttrain loss: 0.3390081750233756\n",
      "\ttrain loss: 0.24416212803309745\n",
      "\ttrain loss: 0.3028640328306898\n",
      "\ttrain loss: 0.32540765541671707\n",
      "\ttrain loss: 0.43107768245554784\n",
      "\ttrain loss: 0.41085955300660115\n",
      "\ttrain loss: 0.5475530270348151\n",
      "\ttrain loss: 0.29234684373556563\n",
      "\ttrain loss: 0.3872607896642228\n",
      "\ttrain loss: 0.3957322133588192\n",
      "\ttrain loss: 0.39959132635006717\n",
      "\ttrain loss: 0.35491610290626785\n",
      "\ttrain loss: 0.40041706277919015\n",
      "\ttrain loss: 0.4002173061368598\n",
      "\ttrain loss: 0.4525871185683417\n",
      "\ttrain loss: 0.37958328399871255\n",
      "\ttrain loss: 0.3922727179466581\n",
      "\ttrain loss: 0.35446943001000686\n",
      "\ttrain loss: 0.40008784179613455\n",
      "\ttrain loss: 0.3508024701697273\n",
      "\ttrain loss: 0.44400340138343064\n",
      "\ttrain loss: 0.6157811379899132\n",
      "\ttrain loss: 0.30690659203166115\n",
      "\ttrain loss: 0.398395059395775\n",
      "\ttrain loss: 0.31612118020563873\n",
      "\ttrain loss: 0.4627428342175905\n",
      "\ttrain loss: 0.40807397928131117\n",
      "\ttrain loss: 0.3959048642942565\n",
      "\ttrain loss: 0.5101679084993156\n",
      "\ttrain loss: 0.4640554028308597\n",
      "\ttrain loss: 0.33445959501968126\n",
      "\ttrain loss: 0.40289427640405445\n",
      "\ttrain loss: 0.5339232101838381\n",
      "\ttrain loss: 0.35632098159899583\n",
      "\ttrain loss: 0.5126271015633143\n",
      "\ttrain loss: 0.604548423098652\n",
      "\ttrain loss: 0.39139155283617894\n",
      "\ttrain loss: 0.30373660248146805\n",
      "\ttrain loss: 0.41785604684340294\n",
      "\ttrain loss: 0.37118079930928183\n",
      "\ttrain loss: 0.46256155176496394\n",
      "\ttrain loss: 0.4098359582749188\n",
      "\ttrain loss: 0.41217999844195324\n",
      "\ttrain loss: 0.45458260589129024\n",
      "\ttrain loss: 0.4078081893663075\n",
      "\ttrain loss: 0.580446021503897\n",
      "\ttrain loss: 0.2877465137336772\n",
      "\ttrain loss: 0.32323308824248853\n",
      "\ttrain loss: 0.431508094145192\n",
      "\ttrain loss: 0.3278928892084164\n",
      "\ttrain loss: 0.4952504814808955\n",
      "\ttrain loss: 0.35336224268665384\n",
      "\ttrain loss: 0.2765159912830717\n",
      "\ttrain loss: 0.31978609155464743\n",
      "\ttrain loss: 0.45316887912232684\n",
      "\ttrain loss: 0.4652750532129318\n",
      "\ttrain loss: 0.3376768265586577\n",
      "\ttrain loss: 0.39239626249170967\n",
      "\ttrain loss: 0.40601556994406585\n",
      "\ttrain loss: 0.35873437481973475\n",
      "\ttrain loss: 0.3783770734775756\n",
      "\ttrain loss: 0.5111014438047119\n",
      "\ttrain loss: 0.47808197747628256\n",
      "\ttrain loss: 0.30745155878052133\n",
      "\ttrain loss: 0.3775926327973178\n",
      "\ttrain loss: 0.5226610812244761\n",
      "\ttrain loss: 0.443571464762642\n",
      "\ttrain loss: 0.5128230859229139\n",
      "\ttrain loss: 0.4327633451054722\n",
      "\ttrain loss: 0.4402497359961316\n",
      "\ttrain loss: 0.4026059569229731\n",
      "\ttrain loss: 0.6539951620675344\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(13/15) is saved!\n",
      "\ttrain loss: 0.4308628294821727\n",
      "\ttrain loss: 0.4250143851420251\n",
      "\ttrain loss: 0.3664342918435079\n",
      "\ttrain loss: 0.4008352229269976\n",
      "\ttrain loss: 0.3863731106006489\n",
      "\ttrain loss: 0.5084346965599187\n",
      "\ttrain loss: 0.3664799800415387\n",
      "\ttrain loss: 0.49423889041771807\n",
      "\ttrain loss: 0.4042244983218729\n",
      "\ttrain loss: 0.36471459796271755\n",
      "\ttrain loss: 0.25639065716459863\n",
      "\ttrain loss: 0.2642350766240058\n",
      "\ttrain loss: 0.47537330594534916\n",
      "\ttrain loss: 0.40047673237680603\n",
      "\ttrain loss: 0.3231502642162164\n",
      "\ttrain loss: 0.32408116103952656\n",
      "\ttrain loss: 0.39724462109866326\n",
      "\ttrain loss: 0.28420020202519414\n",
      "\ttrain loss: 0.2723815862402836\n",
      "\ttrain loss: 0.4730418975285511\n",
      "\ttrain loss: 0.31027331685405923\n",
      "\ttrain loss: 0.3517491076379766\n",
      "\ttrain loss: 0.3111316218854767\n",
      "\ttrain loss: 0.4745649474598802\n",
      "\ttrain loss: 0.6583833493939613\n",
      "\ttrain loss: 0.36042162544291817\n",
      "\ttrain loss: 0.503452649312084\n",
      "\ttrain loss: 0.3762826464130852\n",
      "\ttrain loss: 0.3725803442292242\n",
      "\ttrain loss: 0.5021296064031939\n",
      "\ttrain loss: 0.3412592160827106\n",
      "\ttrain loss: 0.4558816161203041\n",
      "\ttrain loss: 0.2764354535159966\n",
      "\ttrain loss: 0.6706475707776787\n",
      "\ttrain loss: 0.3984148411024421\n",
      "\ttrain loss: 0.5181724734018586\n",
      "\ttrain loss: 0.31424232402108865\n",
      "\ttrain loss: 0.39819779344572553\n",
      "\ttrain loss: 0.3711564358522614\n",
      "\ttrain loss: 0.3396017993618334\n",
      "\ttrain loss: 0.7747289619827158\n",
      "\ttrain loss: 0.39402200586906827\n",
      "\ttrain loss: 0.42135887846444176\n",
      "\ttrain loss: 0.44232741073468784\n",
      "\ttrain loss: 0.47398190735156853\n",
      "\ttrain loss: 0.46564988875975377\n",
      "\ttrain loss: 0.4456938796421775\n",
      "\ttrain loss: 0.38111179235418724\n",
      "\ttrain loss: 0.3139052332613035\n",
      "\ttrain loss: 0.40527333363028983\n",
      "\ttrain loss: 0.48847437492856016\n",
      "\ttrain loss: 0.39144198323283214\n",
      "\ttrain loss: 0.6854617038660824\n",
      "\ttrain loss: 0.31282397317917976\n",
      "\ttrain loss: 0.48033478774303034\n",
      "\ttrain loss: 0.3470553384108548\n",
      "\ttrain loss: 0.3498764277312927\n",
      "\ttrain loss: 0.39661468081979395\n",
      "\ttrain loss: 0.23998447107875864\n",
      "\ttrain loss: 0.455818433386483\n",
      "\ttrain loss: 0.40940162426434673\n",
      "\ttrain loss: 0.4237947778550516\n",
      "\ttrain loss: 0.423229984074061\n",
      "\ttrain loss: 0.41693047681161743\n",
      "\ttrain loss: 0.29790836391888853\n",
      "\ttrain loss: 0.414741737189346\n",
      "\ttrain loss: 0.41834770203746996\n",
      "\ttrain loss: 0.4107418486411747\n",
      "\ttrain loss: 0.528071468599281\n",
      "\ttrain loss: 0.2705628970507942\n",
      "\ttrain loss: 0.4786156465149153\n",
      "\ttrain loss: 0.4336209942890461\n",
      "\ttrain loss: 0.5528826556266254\n",
      "\ttrain loss: 0.4840415260266864\n",
      "\ttrain loss: 0.2516956888647005\n",
      "\ttrain loss: 0.327957536631227\n",
      "\ttrain loss: 0.492520237948137\n",
      "\ttrain loss: 0.19179113114398094\n",
      "\ttrain loss: 0.3978659464011382\n",
      "\ttrain loss: 0.316383530873941\n",
      "\ttrain loss: 0.2822517600766549\n",
      "\ttrain loss: 0.45980063806880833\n",
      "\ttrain loss: 0.5144309283145669\n",
      "\ttrain loss: 0.48121086362660553\n",
      "\ttrain loss: 0.3463966294053353\n",
      "\ttrain loss: 0.4058761344906554\n",
      "\ttrain loss: 0.3808975775636315\n",
      "\ttrain loss: 0.3299654304546804\n",
      "\ttrain loss: 0.38094674909620724\n",
      "\ttrain loss: 0.5764604434310588\n",
      "\ttrain loss: 0.3408964755242781\n",
      "\ttrain loss: 0.4575723501902187\n",
      "\ttrain loss: 0.3719448544347102\n",
      "\ttrain loss: 0.3355116283699171\n",
      "\ttrain loss: 0.49823068522517433\n",
      "\ttrain loss: 0.38924874976784757\n",
      "\ttrain loss: 0.3648331373687782\n",
      "\ttrain loss: 0.4301176903876287\n",
      "\ttrain loss: 0.3894422635786826\n",
      "\ttrain loss: 0.4131854839191643\n",
      "\ttrain loss: 0.32235414674274454\n",
      "\ttrain loss: 0.4320475617928092\n",
      "\ttrain loss: 0.25407633072222063\n",
      "\ttrain loss: 0.3452000611205912\n",
      "\ttrain loss: 0.3271221218921513\n",
      "\ttrain loss: 0.36461265816328403\n",
      "\ttrain loss: 0.2992797593113408\n",
      "\ttrain loss: 0.3209491907330358\n",
      "\ttrain loss: 0.460356372909677\n",
      "\ttrain loss: 0.5193133560055369\n",
      "\ttrain loss: 0.36316981029658935\n",
      "\ttrain loss: 0.6172326188500441\n",
      "\ttrain loss: 0.41866167028874035\n",
      "\ttrain loss: 0.4480311827476593\n",
      "\ttrain loss: 0.45343988759524856\n",
      "\ttrain loss: 0.2655366987060246\n",
      "\ttrain loss: 0.3395166378335368\n",
      "\ttrain loss: 0.354690733268597\n",
      "\ttrain loss: 0.33391641186382837\n",
      "\ttrain loss: 0.46037153287621485\n",
      "\ttrain loss: 0.3957329399298817\n",
      "\ttrain loss: 0.38436344194809674\n",
      "\ttrain loss: 0.4100639185495134\n",
      "\ttrain loss: 0.43828330129305915\n",
      "\ttrain loss: 0.36218985797336695\n",
      "\ttrain loss: 0.30228093112988036\n",
      "\ttrain loss: 0.24454548808494447\n",
      "\ttrain loss: 0.28661030977378255\n",
      "\ttrain loss: 0.3754121699511115\n",
      "\ttrain loss: 0.7538660317425375\n",
      "\ttrain loss: 0.4774410454875334\n",
      "\ttrain loss: 0.27174091067365663\n",
      "\ttrain loss: 0.3557687615608925\n",
      "\ttrain loss: 0.44816020836568304\n",
      "\ttrain loss: 0.37042712359967744\n",
      "\ttrain loss: 0.30154814277527275\n",
      "\ttrain loss: 0.3970507834052991\n",
      "\ttrain loss: 0.4572076434003378\n",
      "\ttrain loss: 0.44927137710708\n",
      "\ttrain loss: 0.3350425470755913\n",
      "\ttrain loss: 0.5053889396496\n",
      "\ttrain loss: 0.48991087711893294\n",
      "\ttrain loss: 0.31697980436781314\n",
      "\ttrain loss: 0.3161715119448332\n",
      "\ttrain loss: 0.3983513169021703\n",
      "\ttrain loss: 0.44560565775627714\n",
      "\ttrain loss: 0.267605443709531\n",
      "\ttrain loss: 0.32588987868206554\n",
      "\ttrain loss: 0.18853567103750474\n",
      "\ttrain loss: 0.4145076983957788\n",
      "\ttrain loss: 0.5547043821403405\n",
      "\ttrain loss: 0.42414841931584246\n",
      "\ttrain loss: 0.42599964597048046\n",
      "\ttrain loss: 0.2603001424274963\n",
      "\ttrain loss: 0.33853046067015846\n",
      "\ttrain loss: 0.36535542973495494\n",
      "\ttrain loss: 0.2673371241283917\n",
      "\ttrain loss: 0.38806975187618903\n",
      "\ttrain loss: 0.39900390386952683\n",
      "\ttrain loss: 0.2340771609995967\n",
      "\ttrain loss: 0.3851393071999673\n",
      "\ttrain loss: 0.5086700668733358\n",
      "\ttrain loss: 0.36565232865332786\n",
      "\ttrain loss: 0.49006868061503905\n",
      "\ttrain loss: 0.5447788684497894\n",
      "\ttrain loss: 0.37652759023948223\n",
      "\ttrain loss: 0.37969480138959233\n",
      "\ttrain loss: 0.32322261651369855\n",
      "\ttrain loss: 0.5838689855368246\n",
      "\ttrain loss: 0.36875457029782377\n",
      "\ttrain loss: 0.3743705753400527\n",
      "\ttrain loss: 0.4274815556410458\n",
      "\ttrain loss: 0.3663712338015501\n",
      "\ttrain loss: 0.3256011145449227\n",
      "\ttrain loss: 0.30978291398697766\n",
      "\ttrain loss: 0.45311126984419664\n",
      "\ttrain loss: 0.3955831854277853\n",
      "\ttrain loss: 0.28819764101256107\n",
      "\ttrain loss: 0.3210044853782915\n",
      "\ttrain loss: 0.5246613742842554\n",
      "\ttrain loss: 0.3799558152712589\n",
      "\ttrain loss: 0.357820818045788\n",
      "\ttrain loss: 0.4224896041391807\n",
      "\ttrain loss: 0.4819431858919667\n",
      "\ttrain loss: 0.4438795136362859\n",
      "\ttrain loss: 0.41385011682485595\n",
      "\ttrain loss: 0.363354416954555\n",
      "\ttrain loss: 0.32102124679401234\n",
      "\ttrain loss: 0.49481358321421365\n",
      "\ttrain loss: 0.5318146765439734\n",
      "\ttrain loss: 0.3428252450121373\n",
      "\ttrain loss: 0.38902113039258546\n",
      "\ttrain loss: 0.5614586736943549\n",
      "\ttrain loss: 0.39888132018354294\n",
      "\ttrain loss: 0.2934853894023695\n",
      "\ttrain loss: 0.3306003416232166\n",
      "\ttrain loss: 0.457055018568452\n",
      "\ttrain loss: 0.3996057465999807\n",
      "\ttrain loss: 0.4583700511511115\n",
      "\ttrain loss: 0.46530226980439393\n",
      "\ttrain loss: 0.35963512479395465\n",
      "\ttrain loss: 0.36276373097142894\n",
      "\ttrain loss: 0.46914482854759887\n",
      "\ttrain loss: 0.3942264482733663\n",
      "\ttrain loss: 0.5207053644434771\n",
      "\ttrain loss: 0.2814822620326454\n",
      "\ttrain loss: 0.3405113407177399\n",
      "\ttrain loss: 0.43044471965924685\n",
      "\ttrain loss: 0.35340174197314367\n",
      "\ttrain loss: 0.4811606595407262\n",
      "\ttrain loss: 0.5117274393856908\n",
      "\ttrain loss: 0.24442088569476966\n",
      "\ttrain loss: 0.24168309362264728\n",
      "\ttrain loss: 0.4054477875605791\n",
      "\ttrain loss: 0.3663071852242933\n",
      "\ttrain loss: 0.3079716969740328\n",
      "\ttrain loss: 0.4474726664533054\n",
      "\ttrain loss: 0.27776956369795847\n",
      "\ttrain loss: 0.47954306060668317\n",
      "\ttrain loss: 0.39040928205953\n",
      "\ttrain loss: 0.41760039492179435\n",
      "\ttrain loss: 0.4399626067975853\n",
      "\ttrain loss: 0.4920621556981991\n",
      "\ttrain loss: 0.48775826060806193\n",
      "\ttrain loss: 0.2668792470818768\n",
      "\ttrain loss: 0.4409074267994369\n",
      "\ttrain loss: 0.3012865721272193\n",
      "\ttrain loss: 0.34244447368546005\n",
      "\ttrain loss: 0.42797208851600965\n",
      "\ttrain loss: 0.4249425791376292\n",
      "\ttrain loss: 0.20537310517411228\n",
      "\ttrain loss: 0.4160464661606321\n",
      "\ttrain loss: 0.49478975460486685\n",
      "\ttrain loss: 0.4854449322671318\n",
      "\ttrain loss: 0.44779455834338033\n",
      "\ttrain loss: 0.3365358236836148\n",
      "\ttrain loss: 0.3506874902968724\n",
      "\ttrain loss: 0.5447331054691533\n",
      "\ttrain loss: 0.3709692597902088\n",
      "\ttrain loss: 0.4281714279455119\n",
      "\ttrain loss: 0.34518881832358633\n",
      "\ttrain loss: 0.354486294741406\n",
      "\ttrain loss: 0.33796554421580366\n",
      "\ttrain loss: 0.529342335563693\n",
      "\ttrain loss: 0.3974987906785594\n",
      "\ttrain loss: 0.4792310651402877\n",
      "\ttrain loss: 0.3584979915670967\n",
      "\ttrain loss: 0.38916771952512175\n",
      "\ttrain loss: 0.5746775603656273\n",
      "\ttrain loss: 0.3724315745513078\n",
      "\ttrain loss: 0.43565817312339195\n",
      "\ttrain loss: 0.3264222842767619\n",
      "\ttrain loss: 0.1992796098010642\n",
      "\ttrain loss: 0.540905687732641\n",
      "\ttrain loss: 0.2797091337675569\n",
      "\ttrain loss: 0.33331169976804076\n",
      "\ttrain loss: 0.254327937856746\n",
      "\ttrain loss: 0.4882590604678907\n",
      "\ttrain loss: 0.4301662840173044\n",
      "\ttrain loss: 0.4396940532315369\n",
      "\ttrain loss: 0.4356995184177997\n",
      "\ttrain loss: 0.45151746220627065\n",
      "\ttrain loss: 0.22296826338661263\n",
      "\ttrain loss: 0.4450733805004666\n",
      "\ttrain loss: 0.35935510420363204\n",
      "\ttrain loss: 0.36190364952830756\n",
      "\ttrain loss: 0.46220640436256366\n",
      "\ttrain loss: 0.32417616884487965\n",
      "\ttrain loss: 0.5005668872236053\n",
      "\ttrain loss: 0.4280852567111162\n",
      "\ttrain loss: 0.5289140551402627\n",
      "\ttrain loss: 0.3731294627596645\n",
      "\ttrain loss: 0.6233182796053682\n",
      "\ttrain loss: 0.3065846233692401\n",
      "\ttrain loss: 0.44275607662106653\n",
      "\ttrain loss: 0.36811009895181435\n",
      "\ttrain loss: 0.5341040822069043\n",
      "\ttrain loss: 0.3578308476066955\n",
      "\ttrain loss: 0.5022007952429663\n",
      "\ttrain loss: 0.5436071315056572\n",
      "\ttrain loss: 0.4884512698161738\n",
      "\ttrain loss: 0.43588501298121135\n",
      "\ttrain loss: 0.2514379505729504\n",
      "\ttrain loss: 0.35026972453290206\n",
      "\ttrain loss: 0.479743433776773\n",
      "\ttrain loss: 0.24504430124230422\n",
      "\ttrain loss: 0.469782227393888\n",
      "\ttrain loss: 0.508099464003974\n",
      "\ttrain loss: 0.4214218064011079\n",
      "\ttrain loss: 0.4236673087915192\n",
      "\ttrain loss: 0.43465232310944035\n",
      "\ttrain loss: 0.37465453912412294\n",
      "\ttrain loss: 0.40560431093771715\n",
      "\ttrain loss: 0.3020317467910685\n",
      "\ttrain loss: 0.3155475090692407\n",
      "\ttrain loss: 0.2977598865768607\n",
      "\ttrain loss: 0.4184107783616433\n",
      "\ttrain loss: 0.37010551747745024\n",
      "\ttrain loss: 0.2868714612383979\n",
      "\ttrain loss: 0.34530067072725723\n",
      "\ttrain loss: 0.4199724580644223\n",
      "\ttrain loss: 0.33446412623091404\n",
      "\ttrain loss: 0.595357631374358\n",
      "\ttrain loss: 0.2540015259173197\n",
      "\ttrain loss: 0.4855960063342346\n",
      "\ttrain loss: 0.30250068527733814\n",
      "\ttrain loss: 0.3950338914443049\n",
      "\ttrain loss: 0.3368413685353583\n",
      "\ttrain loss: 0.3818234770943518\n",
      "\ttrain loss: 0.3332601918394018\n",
      "\ttrain loss: 0.38380815522348777\n",
      "\ttrain loss: 0.4814998377404511\n",
      "\ttrain loss: 0.43176543112175103\n",
      "\ttrain loss: 0.3705005403781795\n",
      "\ttrain loss: 0.31296488979496995\n",
      "\ttrain loss: 0.37037861674361094\n",
      "\ttrain loss: 0.2912730224449175\n",
      "\ttrain loss: 0.3189968388100217\n",
      "\ttrain loss: 0.3112940076392865\n",
      "\ttrain loss: 0.3009867033359892\n",
      "\ttrain loss: 0.3863118467183434\n",
      "\ttrain loss: 0.2470984764256557\n",
      "\ttrain loss: 0.4818855979466583\n",
      "\ttrain loss: 0.5038482315341355\n",
      "\ttrain loss: 0.6329291084710977\n",
      "\ttrain loss: 0.3336017457334479\n",
      "\ttrain loss: 0.40372545802930315\n",
      "\ttrain loss: 0.37870966162640834\n",
      "\ttrain loss: 0.3136254952885557\n",
      "\ttrain loss: 0.3668056855469385\n",
      "\ttrain loss: 0.29814343419873457\n",
      "\ttrain loss: 0.39607320071523927\n",
      "\ttrain loss: 0.6398924192701402\n",
      "\ttrain loss: 0.5403153148933566\n",
      "\ttrain loss: 0.38646383858605365\n",
      "\ttrain loss: 0.554194065248323\n",
      "\ttrain loss: 0.4711904150244073\n",
      "\ttrain loss: 0.5190184138873047\n",
      "\ttrain loss: 0.4002281042259528\n",
      "\ttrain loss: 0.37687728738074044\n",
      "\ttrain loss: 0.3756257802989604\n",
      "\ttrain loss: 0.4573538447607856\n",
      "\ttrain loss: 0.39957981801522857\n",
      "\ttrain loss: 0.5822700375016374\n",
      "\ttrain loss: 0.48041313239285555\n",
      "\ttrain loss: 0.3396422128961709\n",
      "\ttrain loss: 0.4225295092647028\n",
      "\ttrain loss: 0.40606352663164524\n",
      "\ttrain loss: 0.3228925402609314\n",
      "\ttrain loss: 0.6392488466699237\n",
      "\ttrain loss: 0.3936359143526306\n",
      "\ttrain loss: 0.5346450389082873\n",
      "\ttrain loss: 0.35322635024356563\n",
      "\ttrain loss: 0.32252059766601315\n",
      "\ttrain loss: 0.2724660625435696\n",
      "\ttrain loss: 0.25796488302531373\n",
      "\ttrain loss: 0.47077615947641144\n",
      "\ttrain loss: 0.3279697721739988\n",
      "\ttrain loss: 0.5650699544031103\n",
      "\ttrain loss: 0.2934520388388963\n",
      "\ttrain loss: 0.49511889893389405\n",
      "\ttrain loss: 0.32486527552926314\n",
      "\ttrain loss: 0.2919407049548352\n",
      "\ttrain loss: 0.174964704537824\n",
      "\ttrain loss: 0.31959680586772643\n",
      "\ttrain loss: 0.4084358620617239\n",
      "\ttrain loss: 0.574783945286879\n",
      "\ttrain loss: 0.49089306995913773\n",
      "\ttrain loss: 0.433710226574669\n",
      "\ttrain loss: 0.3544255729891692\n",
      "\ttrain loss: 0.4783494895789521\n",
      "\ttrain loss: 0.42582375101830083\n",
      "\ttrain loss: 0.38754267334410947\n",
      "\ttrain loss: 0.43216006071370316\n",
      "\ttrain loss: 0.45936863493796115\n",
      "\ttrain loss: 0.24898878034834743\n",
      "\ttrain loss: 0.4102770158668709\n",
      "\ttrain loss: 0.26733555277231674\n",
      "\ttrain loss: 0.4802893507211825\n",
      "\ttrain loss: 0.39137134271340795\n",
      "\ttrain loss: 0.3897511028182944\n",
      "\ttrain loss: 0.3234716192953708\n",
      "\ttrain loss: 0.3903084404026263\n",
      "\ttrain loss: 0.4621260806610124\n",
      "\ttrain loss: 0.5707372627287286\n",
      "\ttrain loss: 0.31669760089173893\n",
      "\ttrain loss: 0.46922620196510023\n",
      "\ttrain loss: 0.4763687660295647\n",
      "\ttrain loss: 0.4795537672792718\n",
      "\ttrain loss: 0.4079191693066341\n",
      "\ttrain loss: 0.4404821442469974\n",
      "\ttrain loss: 0.35918032691948454\n",
      "\ttrain loss: 0.4488090611808265\n",
      "\ttrain loss: 0.45154236275585435\n",
      "\ttrain loss: 0.3778393654830901\n",
      "\ttrain loss: 0.5087737674608646\n",
      "\ttrain loss: 0.4077635307250265\n",
      "\ttrain loss: 0.43402484673183367\n",
      "\ttrain loss: 0.4791634801491987\n",
      "\ttrain loss: 0.4790614005156427\n",
      "\ttrain loss: 0.3584075646506934\n",
      "\ttrain loss: 0.391933665204351\n",
      "\ttrain loss: 0.35496560020250634\n",
      "\ttrain loss: 0.29806845986666586\n",
      "\ttrain loss: 0.4654447510641674\n",
      "\ttrain loss: 0.33249506284181374\n",
      "\ttrain loss: 0.5126761679811498\n",
      "\ttrain loss: 0.47978343734007667\n",
      "\ttrain loss: 0.475250903731243\n",
      "\ttrain loss: 0.3094094641810964\n",
      "\ttrain loss: 0.40405788259928166\n",
      "\ttrain loss: 0.5428653716468557\n",
      "\ttrain loss: 0.5652159884674425\n",
      "\ttrain loss: 0.26047371250596874\n",
      "\ttrain loss: 0.43726278264222873\n",
      "\ttrain loss: 0.32077856017918693\n",
      "\ttrain loss: 0.340181433798077\n",
      "\ttrain loss: 0.5444818350707445\n",
      "\ttrain loss: 0.4304700950982825\n",
      "\ttrain loss: 0.4601432706891872\n",
      "\ttrain loss: 0.2827783283670996\n",
      "\ttrain loss: 0.41629759966690316\n",
      "\ttrain loss: 0.31770998862671007\n",
      "\ttrain loss: 0.48482991227529026\n",
      "\ttrain loss: 0.4085369644281298\n",
      "\ttrain loss: 0.33315602714433734\n",
      "\ttrain loss: 0.2991320883992891\n",
      "\ttrain loss: 0.4807922939971947\n",
      "\ttrain loss: 0.4285937940939746\n",
      "\ttrain loss: 0.3699956661504177\n",
      "\ttrain loss: 0.3318614267055784\n",
      "\ttrain loss: 0.32507732846807\n",
      "\ttrain loss: 0.34669656515724273\n",
      "\ttrain loss: 0.4070862791447966\n",
      "\ttrain loss: 0.41221911833771024\n",
      "\ttrain loss: 0.2467997323564647\n",
      "\ttrain loss: 0.42258795269387983\n",
      "\ttrain loss: 0.35416549357046895\n",
      "\ttrain loss: 0.49031750948160047\n",
      "\ttrain loss: 0.35472510538574825\n",
      "\ttrain loss: 0.25899257738956083\n",
      "\ttrain loss: 0.3499218841083356\n",
      "\ttrain loss: 0.4653075876793521\n",
      "\ttrain loss: 0.43640989316725465\n",
      "\ttrain loss: 0.31164135645480817\n",
      "\ttrain loss: 0.34740800093672997\n",
      "\ttrain loss: 0.4352175041860321\n",
      "\ttrain loss: 0.2559599065730072\n",
      "\ttrain loss: 0.27424819219789115\n",
      "\ttrain loss: 0.3246199564776061\n",
      "\ttrain loss: 0.4789754374830638\n",
      "\ttrain loss: 0.3475380502412418\n",
      "\ttrain loss: 0.5135453246707906\n",
      "\ttrain loss: 0.41336678347869105\n",
      "\ttrain loss: 0.31229448366144164\n",
      "\ttrain loss: 0.36807181692022894\n",
      "\ttrain loss: 0.4629430174647628\n",
      "\ttrain loss: 0.43510887877024895\n",
      "\ttrain loss: 0.3226995009072953\n",
      "\ttrain loss: 0.6371155331742332\n",
      "\ttrain loss: 0.36073354868375324\n",
      "\ttrain loss: 0.34409240591219503\n",
      "\ttrain loss: 0.340196338103503\n",
      "\ttrain loss: 0.30209486693650905\n",
      "\ttrain loss: 0.3874388410383248\n",
      "\ttrain loss: 0.3601205371485711\n",
      "\ttrain loss: 0.4524948720908483\n",
      "\ttrain loss: 0.45794471501601386\n",
      "\ttrain loss: 0.27611612457693824\n",
      "\ttrain loss: 0.34250131664938005\n",
      "\ttrain loss: 0.46202657336729475\n",
      "\ttrain loss: 0.2972388762424643\n",
      "\ttrain loss: 0.3808328177446227\n",
      "\ttrain loss: 0.31949025626635186\n",
      "\ttrain loss: 0.29522247060349405\n",
      "\ttrain loss: 0.6148936053949041\n",
      "\ttrain loss: 0.4066498299906566\n",
      "\ttrain loss: 0.3546290540153316\n",
      "\ttrain loss: 0.3655458544162242\n",
      "\ttrain loss: 0.4964533764399224\n",
      "\ttrain loss: 0.3188524608453581\n",
      "\ttrain loss: 0.36611571163318246\n",
      "\ttrain loss: 0.46321592456935917\n",
      "\ttrain loss: 0.30123478205261955\n",
      "\ttrain loss: 0.2827495571035035\n",
      "\ttrain loss: 0.43899574565962723\n",
      "\ttrain loss: 0.44873240793946967\n",
      "\ttrain loss: 0.3433214864846371\n",
      "\ttrain loss: 0.6057030408815786\n",
      "\ttrain loss: 0.37622693050183986\n",
      "\ttrain loss: 0.41106902711728943\n",
      "\ttrain loss: 0.4378447435262657\n",
      "\ttrain loss: 0.47430112709464345\n",
      "\ttrain loss: 0.46073751739463126\n",
      "\ttrain loss: 0.4199811564096887\n",
      "\ttrain loss: 0.3748750689999121\n",
      "\ttrain loss: 0.3719523447404013\n",
      "\ttrain loss: 0.31067580107593695\n",
      "\ttrain loss: 0.30422201083059586\n",
      "\ttrain loss: 0.3129305219573421\n",
      "\ttrain loss: 0.3977274247340592\n",
      "\ttrain loss: 0.4667813204101657\n",
      "\ttrain loss: 0.5586601442137956\n",
      "\ttrain loss: 0.5072389913754611\n",
      "\ttrain loss: 0.41102756813038743\n",
      "\ttrain loss: 0.2520007724502715\n",
      "\ttrain loss: 0.3652732290440973\n",
      "\ttrain loss: 0.26665842422502795\n",
      "\ttrain loss: 0.5242246236467747\n",
      "\ttrain loss: 0.40209333355781673\n",
      "\ttrain loss: 0.43588330326672997\n",
      "\ttrain loss: 0.547523958130002\n",
      "\ttrain loss: 0.4314402514449004\n",
      "\ttrain loss: 0.37170253096273076\n",
      "\ttrain loss: 0.4699390124658468\n",
      "\ttrain loss: 0.32660643012251656\n",
      "\ttrain loss: 0.3240401464436523\n",
      "\ttrain loss: 0.2727665977016178\n",
      "\ttrain loss: 0.22908781633237668\n",
      "\ttrain loss: 0.5037174331187424\n",
      "\ttrain loss: 0.36674518623268293\n",
      "\ttrain loss: 0.37201434939083633\n",
      "\ttrain loss: 0.3782010156428943\n",
      "\ttrain loss: 0.4974594775597741\n",
      "\ttrain loss: 0.37909907721032854\n",
      "\ttrain loss: 0.28211343245157156\n",
      "\ttrain loss: 0.3811479954160051\n",
      "\ttrain loss: 0.4082425237171907\n",
      "\ttrain loss: 0.3409837242096824\n",
      "\ttrain loss: 0.3883109226998893\n",
      "\ttrain loss: 0.301285451607485\n",
      "\ttrain loss: 0.3494575522224045\n",
      "\ttrain loss: 0.4188809812450734\n",
      "\ttrain loss: 0.47079332120863526\n",
      "\ttrain loss: 0.35512231505413894\n",
      "\ttrain loss: 0.2841223782551301\n",
      "\ttrain loss: 0.15270180588665316\n",
      "\ttrain loss: 0.46395982120840285\n",
      "\ttrain loss: 0.3145823719833105\n",
      "\ttrain loss: 0.34648225073414607\n",
      "\ttrain loss: 0.6478470193342774\n",
      "\ttrain loss: 0.5890095483867066\n",
      "\ttrain loss: 0.4704371939508477\n",
      "\ttrain loss: 0.5069609441017375\n",
      "\ttrain loss: 0.5571378937032929\n",
      "\ttrain loss: 0.4390396842700678\n",
      "\ttrain loss: 0.5251717851775393\n",
      "\ttrain loss: 0.5469721177848685\n",
      "\ttrain loss: 0.35903311548085537\n",
      "\ttrain loss: 0.3785663653232698\n",
      "\ttrain loss: 0.37381417441399223\n",
      "\ttrain loss: 0.48064203118034604\n",
      "\ttrain loss: 0.4420836050584392\n",
      "\ttrain loss: 0.3142171738142664\n",
      "\ttrain loss: 0.35456541784513096\n",
      "\ttrain loss: 0.31894987904645117\n",
      "\ttrain loss: 0.4573971225565481\n",
      "\ttrain loss: 0.4096384239706148\n",
      "\ttrain loss: 0.4773424272808826\n",
      "\ttrain loss: 0.3911237636564555\n",
      "\ttrain loss: 0.3575733333206863\n",
      "\ttrain loss: 0.38890721904941583\n",
      "\ttrain loss: 0.336802350447735\n",
      "\ttrain loss: 0.32564023849916607\n",
      "\ttrain loss: 0.409937493509142\n",
      "\ttrain loss: 0.3297327510124887\n",
      "\ttrain loss: 0.44051437476010064\n",
      "\ttrain loss: 0.4542020213672019\n",
      "\ttrain loss: 0.34530378857061306\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(14/15) is saved!\n",
      "\ttrain loss: 0.2116782050196192\n",
      "\ttrain loss: 0.35420465794263095\n",
      "\ttrain loss: 0.29596287963545376\n",
      "\ttrain loss: 0.46088089133724364\n",
      "\ttrain loss: 0.3658040346439688\n",
      "\ttrain loss: 0.5051024124172292\n",
      "\ttrain loss: 0.37845454939857837\n",
      "\ttrain loss: 0.367933592559315\n",
      "\ttrain loss: 0.32061626990378145\n",
      "\ttrain loss: 0.3499464436609506\n",
      "\ttrain loss: 0.4403405100576147\n",
      "\ttrain loss: 0.4541518135520378\n",
      "\ttrain loss: 0.4284717513530369\n",
      "\ttrain loss: 0.3595517631560605\n",
      "\ttrain loss: 0.4852365220324022\n",
      "\ttrain loss: 0.4321575149399996\n",
      "\ttrain loss: 0.4385128480134948\n",
      "\ttrain loss: 0.2948629892385614\n",
      "\ttrain loss: 0.2548735191321073\n",
      "\ttrain loss: 0.4837494007050838\n",
      "\ttrain loss: 0.3148549188646721\n",
      "\ttrain loss: 0.3876854789895874\n",
      "\ttrain loss: 0.3978005531902202\n",
      "\ttrain loss: 0.4416120803863032\n",
      "\ttrain loss: 0.33463909146878157\n",
      "\ttrain loss: 0.2597472964776092\n",
      "\ttrain loss: 0.37745328646779097\n",
      "\ttrain loss: 0.32961124604704933\n",
      "\ttrain loss: 0.33242496475035965\n",
      "\ttrain loss: 0.4843379744557197\n",
      "\ttrain loss: 0.3544735449182658\n",
      "\ttrain loss: 0.4328112283310843\n",
      "\ttrain loss: 0.5277307983101895\n",
      "\ttrain loss: 0.4541773062190436\n",
      "\ttrain loss: 0.40539883705698027\n",
      "\ttrain loss: 0.4726290616896043\n",
      "\ttrain loss: 0.3948639347154375\n",
      "\ttrain loss: 0.44132307792539704\n",
      "\ttrain loss: 0.4901580843357868\n",
      "\ttrain loss: 0.22475215874774168\n",
      "\ttrain loss: 0.3572373384239047\n",
      "\ttrain loss: 0.3828154400035687\n",
      "\ttrain loss: 0.47018901623415826\n",
      "\ttrain loss: 0.47929357685268614\n",
      "\ttrain loss: 0.48317210491730733\n",
      "\ttrain loss: 0.35336000427598413\n",
      "\ttrain loss: 0.3196542122098854\n",
      "\ttrain loss: 0.42086816818392014\n",
      "\ttrain loss: 0.42169735693040605\n",
      "\ttrain loss: 0.4019429493777703\n",
      "\ttrain loss: 0.3232391863904728\n",
      "\ttrain loss: 0.3516892331559746\n",
      "\ttrain loss: 0.36545955035035405\n",
      "\ttrain loss: 0.24780703996667908\n",
      "\ttrain loss: 0.31650963948928473\n",
      "\ttrain loss: 0.4430368367028114\n",
      "\ttrain loss: 0.4918852957462396\n",
      "\ttrain loss: 0.33648638661467833\n",
      "\ttrain loss: 0.3288257553210713\n",
      "\ttrain loss: 0.41421320634827824\n",
      "\ttrain loss: 0.3810419663976674\n",
      "\ttrain loss: 0.24948776514469065\n",
      "\ttrain loss: 0.33370286090581947\n",
      "\ttrain loss: 0.24999630775198722\n",
      "\ttrain loss: 0.5576208269790456\n",
      "\ttrain loss: 0.3933232992314198\n",
      "\ttrain loss: 0.2685016933932294\n",
      "\ttrain loss: 0.4098796771402657\n",
      "\ttrain loss: 0.2565113659168039\n",
      "\ttrain loss: 0.535393154397733\n",
      "\ttrain loss: 0.49410019757373047\n",
      "\ttrain loss: 0.3744150113691394\n",
      "\ttrain loss: 0.42404311579647014\n",
      "\ttrain loss: 0.470578460908202\n",
      "\ttrain loss: 0.390163476475925\n",
      "\ttrain loss: 0.6230493607410562\n",
      "\ttrain loss: 0.4536873342552915\n",
      "\ttrain loss: 0.34929925267995643\n",
      "\ttrain loss: 0.24862102718628698\n",
      "\ttrain loss: 0.43334778569442933\n",
      "\ttrain loss: 0.3181300986177329\n",
      "\ttrain loss: 0.3846836726056861\n",
      "\ttrain loss: 0.28997880114799435\n",
      "\ttrain loss: 0.5655731960407444\n",
      "\ttrain loss: 0.5233919136830173\n",
      "\ttrain loss: 0.41341901519802116\n",
      "\ttrain loss: 0.515560593637139\n",
      "\ttrain loss: 0.36779678278679623\n",
      "\ttrain loss: 0.3150851300843741\n",
      "\ttrain loss: 0.47362944274968893\n",
      "\ttrain loss: 0.40752573710400664\n",
      "\ttrain loss: 0.5013859179857343\n",
      "\ttrain loss: 0.4099781607527382\n",
      "\ttrain loss: 0.33412666879570124\n",
      "\ttrain loss: 0.466660860838528\n",
      "\ttrain loss: 0.32865925672448426\n",
      "\ttrain loss: 0.34393159279674\n",
      "\ttrain loss: 0.3526443984758565\n",
      "\ttrain loss: 0.4706375968179325\n",
      "\ttrain loss: 0.5279470314241556\n",
      "\ttrain loss: 0.3683711807199229\n",
      "\ttrain loss: 0.38847381969835526\n",
      "\ttrain loss: 0.24685319736177336\n",
      "\ttrain loss: 0.34237741188408416\n",
      "\ttrain loss: 0.4034022075099667\n",
      "\ttrain loss: 0.3561187788672787\n",
      "\ttrain loss: 0.39549138711708065\n",
      "\ttrain loss: 0.32494832684512304\n",
      "\ttrain loss: 0.3401333024479962\n",
      "\ttrain loss: 0.425730026622123\n",
      "\ttrain loss: 0.30384118507714686\n",
      "\ttrain loss: 0.301503145425138\n",
      "\ttrain loss: 0.25740784605599754\n",
      "\ttrain loss: 0.3013694024811676\n",
      "\ttrain loss: 0.30645661082922665\n",
      "\ttrain loss: 0.4566600575854899\n",
      "\ttrain loss: 0.32254843116578197\n",
      "\ttrain loss: 0.36494289471962593\n",
      "\ttrain loss: 0.3950236065470439\n",
      "\ttrain loss: 0.4365706637608029\n",
      "\ttrain loss: 0.4454319176661081\n",
      "\ttrain loss: 0.31636692776162134\n",
      "\ttrain loss: 0.3585695947170668\n",
      "\ttrain loss: 0.34373866126640207\n",
      "\ttrain loss: 0.48583875495132206\n",
      "\ttrain loss: 0.3466325775118432\n",
      "\ttrain loss: 0.5566055832455905\n",
      "\ttrain loss: 0.2598990813683591\n",
      "\ttrain loss: 0.5316175650338077\n",
      "\ttrain loss: 0.35190864867038407\n",
      "\ttrain loss: 0.3773323114446739\n",
      "\ttrain loss: 0.2883390721097921\n",
      "\ttrain loss: 0.3039557605392562\n",
      "\ttrain loss: 0.47180209900822223\n",
      "\ttrain loss: 0.4698346200365739\n",
      "\ttrain loss: 0.3609150330555181\n",
      "\ttrain loss: 0.496023622589725\n",
      "\ttrain loss: 0.2820531641189653\n",
      "\ttrain loss: 0.37038268919506445\n",
      "\ttrain loss: 0.3081322641889502\n",
      "\ttrain loss: 0.5080196577775743\n",
      "\ttrain loss: 0.3084849486454968\n",
      "\ttrain loss: 0.39210469081226146\n",
      "\ttrain loss: 0.3405064955639208\n",
      "\ttrain loss: 0.48030939021732055\n",
      "\ttrain loss: 0.42015132005092654\n",
      "\ttrain loss: 0.4964119710850811\n",
      "\ttrain loss: 0.5436507133801569\n",
      "\ttrain loss: 0.3007589256582096\n",
      "\ttrain loss: 0.3403914751132998\n",
      "\ttrain loss: 0.5374254184622245\n",
      "\ttrain loss: 0.3818759897180898\n",
      "\ttrain loss: 0.3460534672791299\n",
      "\ttrain loss: 0.3335545670458865\n",
      "\ttrain loss: 0.5266136024054366\n",
      "\ttrain loss: 0.38625383803091545\n",
      "\ttrain loss: 0.4291006253907662\n",
      "\ttrain loss: 0.5594513922505377\n",
      "\ttrain loss: 0.53257421789488\n",
      "\ttrain loss: 0.40358923034431393\n",
      "\ttrain loss: 0.37653414486500925\n",
      "\ttrain loss: 0.4672152283649993\n",
      "\ttrain loss: 0.5206460262103217\n",
      "\ttrain loss: 0.4646644830620132\n",
      "\ttrain loss: 0.44222631214904273\n",
      "\ttrain loss: 0.4449913854565424\n",
      "\ttrain loss: 0.3529044379546762\n",
      "\ttrain loss: 0.2855727265037461\n",
      "\ttrain loss: 0.4489402724723881\n",
      "\ttrain loss: 0.4479172185568248\n",
      "\ttrain loss: 0.4879692758370453\n",
      "\ttrain loss: 0.5124043427298576\n",
      "\ttrain loss: 0.49108061331736513\n",
      "\ttrain loss: 0.2655724948743262\n",
      "\ttrain loss: 0.3416088218773562\n",
      "\ttrain loss: 0.33643243268560274\n",
      "\ttrain loss: 0.22914161292115337\n",
      "\ttrain loss: 0.34748454813427243\n",
      "\ttrain loss: 0.27749990121700324\n",
      "\ttrain loss: 0.3196214558843152\n",
      "\ttrain loss: 0.3031566694110118\n",
      "\ttrain loss: 0.24322160918851427\n",
      "\ttrain loss: 0.2562794642529371\n",
      "\ttrain loss: 0.6470641739692127\n",
      "\ttrain loss: 0.6157591335957209\n",
      "\ttrain loss: 0.23966298029255348\n",
      "\ttrain loss: 0.46284564145819534\n",
      "\ttrain loss: 0.3541164926713307\n",
      "\ttrain loss: 0.39473608317434006\n",
      "\ttrain loss: 0.3030091724442549\n",
      "\ttrain loss: 0.3610844865553807\n",
      "\ttrain loss: 0.30182982473526226\n",
      "\ttrain loss: 0.4149719382329944\n",
      "\ttrain loss: 0.39889361250223104\n",
      "\ttrain loss: 0.4250663513948775\n",
      "\ttrain loss: 0.31171713803259715\n",
      "\ttrain loss: 0.28667245174540457\n",
      "\ttrain loss: 0.47855947520196757\n",
      "\ttrain loss: 0.25114423239793804\n",
      "\ttrain loss: 0.3177370883634091\n",
      "\ttrain loss: 0.3100444677247143\n",
      "\ttrain loss: 0.4944996019037795\n",
      "\ttrain loss: 0.5814333397234392\n",
      "\ttrain loss: 0.4092090756017611\n",
      "\ttrain loss: 0.35230522272102377\n",
      "\ttrain loss: 0.3112439800225156\n",
      "\ttrain loss: 0.27461207269210713\n",
      "\ttrain loss: 0.469696811139226\n",
      "\ttrain loss: 0.38841314779718533\n",
      "\ttrain loss: 0.5963983136944329\n",
      "\ttrain loss: 0.350287461862193\n",
      "\ttrain loss: 0.3652150596163404\n",
      "\ttrain loss: 0.34678772423822757\n",
      "\ttrain loss: 0.4829596563504379\n",
      "\ttrain loss: 0.3153335667112828\n",
      "\ttrain loss: 0.4246508904951969\n",
      "\ttrain loss: 0.3904464533359597\n",
      "\ttrain loss: 0.3162140399963973\n",
      "\ttrain loss: 0.5826501144352335\n",
      "\ttrain loss: 0.3711073748778628\n",
      "\ttrain loss: 0.2686171331965417\n",
      "\ttrain loss: 0.34954462583162016\n",
      "\ttrain loss: 0.47417557948674466\n",
      "\ttrain loss: 0.40225968503922227\n",
      "\ttrain loss: 0.4406232764527255\n",
      "\ttrain loss: 0.4968813628265093\n",
      "\ttrain loss: 0.36897153679162226\n",
      "\ttrain loss: 0.33870186842796596\n",
      "\ttrain loss: 0.5432865438635864\n",
      "\ttrain loss: 0.48057076768909496\n",
      "\ttrain loss: 0.5226881940014301\n",
      "\ttrain loss: 0.44601547440374784\n",
      "\ttrain loss: 0.2691320011530639\n",
      "\ttrain loss: 0.37402808342480687\n",
      "\ttrain loss: 0.45094176233246297\n",
      "\ttrain loss: 0.6335790817037896\n",
      "\ttrain loss: 0.6353325544418942\n",
      "\ttrain loss: 0.559508227670894\n",
      "\ttrain loss: 0.47349256353245484\n",
      "\ttrain loss: 0.3386715292269394\n",
      "\ttrain loss: 0.4594079842438967\n",
      "\ttrain loss: 0.2994460634932474\n",
      "\ttrain loss: 0.5037621299738181\n",
      "\ttrain loss: 0.3604913166354478\n",
      "\ttrain loss: 0.4886963051763517\n",
      "\ttrain loss: 0.33113544454925914\n",
      "\ttrain loss: 0.3621713141674109\n",
      "\ttrain loss: 0.4039305679169952\n",
      "\ttrain loss: 0.32919070033039555\n",
      "\ttrain loss: 0.3866217365293926\n",
      "\ttrain loss: 0.3222264783529031\n",
      "\ttrain loss: 0.4882686498206371\n",
      "\ttrain loss: 0.43555797184377565\n",
      "\ttrain loss: 0.3937210540938367\n",
      "\ttrain loss: 0.43431843354429067\n",
      "\ttrain loss: 0.3639125026632659\n",
      "\ttrain loss: 0.5776652525826229\n",
      "\ttrain loss: 0.27117275011832453\n",
      "\ttrain loss: 0.3483429269649184\n",
      "\ttrain loss: 0.3275287715368663\n",
      "\ttrain loss: 0.31719595813054147\n",
      "\ttrain loss: 0.34235031498477897\n",
      "\ttrain loss: 0.5126709400878857\n",
      "\ttrain loss: 0.4250709717055387\n",
      "\ttrain loss: 0.5115900306561836\n",
      "\ttrain loss: 0.47652178143893986\n",
      "\ttrain loss: 0.4340783792206977\n",
      "\ttrain loss: 0.3717212032029181\n",
      "\ttrain loss: 0.3283795266023234\n",
      "\ttrain loss: 0.44539388211219666\n",
      "\ttrain loss: 0.36651049261709134\n",
      "\ttrain loss: 0.39710669903340545\n",
      "\ttrain loss: 0.2711919006432132\n",
      "\ttrain loss: 0.3847643445786914\n",
      "\ttrain loss: 0.30981119407434715\n",
      "\ttrain loss: 0.4330573170683641\n",
      "\ttrain loss: 0.46792303707289573\n",
      "\ttrain loss: 0.34135785143750497\n",
      "\ttrain loss: 0.5707533683335755\n",
      "\ttrain loss: 0.38702050765535756\n",
      "\ttrain loss: 0.42510352158408293\n",
      "\ttrain loss: 0.4687185642582681\n",
      "\ttrain loss: 0.46612180263279945\n",
      "\ttrain loss: 0.3263679085985288\n",
      "\ttrain loss: 0.4502134268793169\n",
      "\ttrain loss: 0.32358525475792366\n",
      "\ttrain loss: 0.3631822555674218\n",
      "\ttrain loss: 0.29468325234025716\n",
      "\ttrain loss: 0.46374014965693944\n",
      "\ttrain loss: 0.30189912690172854\n",
      "\ttrain loss: 0.26376840348974906\n",
      "\ttrain loss: 0.460590274284878\n",
      "\ttrain loss: 0.3478928460660745\n",
      "\ttrain loss: 0.4869976140316256\n",
      "\ttrain loss: 0.2833668853523508\n",
      "\ttrain loss: 0.27945722196303424\n",
      "\ttrain loss: 0.30527068073907815\n",
      "\ttrain loss: 0.31845128565108516\n",
      "\ttrain loss: 0.299628997250693\n",
      "\ttrain loss: 0.5415532893456575\n",
      "\ttrain loss: 0.30562191032535135\n",
      "\ttrain loss: 0.45666387768506095\n",
      "\ttrain loss: 0.17984380684956944\n",
      "\ttrain loss: 0.6326740382768717\n",
      "\ttrain loss: 0.4385965918870855\n",
      "\ttrain loss: 0.48251045908210444\n",
      "\ttrain loss: 0.41618454897146195\n",
      "\ttrain loss: 0.7231206831916585\n",
      "\ttrain loss: 0.36020586652105263\n",
      "\ttrain loss: 0.32462010552247295\n",
      "\ttrain loss: 0.6674115759390125\n",
      "\ttrain loss: 0.3674253249136746\n",
      "\ttrain loss: 0.29496071458897377\n",
      "\ttrain loss: 0.3756356708995183\n",
      "\ttrain loss: 0.34111837365308256\n",
      "\ttrain loss: 0.6089647039895132\n",
      "\ttrain loss: 0.48819135425181626\n",
      "\ttrain loss: 0.3035695971091443\n",
      "\ttrain loss: 0.3632194159438349\n",
      "\ttrain loss: 0.2663896608320926\n",
      "\ttrain loss: 0.21747579395806232\n",
      "\ttrain loss: 0.399946319440498\n",
      "\ttrain loss: 0.36443124301387586\n",
      "\ttrain loss: 0.35415232630272875\n",
      "\ttrain loss: 0.2783219311504921\n",
      "\ttrain loss: 0.4282431689952103\n",
      "\ttrain loss: 0.3857829385127097\n",
      "\ttrain loss: 0.43110179664497095\n",
      "\ttrain loss: 0.305548344249372\n",
      "\ttrain loss: 0.34027712144925093\n",
      "\ttrain loss: 0.512929707608561\n",
      "\ttrain loss: 0.3459973143365149\n",
      "\ttrain loss: 0.4232325904526586\n",
      "\ttrain loss: 0.46814488225377104\n",
      "\ttrain loss: 0.3135167801859071\n",
      "\ttrain loss: 0.5404666424684152\n",
      "\ttrain loss: 0.2712650294533381\n",
      "\ttrain loss: 0.5773730934443184\n",
      "\ttrain loss: 0.41931567341463827\n",
      "\ttrain loss: 0.42047067590316956\n",
      "\ttrain loss: 0.4828098942690653\n",
      "\ttrain loss: 0.4883345545833399\n",
      "\ttrain loss: 0.3749394976943715\n",
      "\ttrain loss: 0.29759021983352285\n",
      "\ttrain loss: 0.43987539586814345\n",
      "\ttrain loss: 0.3941032903898489\n",
      "\ttrain loss: 0.5924239062622432\n",
      "\ttrain loss: 0.4244952794425712\n",
      "\ttrain loss: 0.47155680161724933\n",
      "\ttrain loss: 0.3781218603065325\n",
      "\ttrain loss: 0.4090721068028652\n",
      "\ttrain loss: 0.2951653762190186\n",
      "\ttrain loss: 0.7263828875544531\n",
      "\ttrain loss: 0.3213486072644059\n",
      "\ttrain loss: 0.5475864192118307\n",
      "\ttrain loss: 0.5178223519228718\n",
      "\ttrain loss: 0.572379823996546\n",
      "\ttrain loss: 0.35590848136742304\n",
      "\ttrain loss: 0.561852583849231\n",
      "\ttrain loss: 0.4780972940463867\n",
      "\ttrain loss: 0.40318142224123155\n",
      "\ttrain loss: 0.36773164855929075\n",
      "\ttrain loss: 0.30395779986231203\n",
      "\ttrain loss: 0.3683322388505139\n",
      "\ttrain loss: 0.3762929334539835\n",
      "\ttrain loss: 0.26366605432594237\n",
      "\ttrain loss: 0.3673787375480493\n",
      "\ttrain loss: 0.43187093690531164\n",
      "\ttrain loss: 0.38799466675358085\n",
      "\ttrain loss: 0.49317567934692075\n",
      "\ttrain loss: 0.28399077690946967\n",
      "\ttrain loss: 0.37585612806924\n",
      "\ttrain loss: 0.27582224071687933\n",
      "\ttrain loss: 0.27037655472723165\n",
      "\ttrain loss: 0.5697397789764376\n",
      "\ttrain loss: 0.31678840190665397\n",
      "\ttrain loss: 0.39682009298578197\n",
      "\ttrain loss: 0.37057672169975386\n",
      "\ttrain loss: 0.3866134895242878\n",
      "\ttrain loss: 0.46248564505984\n",
      "\ttrain loss: 0.2780089616899035\n",
      "\ttrain loss: 0.37159992253516366\n",
      "\ttrain loss: 0.32667449459210396\n",
      "\ttrain loss: 0.5397600370681749\n",
      "\ttrain loss: 0.50871098521621\n",
      "\ttrain loss: 0.42815407807266675\n",
      "\ttrain loss: 0.35484826206760306\n",
      "\ttrain loss: 0.42395274461784915\n",
      "\ttrain loss: 0.4533792709797053\n",
      "\ttrain loss: 0.5221862598156267\n",
      "\ttrain loss: 0.4435661463956162\n",
      "\ttrain loss: 0.37091572140010143\n",
      "\ttrain loss: 0.3570622506337971\n",
      "\ttrain loss: 0.43343641946837674\n",
      "\ttrain loss: 0.5499627550299506\n",
      "\ttrain loss: 0.5104086854779508\n",
      "\ttrain loss: 0.27730074919184566\n",
      "\ttrain loss: 0.6258802468154193\n",
      "\ttrain loss: 0.3474201400537521\n",
      "\ttrain loss: 0.37700983638354607\n",
      "\ttrain loss: 0.4588379089020864\n",
      "\ttrain loss: 0.42588273795771936\n",
      "\ttrain loss: 0.3408636435986683\n",
      "\ttrain loss: 0.5049692207989728\n",
      "\ttrain loss: 0.4532093716641952\n",
      "\ttrain loss: 0.46737367287256765\n",
      "\ttrain loss: 0.3296547138795633\n",
      "\ttrain loss: 0.4636140456919702\n",
      "\ttrain loss: 0.3712224902186724\n",
      "\ttrain loss: 0.4068275151659032\n",
      "\ttrain loss: 0.46861282353459577\n",
      "\ttrain loss: 0.3705359116276218\n",
      "\ttrain loss: 0.3528444523638746\n",
      "\ttrain loss: 0.3802782065971864\n",
      "\ttrain loss: 0.4156892550185445\n",
      "\ttrain loss: 0.4016010007798193\n",
      "\ttrain loss: 0.3489136975424154\n",
      "\ttrain loss: 0.34914304992171136\n",
      "\ttrain loss: 0.5294845035140996\n",
      "\ttrain loss: 0.3036749356017954\n",
      "\ttrain loss: 0.3959663793054469\n",
      "\ttrain loss: 0.38510371619027883\n",
      "\ttrain loss: 0.29816611991709696\n",
      "\ttrain loss: 0.39581845301847324\n",
      "\ttrain loss: 0.39019984373988165\n",
      "\ttrain loss: 0.38916280012529425\n",
      "\ttrain loss: 0.3358005341712796\n",
      "\ttrain loss: 0.3701402356291527\n",
      "\ttrain loss: 0.46758344678175234\n",
      "\ttrain loss: 0.28241453205630446\n",
      "\ttrain loss: 0.35254513656308023\n",
      "\ttrain loss: 0.3012713455691698\n",
      "\ttrain loss: 0.5933413324776636\n",
      "\ttrain loss: 0.4130670751500535\n",
      "\ttrain loss: 0.3246969366519727\n",
      "\ttrain loss: 0.33176735783690603\n",
      "\ttrain loss: 0.2904169099803415\n",
      "\ttrain loss: 0.3603521143672018\n",
      "\ttrain loss: 0.4850906574647701\n",
      "\ttrain loss: 0.5354302256468171\n",
      "\ttrain loss: 0.5232959197639244\n",
      "\ttrain loss: 0.336586514408039\n",
      "\ttrain loss: 0.4302744834782197\n",
      "\ttrain loss: 0.37849372047917196\n",
      "\ttrain loss: 0.40520371097289787\n",
      "\ttrain loss: 0.31300922804618364\n",
      "\ttrain loss: 0.3188069594824884\n",
      "\ttrain loss: 0.3455186797400566\n",
      "\ttrain loss: 0.3823716508032402\n",
      "\ttrain loss: 0.31504217320110317\n",
      "\ttrain loss: 0.5562543859233747\n",
      "\ttrain loss: 0.5269553982927289\n",
      "\ttrain loss: 0.40039776094289303\n",
      "\ttrain loss: 0.25104603652282076\n",
      "\ttrain loss: 0.42551179059448446\n",
      "\ttrain loss: 0.45809836081884536\n",
      "\ttrain loss: 0.32406085478170266\n",
      "\ttrain loss: 0.31431622740944876\n",
      "\ttrain loss: 0.2943886900867584\n",
      "\ttrain loss: 0.37241930663298994\n",
      "\ttrain loss: 0.3906391070523151\n",
      "\ttrain loss: 0.36354436706844406\n",
      "\ttrain loss: 0.3032123305490155\n",
      "\ttrain loss: 0.527806405838392\n",
      "\ttrain loss: 0.2581410407109078\n",
      "\ttrain loss: 0.3243714321365539\n",
      "\ttrain loss: 0.38819054390385166\n",
      "\ttrain loss: 0.4320289081714989\n",
      "\ttrain loss: 0.22617773865283383\n",
      "\ttrain loss: 0.2992199038972072\n",
      "\ttrain loss: 0.3776783100714511\n",
      "\ttrain loss: 0.44468010620349896\n",
      "\ttrain loss: 0.4520553053073305\n",
      "\ttrain loss: 0.5378375505127551\n",
      "\ttrain loss: 0.5194395767225772\n",
      "\ttrain loss: 0.33303899533968695\n",
      "\ttrain loss: 0.4311913156395197\n",
      "\ttrain loss: 0.3940850687885135\n",
      "\ttrain loss: 0.6812454271901873\n",
      "\ttrain loss: 0.4850791562394601\n",
      "\ttrain loss: 0.4044003392533595\n",
      "\ttrain loss: 0.26949970761947073\n",
      "\ttrain loss: 0.5839300600187753\n",
      "\ttrain loss: 0.4454377712607926\n",
      "\ttrain loss: 0.3091221187451767\n",
      "\ttrain loss: 0.27405548168448773\n",
      "\ttrain loss: 0.31289038935746105\n",
      "\ttrain loss: 0.2734895865092626\n",
      "\ttrain loss: 0.42738313682477014\n",
      "\ttrain loss: 0.3629418226287421\n",
      "\ttrain loss: 0.37873644354157254\n",
      "\ttrain loss: 0.4150746965442129\n",
      "\ttrain loss: 0.3202509460038627\n",
      "\ttrain loss: 0.3789075319863529\n",
      "\ttrain loss: 0.45196259471483674\n",
      "\ttrain loss: 0.4047579183202204\n",
      "\ttrain loss: 0.3489593940003308\n",
      "\ttrain loss: 0.3385052444575388\n",
      "\ttrain loss: 0.308634747411774\n",
      "\ttrain loss: 0.4518147523097524\n",
      "\ttrain loss: 0.4969836637124884\n",
      "\ttrain loss: 0.6448027979434348\n",
      "\ttrain loss: 0.3248924094027277\n",
      "\ttrain loss: 0.3162952819077313\n",
      "\ttrain loss: 0.3228923748086614\n",
      "\ttrain loss: 0.3581679662120956\n",
      "\ttrain loss: 0.40166380975948285\n",
      "\ttrain loss: 0.49780406006855815\n",
      "\ttrain loss: 0.31768646833150044\n",
      "\ttrain loss: 0.4619050268805423\n",
      "\ttrain loss: 0.35550832302219576\n",
      "\ttrain loss: 0.4582199319588407\n",
      "\ttrain loss: 0.44905411558889613\n",
      "\ttrain loss: 0.42357598082299164\n",
      "\ttrain loss: 0.3534819206030322\n",
      "\ttrain loss: 0.2769906095652046\n",
      "\ttrain loss: 0.46835339857221303\n",
      "\ttrain loss: 0.40137240438788746\n",
      "\ttrain loss: 0.3439507809204223\n",
      "\ttrain loss: 0.4141098093088187\n",
      "\ttrain loss: 0.46912611268728\n",
      "\ttrain loss: 0.3780113021738295\n",
      "\ttrain loss: 0.5012197326978095\n",
      "\ttrain loss: 0.5180977249466221\n",
      "\ttrain loss: 0.29122287391369145\n",
      "\ttrain loss: 0.34851046581707923\n",
      "\ttrain loss: 0.4530545478413625\n",
      "\ttrain loss: 0.5061063372337564\n",
      "\ttrain loss: 0.49970889504507643\n",
      "\ttrain loss: 0.5162061981110746\n",
      "\ttrain loss: 0.529307708113524\n",
      "\ttrain loss: 0.3225251532757045\n",
      "\ttrain loss: 0.45087712195831564\n",
      "\ttrain loss: 0.3520171975762525\n",
      "\ttrain loss: 0.2771179093690147\n",
      "\ttrain loss: 0.22676599996161667\n",
      "\ttrain loss: 0.3348299563776923\n",
      "\ttrain loss: 0.45132561892307266\n",
      "\ttrain loss: 0.33988696999403134\n",
      "\ttrain loss: 0.3655044494325374\n",
      "\ttrain loss: 0.2841247725334123\n",
      "\ttrain loss: 0.4843223879873986\n",
      "\ttrain loss: 0.524263979540015\n",
      "\ttrain loss: 0.5142354605977785\n",
      "\ttrain loss: 0.3527972279464424\n",
      "\ttrain loss: 0.49938155520931826\n",
      "\ttrain loss: 0.48907345000814684\n",
      "\ttrain loss: 0.3289812157518746\n",
      "\ttrain loss: 0.35615617085338136\n",
      "\ttrain loss: 0.38059183331651625\n",
      "\ttrain loss: 0.38795860360981754\n",
      "\ttrain loss: 0.29876673568168655\n",
      "\ttrain loss: 0.5017382234165746\n",
      "\ttrain loss: 0.40937198110375134\n",
      "\ttrain loss: 0.45909234455585524\n",
      "\ttrain loss: 0.43879655119454375\n",
      "\ttrain loss: 0.32252326228965383\n",
      "\ttrain loss: 0.7226034812765013\n",
      "\ttrain loss: 0.3528065900949231\n",
      "\ttrain loss: 0.47060418091471046\n",
      "\ttrain loss: 0.6569514497750701\n",
      "\ttrain loss: 0.7689111289133232\n",
      "\ttrain loss: 0.34346532680586284\n",
      "\ttrain loss: 0.4436860394359361\n",
      "\ttrain loss: 0.2297992023474062\n",
      "\ttrain loss: 0.3333724798922515\n",
      "\ttrain loss: 0.46930028505809807\n",
      "\ttrain loss: 0.3183875671576026\n",
      "\ttrain loss: 0.45490376457122444\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "model(15/15) is saved!\n",
      "training network params: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "Saved Newwork Parameters!\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(network, x_train_loader = train_loader, x_test_loader = test_loader,\n",
    "                 epochs=15, mini_batch_size=32,\n",
    "                 optimizer='Adam', optimizer_param={'lr':0.0001},\n",
    "                 evaluate_sample_num_per_epoch=3)\n",
    "\n",
    "# trainer.network.load_params(file_name='epoch_1.pkl')\n",
    "\n",
    "trainer.train(0)\n",
    "\n",
    "network.save_params(\"VGG6.pkl\")\n",
    "print(\"Saved Newwork Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ecb9750e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params is successfully loaded!: dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W3', 'b3', 'gamma3', 'beta3', 'W4', 'b4', 'gamma4', 'beta4', 'W5', 'b5', 'W6', 'b6'])\n",
      "(1, 3, 224, 224)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03602253,  0.05814829]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "network.load_params('epoch_15.pkl')\n",
    "\n",
    "img = cv2.imread('/Users/yuchul/Anaconda/Baram/MaskClassificaion/data_old/mask_cnn/Train/Non_Mask/586_1.jpg')\n",
    "# img = cv2.imread('/Users/yuchul/Anaconda/Baram/MaskClassificaion/data_old/mask_cnn/Train/Mask/0_0.jpg')\n",
    "img_resize = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "fix_img = cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB)\n",
    "fimg = fix_img.transpose(2,0,1)\n",
    "\n",
    "fixed_img = np.expand_dims(fimg, axis=0)\n",
    "\n",
    "print(fixed_img.shape)\n",
    "# print(fixed_img.label)\n",
    "imshow(fix_img)\n",
    "\n",
    "network.predict(fixed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7979363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n",
      "[[-0.03602253  0.05814829]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         reshaped\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mreshape(normalized,(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         print(reshaped.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         reshaped = np.vstack([reshaped])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#         print(reshaped.shape)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         result\u001b[38;5;241m=\u001b[39m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     34\u001b[0m         label\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(result,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36mVGG6.predict\u001b[0;34m(self, x, train_flg, first_flg)\u001b[0m\n\u001b[1;32m     72\u001b[0m             x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, train_flg)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 8\u001b[0m out[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels_dict={0:'Mask',1:'NoMask'}\n",
    "color_dict={1:(0,0,255),0:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(224,224))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,3,224,224))\n",
    "#         print(reshaped.shape)\n",
    "#         reshaped = np.vstack([reshaped])\n",
    "#         print(reshaped.shape)\n",
    "        result=network.predict(reshaped)\n",
    "        print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(100)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
